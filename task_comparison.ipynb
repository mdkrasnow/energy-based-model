{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzh7-Ram-j9r"
   },
   "source": [
    "# IRED Tasks: Baseline vs Aggressive Curriculum Comparison\n",
    "\n",
    "Comparative analysis of baseline training vs aggressive adversarial curriculum on core IRED reasoning tasks:\n",
    "- **Connectivity**: Graph connectivity reasoning (12x12 graphs, 0.1 edge probability)\n",
    "- **Sudoku**: Sudoku completion task (standard difficulty)\n",
    "\n",
    "This notebook evaluates whether aggressive curriculum learning improves identity error performance within limited training steps for reasoning tasks.\n",
    "\n",
    "**Key Research Question**: Does aggressive curriculum learning outperform baseline training on identity error metrics for IRED reasoning tasks?\n",
    "\n",
    "**Tasks Evaluated**: Easier versions (non-OOD) of connectivity and sudoku tasks as described in IRED paper by Yilun Du."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttlLgjdR-j9t",
    "outputId": "d4d35e3f-d98e-46d6-e01a-2a91434ecad3"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "%cd energy-based-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUU1Z1rj-j9u"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAmsGBDQ-j9v"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZLi5DGT-j9v"
   },
   "source": [
    "## Task and Curriculum Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0MwXmDg-j9v",
    "outputId": "0f2b9706-161e-4bbc-bb09-6106e9b278cc"
   },
   "outputs": [],
   "source": [
    "# Task-specific configurations based on IRED paper\n",
    "TASKS = {\n",
    "    'connectivity': {\n",
    "        'name': 'Graph Connectivity',\n",
    "        'description': 'Graph connectivity reasoning (12x12, p=0.1)',\n",
    "        'dataset': 'connectivity',\n",
    "        'model': 'gnn-conv-1d-v2',\n",
    "        'batch_size': 64,  # From train.py validation_batch_size\n",
    "        'train_steps': 150,  # Longer for reasoning tasks\n",
    "        'extra_args': [],\n",
    "        'color': '#2E8B57',\n",
    "        'expected_metric': 'identity_error'\n",
    "    },\n",
    "    'sudoku': {\n",
    "        'name': 'Sudoku Completion', \n",
    "        'description': 'Sudoku puzzle completion task',\n",
    "        'dataset': 'sudoku',\n",
    "        'model': 'sudoku',\n",
    "        'batch_size': 64,  # From train.py validation_batch_size  \n",
    "        'train_steps': 150,\n",
    "        'extra_args': ['--cond_mask', 'True'],  # Required for sudoku\n",
    "        'color': '#4169E1',\n",
    "        'expected_metric': 'sudoku_accuracy'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Only 2 curricula to compare: baseline vs aggressive\n",
    "CURRICULA = {\n",
    "    'baseline': {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'No curriculum learning',\n",
    "        'args': ['--disable-curriculum', 'True'],\n",
    "        'color': '#1f77b4'\n",
    "    },\n",
    "    'aggressive': {\n",
    "        'name': 'Aggressive Curriculum', \n",
    "        'description': 'Rapid adversarial escalation',\n",
    "        'args': ['--curriculum-config', 'aggressive'],\n",
    "        'color': '#d62728'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Common training parameters\n",
    "COMMON_ARGS = {\n",
    "    'diffusion_steps': 10,\n",
    "    'supervise_energy_landscape': 'True',\n",
    "    'save_csv_logs': True,\n",
    "    'csv_log_interval': 100\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(TASKS)} IRED reasoning tasks:\")\n",
    "for key, config in TASKS.items():\n",
    "    print(f\"  â€¢ {config['name']}: {config['description']}\")\n",
    "    print(f\"    Model: {config['model']}, Steps: {config['train_steps']}\")\n",
    "\n",
    "print(f\"\\nComparing {len(CURRICULA)} curriculum approaches:\")\n",
    "for key, config in CURRICULA.items():\n",
    "    print(f\"  â€¢ {config['name']}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHwwUrR-j9w"
   },
   "source": [
    "## Training Command Builder and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBg0XbEp-j9w"
   },
   "outputs": [],
   "source": [
    "def build_task_training_command(task_key: str, curriculum_key: str) -> str:\n",
    "    \"\"\"Build training command for a specific task-curriculum combination.\"\"\"\n",
    "    task = TASKS[task_key]\n",
    "    curriculum = CURRICULA[curriculum_key]\n",
    "    \n",
    "    base_cmd = f\"\"\"python train.py \\\n",
    "        --dataset {task['dataset']} \\\n",
    "        --model {task['model']} \\\n",
    "        --batch_size {task['batch_size']} \\\n",
    "        --diffusion_steps {COMMON_ARGS['diffusion_steps']} \\\n",
    "        --supervise-energy-landscape {COMMON_ARGS['supervise_energy_landscape']} \\\n",
    "        --train-num-steps {task['train_steps']} \\\n",
    "        --save-csv-logs \\\n",
    "        --csv-log-interval {COMMON_ARGS['csv_log_interval']} \\\n",
    "        --csv-log-dir ./csv_logs_{task_key}_{curriculum_key}\"\"\"\n",
    "    \n",
    "    # Add task-specific arguments\n",
    "    if task['extra_args']:\n",
    "        base_cmd += ' \\\n",
    "        ' + ' \\\n",
    "        '.join(task['extra_args'])\n",
    "    \n",
    "    # Add curriculum arguments  \n",
    "    if curriculum['args']:\n",
    "        base_cmd += ' \\\n",
    "        ' + ' \\\n",
    "        '.join(curriculum['args'])\n",
    "    \n",
    "    return base_cmd\n",
    "\n",
    "def load_csv_data(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV data with error handling.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            return pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"Warning: {csv_path} not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_get_final_value(df: pd.DataFrame, column: str, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the final value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].iloc[-1])\n",
    "\n",
    "def safe_get_best_value(df: pd.DataFrame, column: str, minimize: bool = True, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the best (min/max) value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].min() if minimize else df[column].max())\n",
    "\n",
    "def calculate_convergence_step(df: pd.DataFrame, column: str, threshold_pct: float = 0.9) -> int:\n",
    "    \"\"\"Calculate step where metric reaches threshold percentage of final improvement.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) < 5:\n",
    "        return -1\n",
    "    \n",
    "    initial_val = df[column].iloc[:5].mean()\n",
    "    final_val = df[column].iloc[-5:].mean()\n",
    "    target_val = initial_val - threshold_pct * (initial_val - final_val)\n",
    "    \n",
    "    below_target = df[df[column] <= target_val]\n",
    "    if not below_target.empty:\n",
    "        return int(below_target['step'].iloc[0])\n",
    "    else:\n",
    "        return int(df['step'].iloc[-1])\n",
    "\n",
    "print(\"Utility functions defined successfully!\")\n",
    "print(\"\\nExample command structure:\")\n",
    "print(build_task_training_command('connectivity', 'baseline')[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i95hNsVz-j9w"
   },
   "source": [
    "## Multi-Task Multi-Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i95hNsVz-j9w",
    "outputId": "2366f057-5bd2-43a1-9267-ef9745a36b8d"
   },
   "outputs": [],
   "source": [
    "# Train all task-curriculum combinations\n",
    "training_results = {}\n",
    "\n",
    "for task_key, task_config in TASKS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING TASK: {task_config['name']}\")\n",
    "    print(f\"Description: {task_config['description']}\")\n",
    "    print(f\"Dataset: {task_config['dataset']}, Model: {task_config['model']}\")\n",
    "    print(f\"Training Steps: {task_config['train_steps']}, Batch Size: {task_config['batch_size']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    task_results = {}\n",
    "    \n",
    "    for curriculum_key, curriculum_config in CURRICULA.items():\n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(f\"Starting {task_config['name']} with {curriculum_config['name']}\")\n",
    "        print(f\"Curriculum Description: {curriculum_config['description']}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        # Build training command\n",
    "        cmd = build_task_training_command(task_key, curriculum_key)\n",
    "        print(f\"\\nCommand: {cmd}\")\n",
    "        print(\"\\nTraining output:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Execute training with real-time output\n",
    "        try:\n",
    "            # Use subprocess to capture and display output in real-time\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Display output line by line as it comes\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                if line:\n",
    "                    print(line.rstrip())\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "            # Wait for process to complete\n",
    "            result = process.wait()\n",
    "            task_results[curriculum_key] = result\n",
    "            \n",
    "            if result == 0:\n",
    "                print(f\"âœ“ {task_config['name']} with {curriculum_config['name']} completed successfully\")\n",
    "            else:\n",
    "                print(f\"âœ— {task_config['name']} with {curriculum_config['name']} failed with exit code {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error during {task_config['name']} with {curriculum_config['name']} training: {e}\")\n",
    "            task_results[curriculum_key] = -1\n",
    "            \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    training_results[task_key] = task_results\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL TASK TRAINING COMPLETED!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\nTRAINING SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "total_successful = 0\n",
    "total_experiments = 0\n",
    "\n",
    "for task_key, task_results in training_results.items():\n",
    "    task_name = TASKS[task_key]['name']\n",
    "    successful = sum(1 for result in task_results.values() if result == 0)\n",
    "    total = len(task_results)\n",
    "    total_successful += successful\n",
    "    total_experiments += total\n",
    "    \n",
    "    print(f\"\\n{task_name}: {successful}/{total} completed successfully\")\n",
    "    for curriculum_key, result in task_results.items():\n",
    "        curriculum_name = CURRICULA[curriculum_key]['name']\n",
    "        status = \"âœ“ Success\" if result == 0 else \"âœ— Failed\"\n",
    "        print(f\"  {curriculum_name}: {status}\")\n",
    "\n",
    "success_rate = (total_successful / total_experiments * 100) if total_experiments > 0 else 0\n",
    "print(f\"\\nOVERALL SUCCESS RATE: {total_successful}/{total_experiments} ({success_rate:.1f}%)\")\n",
    "\n",
    "if success_rate >= 75:\n",
    "    print(\"ðŸŽ¯ Training pipeline working well! Proceeding to analysis...\")\n",
    "elif success_rate >= 50:\n",
    "    print(\"âš ï¸ Some training issues detected, but proceeding with available data...\")\n",
    "else:\n",
    "    print(\"âŒ Significant training issues detected. Check configurations and logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNqMLoV_-j9x"
   },
   "source": [
    "## Task-Specific Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "YNPgrc2A-j9x",
    "outputId": "31e5a93b-e184-4817-f100-a0d58cd84c50"
   },
   "outputs": [],
   "source": [
    "def load_all_task_results() -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Load results organized by task, then curriculum, then data type.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for task_key in TASKS.keys():\n",
    "        print(f\"Loading {task_key} results...\")\n",
    "        task_data = {}\n",
    "        \n",
    "        for curriculum_key in CURRICULA.keys():\n",
    "            csv_dir = Path(f\"./csv_logs_{task_key}_{curriculum_key}\")\n",
    "            \n",
    "            curriculum_data = {}\n",
    "            patterns = {\n",
    "                'training': 'training_metrics_*.csv',\n",
    "                'validation': 'validation_metrics_*.csv', \n",
    "                'energy': 'energy_metrics_*.csv',\n",
    "                'curriculum': 'curriculum_metrics_*.csv',\n",
    "                'robustness': 'robustness_metrics_*.csv'\n",
    "            }\n",
    "            \n",
    "            for key, pattern in patterns.items():\n",
    "                files = glob.glob(str(csv_dir / pattern))\n",
    "                if files:\n",
    "                    # Get most recent file\n",
    "                    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n",
    "                    curriculum_data[key] = load_csv_data(Path(latest_file))\n",
    "                else:\n",
    "                    curriculum_data[key] = None\n",
    "            \n",
    "            available = sum(1 for df in curriculum_data.values() if df is not None)\n",
    "            print(f\"  {curriculum_key}: {available}/5 data files found\")\n",
    "            task_data[curriculum_key] = curriculum_data\n",
    "        \n",
    "        results[task_key] = task_data\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_task_metrics(task_key: str, task_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract key metrics for a specific task.\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for curriculum_key, data in task_data.items():\n",
    "        curriculum_config = CURRICULA[curriculum_key]\n",
    "        training_df = data.get('training')\n",
    "        validation_df = data.get('validation') \n",
    "        energy_df = data.get('energy')\n",
    "        \n",
    "        # Extract identity error (key metric for reasoning tasks)\n",
    "        identity_error = float('inf')\n",
    "        best_identity_error = float('inf')\n",
    "        final_accuracy = 0.0\n",
    "        best_accuracy = 0.0\n",
    "        convergence_step = -1\n",
    "        \n",
    "        if validation_df is not None:\n",
    "            # Look for identity_error metric\n",
    "            identity_rows = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "            if not identity_rows.empty:\n",
    "                identity_error = identity_rows['metric_value'].iloc[-1]\n",
    "                best_identity_error = identity_rows['metric_value'].min()\n",
    "            \n",
    "            # Look for accuracy metric\n",
    "            accuracy_rows = validation_df[validation_df['metric_name'] == 'accuracy']\n",
    "            if not accuracy_rows.empty:\n",
    "                final_accuracy = accuracy_rows['metric_value'].iloc[-1] * 100  # Convert to percentage\n",
    "                best_accuracy = accuracy_rows['metric_value'].max() * 100\n",
    "            \n",
    "            # Look for sudoku-specific accuracy if applicable\n",
    "            if task_key == 'sudoku':\n",
    "                sudoku_acc_rows = validation_df[validation_df['metric_name'] == 'sudoku_accuracy']\n",
    "                if not sudoku_acc_rows.empty:\n",
    "                    final_accuracy = sudoku_acc_rows['metric_value'].iloc[-1] * 100\n",
    "                    best_accuracy = sudoku_acc_rows['metric_value'].max() * 100\n",
    "        \n",
    "        # Calculate convergence step for total loss\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            convergence_step = calculate_convergence_step(training_df, 'total_loss')\n",
    "        \n",
    "        metrics = {\n",
    "            'task': task_key,\n",
    "            'task_name': TASKS[task_key]['name'],\n",
    "            'curriculum': curriculum_key,\n",
    "            'curriculum_name': curriculum_config['name'],\n",
    "            'color': curriculum_config['color'],\n",
    "            \n",
    "            # Training metrics\n",
    "            'final_total_loss': safe_get_final_value(training_df, 'total_loss'),\n",
    "            'best_total_loss': safe_get_best_value(training_df, 'total_loss', minimize=True),\n",
    "            'final_energy_loss': safe_get_final_value(training_df, 'loss_energy'),\n",
    "            'final_denoise_loss': safe_get_final_value(training_df, 'loss_denoise'),\n",
    "            \n",
    "            # Key reasoning metrics\n",
    "            'final_identity_error': identity_error,\n",
    "            'best_identity_error': best_identity_error,\n",
    "            'final_accuracy': final_accuracy,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            \n",
    "            # Performance metrics\n",
    "            'training_time': training_df['nn_time'].mean() if training_df is not None and 'nn_time' in training_df.columns else 0.0,\n",
    "            'convergence_step': convergence_step,\n",
    "            \n",
    "            # Energy landscape metrics\n",
    "            'final_energy_margin': safe_get_final_value(energy_df, 'energy_margin'),\n",
    "            'curriculum_weight': safe_get_best_value(energy_df, 'curriculum_weight', minimize=False) if energy_df is not None and 'curriculum_weight' in energy_df.columns else 0.0\n",
    "        }\n",
    "        \n",
    "        processed_data.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "def calculate_improvement_metrics(task_summary: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate improvement metrics for aggressive vs baseline.\"\"\"\n",
    "    baseline_row = task_summary[task_summary['curriculum'] == 'baseline']\n",
    "    aggressive_row = task_summary[task_summary['curriculum'] == 'aggressive']\n",
    "    \n",
    "    improvements = {}\n",
    "    \n",
    "    if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "        baseline = baseline_row.iloc[0]\n",
    "        aggressive = aggressive_row.iloc[0]\n",
    "        \n",
    "        # Identity error improvement (lower is better)\n",
    "        if baseline['final_identity_error'] != float('inf') and aggressive['final_identity_error'] != float('inf'):\n",
    "            improvements['identity_error'] = (baseline['final_identity_error'] - aggressive['final_identity_error']) / baseline['final_identity_error'] * 100\n",
    "        else:\n",
    "            improvements['identity_error'] = 0.0\n",
    "        \n",
    "        # Loss improvement (lower is better)\n",
    "        if baseline['final_total_loss'] > 0 and aggressive['final_total_loss'] > 0:\n",
    "            improvements['total_loss'] = (baseline['final_total_loss'] - aggressive['final_total_loss']) / baseline['final_total_loss'] * 100\n",
    "        else:\n",
    "            improvements['total_loss'] = 0.0\n",
    "        \n",
    "        # Accuracy improvement (higher is better)\n",
    "        if baseline['final_accuracy'] > 0:\n",
    "            improvements['accuracy'] = (aggressive['final_accuracy'] - baseline['final_accuracy']) / baseline['final_accuracy'] * 100\n",
    "        else:\n",
    "            improvements['accuracy'] = 0.0\n",
    "        \n",
    "        # Training speed comparison (lower time is better)\n",
    "        if baseline['training_time'] > 0 and aggressive['training_time'] > 0:\n",
    "            improvements['training_speed'] = (baseline['training_time'] - aggressive['training_time']) / baseline['training_time'] * 100\n",
    "        else:\n",
    "            improvements['training_speed'] = 0.0\n",
    "        \n",
    "        # Convergence speed (lower steps is better)\n",
    "        if baseline['convergence_step'] > 0 and aggressive['convergence_step'] > 0:\n",
    "            improvements['convergence_speed'] = (baseline['convergence_step'] - aggressive['convergence_step']) / baseline['convergence_step'] * 100\n",
    "        else:\n",
    "            improvements['convergence_speed'] = 0.0\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "# Load all results\n",
    "print(\"Loading all task-curriculum results...\")\n",
    "all_results = load_all_task_results()\n",
    "\n",
    "# Process each task separately  \n",
    "task_summaries = {}\n",
    "task_improvements = {}\n",
    "\n",
    "print(\"\\nProcessing task data and calculating improvements...\")\n",
    "for task_key, task_data in all_results.items():\n",
    "    task_summaries[task_key] = extract_task_metrics(task_key, task_data)\n",
    "    task_improvements[task_key] = calculate_improvement_metrics(task_summaries[task_key])\n",
    "    \n",
    "    print(f\"\\n{TASKS[task_key]['name']} Summary:\")\n",
    "    display_cols = ['curriculum_name', 'final_total_loss', 'final_identity_error', 'final_accuracy', 'convergence_step']\n",
    "    display(task_summaries[task_key][display_cols].round(4))\n",
    "    \n",
    "    # Print improvements\n",
    "    improvements = task_improvements[task_key]\n",
    "    print(f\"\\nImprovements (Aggressive vs Baseline):\")\n",
    "    for metric, improvement in improvements.items():\n",
    "        sign = \"+\" if improvement > 0 else \"\"\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {sign}{improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\nLoaded and processed data for {len(task_summaries)} tasks\")\n",
    "print(\"Data loading and processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1wtufOg-j9x"
   },
   "source": [
    "## Task-Specific Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kkTaPFho-j9x",
    "outputId": "581b4dfa-db3a-49dd-9a32-1c3777b540e1"
   },
   "outputs": [],
   "source": [
    "def visualize_task_comparison(task_key: str, task_data: Dict, task_summary: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualizations for a single task.\"\"\"\n",
    "    task_config = TASKS[task_key]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'{task_config[\"name\"]} - Baseline vs Aggressive Curriculum', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Training Loss Curves\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in task_data.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            curriculum_config = CURRICULA[curriculum_key]\n",
    "            # Apply smoothing for better visualization\n",
    "            window = min(10, len(training_df) // 20)\n",
    "            if window > 1:\n",
    "                smoothed_loss = training_df['total_loss'].rolling(window=window, min_periods=1).mean()\n",
    "            else:\n",
    "                smoothed_loss = training_df['total_loss']\n",
    "            \n",
    "            ax.plot(training_df['step'], smoothed_loss,\n",
    "                    color=curriculum_config['color'], label=curriculum_config['name'], \n",
    "                    linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Training Loss Evolution (Smoothed)', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')  # Log scale for better loss visualization\n",
    "    \n",
    "    # 2. Identity Error Evolution (KEY METRIC)\n",
    "    ax = axes[0, 1] \n",
    "    data_found = False\n",
    "    for curriculum_key, data in task_data.items():\n",
    "        validation_df = data.get('validation')\n",
    "        if validation_df is not None:\n",
    "            curriculum_config = CURRICULA[curriculum_key]\n",
    "            identity_df = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "            if not identity_df.empty:\n",
    "                ax.plot(identity_df['step'], identity_df['metric_value'],\n",
    "                        color=curriculum_config['color'], label=curriculum_config['name'],\n",
    "                        linewidth=3, alpha=0.9, marker='o', markersize=4)\n",
    "                data_found = True\n",
    "    \n",
    "    if data_found:\n",
    "        ax.set_title('Identity Error Evolution (Lower = Better)', fontweight='bold')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('||Pred @ Input - I||Â²')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')  # Log scale for identity error\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No identity error data available\\n(Run longer training for validation)', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 3. Final Identity Error Comparison (Bar Chart)\n",
    "    ax = axes[1, 0]\n",
    "    curricula_names = task_summary['curriculum_name'].tolist()\n",
    "    identity_errors = task_summary['final_identity_error'].tolist()\n",
    "    colors = task_summary['color'].tolist()\n",
    "    \n",
    "    # Filter out inf values\n",
    "    valid_data = [(name, error, color) for name, error, color in zip(curricula_names, identity_errors, colors) \n",
    "                  if error != float('inf') and error > 0]\n",
    "    \n",
    "    if valid_data:\n",
    "        names, errors, cols = zip(*valid_data)\n",
    "        bars = ax.bar(names, errors, color=cols, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.set_title('Final Identity Error Comparison', fontweight='bold')\n",
    "        ax.set_ylabel('Identity Error')\n",
    "        ax.set_yscale('log')  # Log scale for better comparison\n",
    "        \n",
    "        # Add improvement annotation\n",
    "        if len(errors) == 2:\n",
    "            baseline_idx = 0 if 'baseline' in names[0].lower() else 1\n",
    "            aggressive_idx = 1 - baseline_idx\n",
    "            \n",
    "            baseline_error = errors[baseline_idx]\n",
    "            aggressive_error = errors[aggressive_idx]\n",
    "            improvement = (baseline_error - aggressive_error) / baseline_error * 100\n",
    "            \n",
    "            color = 'green' if improvement > 0 else 'red'\n",
    "            ax.text(0.5, 0.8, f'Aggressive Improvement: {improvement:+.1f}%', \n",
    "                    ha='center', transform=ax.transAxes, fontweight='bold', fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, error in zip(bars, errors):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n",
    "                    f'{error:.2e}', ha='center', va='bottom', fontweight='bold', rotation=45)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No valid identity error data\\nfor comparison', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error Comparison (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 4. Task-Specific Metric (Accuracy or Loss)\n",
    "    ax = axes[1, 1]\n",
    "    accuracies = task_summary['final_accuracy'].tolist()\n",
    "    \n",
    "    if max(accuracies) > 0:\n",
    "        bars = ax.bar(curricula_names, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.set_title('Final Accuracy Comparison', fontweight='bold')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_ylim(0, max(accuracies) * 1.2)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(accuracies)*0.02,\n",
    "                    f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add improvement annotation for accuracy\n",
    "        if len(accuracies) == 2:\n",
    "            baseline_idx = 0 if 'baseline' in curricula_names[0].lower() else 1\n",
    "            aggressive_idx = 1 - baseline_idx\n",
    "            \n",
    "            if accuracies[baseline_idx] > 0:\n",
    "                acc_improvement = (accuracies[aggressive_idx] - accuracies[baseline_idx]) / accuracies[baseline_idx] * 100\n",
    "                color = 'green' if acc_improvement > 0 else 'red'\n",
    "                ax.text(0.5, 0.9, f'Accuracy Improvement: {acc_improvement:+.1f}%', \n",
    "                        ha='center', transform=ax.transAxes, fontweight='bold',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "    else:\n",
    "        # Show final loss instead\n",
    "        final_losses = task_summary['final_total_loss'].tolist()\n",
    "        if max(final_losses) > 0:\n",
    "            bars = ax.bar(curricula_names, final_losses, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "            ax.set_title('Final Total Loss Comparison', fontweight='bold')\n",
    "            ax.set_ylabel('Total Loss')\n",
    "            ax.set_yscale('log')\n",
    "            \n",
    "            for bar, loss in zip(bars, final_losses):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n",
    "                        f'{loss:.2e}', ha='center', va='bottom', fontweight='bold', rotation=45)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No accuracy or loss data available', ha='center', va='center', \n",
    "                    transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Performance Metrics (No Data)', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_cross_task_comparison(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Compare aggressive curriculum performance across tasks.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Cross-Task Performance: Aggressive vs Baseline Curriculum', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data for comparison\n",
    "    task_names = []\n",
    "    baseline_errors = []\n",
    "    aggressive_errors = []\n",
    "    identity_improvements = []\n",
    "    loss_improvements = []\n",
    "    \n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        task_names.append(TASKS[task_key]['name'])\n",
    "        \n",
    "        baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "        \n",
    "        if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "            baseline_error = baseline_row['final_identity_error'].iloc[0]\n",
    "            aggressive_error = aggressive_row['final_identity_error'].iloc[0]\n",
    "            \n",
    "            baseline_errors.append(baseline_error if baseline_error != float('inf') else None)\n",
    "            aggressive_errors.append(aggressive_error if aggressive_error != float('inf') else None)\n",
    "            \n",
    "            # Get improvements from pre-calculated data\n",
    "            improvements = task_improvements[task_key]\n",
    "            identity_improvements.append(improvements.get('identity_error', 0))\n",
    "            loss_improvements.append(improvements.get('total_loss', 0))\n",
    "        else:\n",
    "            baseline_errors.append(None)\n",
    "            aggressive_errors.append(None)\n",
    "            identity_improvements.append(0)\n",
    "            loss_improvements.append(0)\n",
    "    \n",
    "    # 1. Identity Error Comparison\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(task_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_baseline = [e for e in baseline_errors if e is not None]\n",
    "    valid_aggressive = [e for e in aggressive_errors if e is not None]\n",
    "    valid_names = [name for name, b, a in zip(task_names, baseline_errors, aggressive_errors) \n",
    "                   if b is not None and a is not None]\n",
    "    \n",
    "    if valid_baseline and valid_aggressive:\n",
    "        valid_x = np.arange(len(valid_names))\n",
    "        baseline_bars = ax.bar(valid_x - width/2, valid_baseline, width, label='Baseline', \n",
    "                              color=CURRICULA['baseline']['color'], alpha=0.7, edgecolor='black')\n",
    "        aggressive_bars = ax.bar(valid_x + width/2, valid_aggressive, width, label='Aggressive', \n",
    "                                color=CURRICULA['aggressive']['color'], alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_title('Identity Error by Task (Lower = Better)', fontweight='bold')\n",
    "        ax.set_ylabel('Identity Error')\n",
    "        ax.set_xlabel('Task')\n",
    "        ax.set_xticks(valid_x)\n",
    "        ax.set_xticklabels(valid_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No valid identity error data\\nfor cross-task comparison', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error by Task (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 2. Identity Error Improvement Percentage\n",
    "    ax = axes[0, 1]\n",
    "    valid_improvements = [(name, imp) for name, imp in zip(task_names, identity_improvements) if imp != 0]\n",
    "    \n",
    "    if valid_improvements:\n",
    "        names, improvements = zip(*valid_improvements)\n",
    "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "        bars = ax.bar(names, improvements, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Identity Error Improvement (%)', fontweight='bold')\n",
    "        ax.set_ylabel('Improvement (%)')\n",
    "        ax.set_xlabel('Task')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, imp in zip(bars, improvements):\n",
    "            y_pos = bar.get_height() + (1 if imp > 0 else -3)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                    f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontweight='bold')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No improvement data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error Improvement (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 3. Loss Improvement Comparison\n",
    "    ax = axes[1, 0]\n",
    "    valid_loss_improvements = [(name, imp) for name, imp in zip(task_names, loss_improvements) if imp != 0]\n",
    "    \n",
    "    if valid_loss_improvements:\n",
    "        names, improvements = zip(*valid_loss_improvements)\n",
    "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "        bars = ax.bar(names, improvements, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Total Loss Improvement (%)', fontweight='bold')\n",
    "        ax.set_ylabel('Improvement (%)')\n",
    "        ax.set_xlabel('Task')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, imp in zip(bars, improvements):\n",
    "            y_pos = bar.get_height() + (0.5 if imp > 0 else -1.5)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                    f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontweight='bold')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No loss improvement data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Loss Improvement (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 4. Overall Performance Summary\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Create a summary score for each task\n",
    "    task_scores = []\n",
    "    score_names = []\n",
    "    \n",
    "    for task_key, improvements in task_improvements.items():\n",
    "        if any(imp != 0 for imp in improvements.values()):\n",
    "            # Weighted score: identity error (50%), loss (30%), accuracy (20%)\n",
    "            score = (improvements.get('identity_error', 0) * 0.5 + \n",
    "                    improvements.get('total_loss', 0) * 0.3 + \n",
    "                    improvements.get('accuracy', 0) * 0.2)\n",
    "            task_scores.append(score)\n",
    "            score_names.append(TASKS[task_key]['name'])\n",
    "    \n",
    "    if task_scores:\n",
    "        colors = ['green' if score > 0 else 'red' for score in task_scores]\n",
    "        bars = ax.bar(score_names, task_scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Overall Performance Score\\n(Weighted: Identity 50%, Loss 30%, Accuracy 20%)', fontweight='bold')\n",
    "        ax.set_ylabel('Composite Score (%)')\n",
    "        ax.set_xlabel('Task')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, task_scores):\n",
    "            y_pos = bar.get_height() + (1 if score > 0 else -3)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                    f'{score:+.1f}', ha='center', va='bottom' if score > 0 else 'top', fontweight='bold')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No performance data available\\nfor composite scoring', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Overall Performance Score (No Data)', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations for each task\n",
    "print(\"Generating task-specific visualizations...\")\n",
    "for task_key, task_data in all_results.items():\n",
    "    print(f\"\\nGenerating visualizations for {TASKS[task_key]['name']}...\")\n",
    "    visualize_task_comparison(task_key, task_data, task_summaries[task_key])\n",
    "\n",
    "print(\"\\nGenerating cross-task comparison...\")\n",
    "create_cross_task_comparison(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nAll visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwCctp-A-j9y"
   },
   "source": [
    "## Comprehensive Analysis and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N97tTHix-j9y",
    "outputId": "eb1787d1-fa57-4637-aabf-37893a596711"
   },
   "outputs": [],
   "source": [
    "def generate_comprehensive_analysis(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Generate detailed analysis of curriculum effectiveness across tasks.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE TASK-CURRICULUM ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“Š TASK PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create unified comparison table\n",
    "    all_results = []\n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        for _, row in summary_df.iterrows():\n",
    "            result = {\n",
    "                'Task': TASKS[task_key]['name'],\n",
    "                'Curriculum': row['curriculum_name'],\n",
    "                'Final Loss': f\"{row['final_total_loss']:.4f}\" if row['final_total_loss'] > 0 else 'N/A',\n",
    "                'Identity Error': f\"{row['final_identity_error']:.2e}\" if row['final_identity_error'] != float('inf') else 'N/A',\n",
    "                'Accuracy (%)': f\"{row['final_accuracy']:.1f}\" if row['final_accuracy'] > 0 else 'N/A',\n",
    "                'Training Time (s)': f\"{row['training_time']:.3f}\" if row['training_time'] > 0 else 'N/A',\n",
    "                'Convergence Step': f\"{row['convergence_step']}\" if row['convergence_step'] > 0 else 'N/A'\n",
    "            }\n",
    "            all_results.append(result)\n",
    "    \n",
    "    unified_df = pd.DataFrame(all_results)\n",
    "    print(unified_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ CURRICULUM EFFECTIVENESS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Detailed analysis for each task\n",
    "    total_tasks = 0\n",
    "    successful_identity_improvements = 0\n",
    "    successful_loss_improvements = 0\n",
    "    successful_accuracy_improvements = 0\n",
    "    \n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        task_name = TASKS[task_key]['name']\n",
    "        print(f\"\\n{task_name}:\")\n",
    "        print(f\"  Dataset: {TASKS[task_key]['dataset']}, Model: {TASKS[task_key]['model']}\")\n",
    "        \n",
    "        baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "        \n",
    "        if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "            total_tasks += 1\n",
    "            baseline = baseline_row.iloc[0]\n",
    "            aggressive = aggressive_row.iloc[0]\n",
    "            improvements = task_improvements[task_key]\n",
    "            \n",
    "            # Identity Error Analysis\n",
    "            identity_improvement = improvements.get('identity_error', 0)\n",
    "            if identity_improvement > 0:\n",
    "                successful_identity_improvements += 1\n",
    "                print(f\"  âœ… IDENTITY ERROR: Improved by {identity_improvement:.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_identity_error']:.2e} â†’ Aggressive: {aggressive['final_identity_error']:.2e}\")\n",
    "            elif identity_improvement < 0:\n",
    "                print(f\"  âŒ IDENTITY ERROR: Worsened by {abs(identity_improvement):.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_identity_error']:.2e} â†’ Aggressive: {aggressive['final_identity_error']:.2e}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  IDENTITY ERROR: No valid comparison data\")\n",
    "            \n",
    "            # Loss Analysis\n",
    "            loss_improvement = improvements.get('total_loss', 0)\n",
    "            if loss_improvement > 0:\n",
    "                successful_loss_improvements += 1\n",
    "                print(f\"  âœ… TOTAL LOSS: Improved by {loss_improvement:.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_total_loss']:.4f} â†’ Aggressive: {aggressive['final_total_loss']:.4f}\")\n",
    "            elif loss_improvement < 0:\n",
    "                print(f\"  âŒ TOTAL LOSS: Worsened by {abs(loss_improvement):.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_total_loss']:.4f} â†’ Aggressive: {aggressive['final_total_loss']:.4f}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  TOTAL LOSS: No valid comparison data\")\n",
    "            \n",
    "            # Accuracy Analysis\n",
    "            accuracy_improvement = improvements.get('accuracy', 0)\n",
    "            if accuracy_improvement > 0:\n",
    "                successful_accuracy_improvements += 1\n",
    "                print(f\"  âœ… ACCURACY: Improved by {accuracy_improvement:.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_accuracy']:.1f}% â†’ Aggressive: {aggressive['final_accuracy']:.1f}%\")\n",
    "            elif accuracy_improvement < 0:\n",
    "                print(f\"  âŒ ACCURACY: Worsened by {abs(accuracy_improvement):.1f}%\")\n",
    "                print(f\"     Baseline: {baseline['final_accuracy']:.1f}% â†’ Aggressive: {aggressive['final_accuracy']:.1f}%\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  ACCURACY: No valid comparison data\")\n",
    "            \n",
    "            # Training efficiency\n",
    "            speed_improvement = improvements.get('training_speed', 0)\n",
    "            if speed_improvement != 0:\n",
    "                print(f\"  ðŸƒ TRAINING SPEED: {'Faster' if speed_improvement > 0 else 'Slower'} by {abs(speed_improvement):.1f}%\")\n",
    "            \n",
    "            # Convergence analysis\n",
    "            conv_improvement = improvements.get('convergence_speed', 0)\n",
    "            if conv_improvement != 0:\n",
    "                print(f\"  ðŸŽ¯ CONVERGENCE: {'Faster' if conv_improvement > 0 else 'Slower'} by {abs(conv_improvement):.1f}%\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Missing training results for comparison\")\n",
    "    \n",
    "    # Overall statistical summary\n",
    "    print(\"\\nðŸ† OVERALL STATISTICAL SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if total_tasks > 0:\n",
    "        identity_success_rate = successful_identity_improvements / total_tasks * 100\n",
    "        loss_success_rate = successful_loss_improvements / total_tasks * 100\n",
    "        accuracy_success_rate = successful_accuracy_improvements / total_tasks * 100\n",
    "        \n",
    "        print(f\"ðŸ“ˆ IMPROVEMENT SUCCESS RATES:\")\n",
    "        print(f\"   â€¢ Identity Error: {successful_identity_improvements}/{total_tasks} tasks ({identity_success_rate:.0f}%)\")\n",
    "        print(f\"   â€¢ Total Loss: {successful_loss_improvements}/{total_tasks} tasks ({loss_success_rate:.0f}%)\")\n",
    "        print(f\"   â€¢ Accuracy: {successful_accuracy_improvements}/{total_tasks} tasks ({accuracy_success_rate:.0f}%)\")\n",
    "        \n",
    "        # Calculate average improvements\n",
    "        avg_identity_improvement = np.mean([improvements.get('identity_error', 0) \n",
    "                                          for improvements in task_improvements.values()])\n",
    "        avg_loss_improvement = np.mean([improvements.get('total_loss', 0) \n",
    "                                      for improvements in task_improvements.values()])\n",
    "        avg_accuracy_improvement = np.mean([improvements.get('accuracy', 0) \n",
    "                                          for improvements in task_improvements.values()])\n",
    "        \n",
    "        print(f\"\\nðŸ“Š AVERAGE IMPROVEMENTS:\")\n",
    "        print(f\"   â€¢ Identity Error: {avg_identity_improvement:+.1f}%\")\n",
    "        print(f\"   â€¢ Total Loss: {avg_loss_improvement:+.1f}%\")\n",
    "        print(f\"   â€¢ Accuracy: {avg_accuracy_improvement:+.1f}%\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        print(\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if identity_success_rate >= 50:\n",
    "            print(\"âœ… IDENTITY ERROR: Aggressive curriculum shows promise for IRED reasoning tasks\")\n",
    "        else:\n",
    "            print(\"âŒ IDENTITY ERROR: Baseline training competitive with aggressive curriculum\")\n",
    "        \n",
    "        if loss_success_rate >= 50:\n",
    "            print(\"âœ… TRAINING LOSS: Aggressive curriculum improves optimization\")\n",
    "        else:\n",
    "            print(\"âŒ TRAINING LOSS: Baseline optimization competitive\")\n",
    "        \n",
    "        if accuracy_success_rate >= 50:\n",
    "            print(\"âœ… TASK ACCURACY: Aggressive curriculum enhances task performance\")\n",
    "        else:\n",
    "            print(\"âŒ TASK ACCURACY: Baseline performance competitive\")\n",
    "        \n",
    "        # Final verdict\n",
    "        overall_success_rate = (identity_success_rate + loss_success_rate + accuracy_success_rate) / 3\n",
    "        print(f\"\\nðŸ† OVERALL VERDICT:\")\n",
    "        if overall_success_rate >= 66:\n",
    "            print(f\"   ðŸŽ¯ STRONG RECOMMENDATION: Deploy aggressive curriculum (Success: {overall_success_rate:.0f}%)\")\n",
    "        elif overall_success_rate >= 33:\n",
    "            print(f\"   âš–ï¸  MIXED RESULTS: Task-dependent benefits (Success: {overall_success_rate:.0f}%)\")\n",
    "        else:\n",
    "            print(f\"   ðŸŽ¯ RECOMMENDATION: Stick with baseline training (Success: {overall_success_rate:.0f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸  Insufficient data for overall assessment\")\n",
    "    \n",
    "    # Experimental details\n",
    "    print(f\"\\nðŸ“‹ EXPERIMENTAL SETUP:\")\n",
    "    print(f\"   â€¢ Tasks tested: {', '.join([TASKS[k]['name'] for k in TASKS.keys()])}\")\n",
    "    print(f\"   â€¢ Training steps: {list(TASKS.values())[0]['train_steps']} per task\")\n",
    "    print(f\"   â€¢ Diffusion steps: {COMMON_ARGS['diffusion_steps']}\")\n",
    "    print(f\"   â€¢ Focus metric: Identity error (||Pred @ Input - I||Â²)\")\n",
    "    print(f\"   â€¢ Validation interval: Every {COMMON_ARGS['csv_log_interval']} steps\")\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "def create_performance_radar_chart(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Create radar chart comparing curriculum performance across multiple dimensions.\"\"\"\n",
    "    \n",
    "    # Calculate aggregated metrics across tasks\n",
    "    baseline_metrics = {\n",
    "        'Identity Error': [],\n",
    "        'Total Loss': [],\n",
    "        'Accuracy': [],\n",
    "        'Training Speed': [],\n",
    "        'Convergence Speed': []\n",
    "    }\n",
    "    \n",
    "    aggressive_metrics = {\n",
    "        'Identity Error': [],\n",
    "        'Total Loss': [],\n",
    "        'Accuracy': [],\n",
    "        'Training Speed': [],\n",
    "        'Convergence Speed': []\n",
    "    }\n",
    "    \n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "        \n",
    "        if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "            baseline = baseline_row.iloc[0]\n",
    "            aggressive = aggressive_row.iloc[0]\n",
    "            \n",
    "            # Normalize metrics (convert to 0-1 scale where higher = better)\n",
    "            # Identity Error (lower is better, so invert)\n",
    "            if baseline['final_identity_error'] != float('inf') and aggressive['final_identity_error'] != float('inf'):\n",
    "                max_error = max(baseline['final_identity_error'], aggressive['final_identity_error'])\n",
    "                baseline_metrics['Identity Error'].append(1 - baseline['final_identity_error'] / (max_error + 1e-8))\n",
    "                aggressive_metrics['Identity Error'].append(1 - aggressive['final_identity_error'] / (max_error + 1e-8))\n",
    "            \n",
    "            # Total Loss (lower is better, so invert)\n",
    "            if baseline['final_total_loss'] > 0 and aggressive['final_total_loss'] > 0:\n",
    "                max_loss = max(baseline['final_total_loss'], aggressive['final_total_loss'])\n",
    "                baseline_metrics['Total Loss'].append(1 - baseline['final_total_loss'] / max_loss)\n",
    "                aggressive_metrics['Total Loss'].append(1 - aggressive['final_total_loss'] / max_loss)\n",
    "            \n",
    "            # Accuracy (higher is better)\n",
    "            if baseline['final_accuracy'] > 0 or aggressive['final_accuracy'] > 0:\n",
    "                max_acc = max(baseline['final_accuracy'], aggressive['final_accuracy'])\n",
    "                baseline_metrics['Accuracy'].append(baseline['final_accuracy'] / (max_acc + 1e-8))\n",
    "                aggressive_metrics['Accuracy'].append(aggressive['final_accuracy'] / (max_acc + 1e-8))\n",
    "            \n",
    "            # Training Speed (lower time is better, so invert)\n",
    "            if baseline['training_time'] > 0 and aggressive['training_time'] > 0:\n",
    "                max_time = max(baseline['training_time'], aggressive['training_time'])\n",
    "                baseline_metrics['Training Speed'].append(1 - baseline['training_time'] / max_time)\n",
    "                aggressive_metrics['Training Speed'].append(1 - aggressive['training_time'] / max_time)\n",
    "            \n",
    "            # Convergence Speed (lower steps is better, so invert)\n",
    "            if baseline['convergence_step'] > 0 and aggressive['convergence_step'] > 0:\n",
    "                max_conv = max(baseline['convergence_step'], aggressive['convergence_step'])\n",
    "                baseline_metrics['Convergence Speed'].append(1 - baseline['convergence_step'] / max_conv)\n",
    "                aggressive_metrics['Convergence Speed'].append(1 - aggressive['convergence_step'] / max_conv)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    categories = []\n",
    "    baseline_scores = []\n",
    "    aggressive_scores = []\n",
    "    \n",
    "    for category, values in baseline_metrics.items():\n",
    "        if values and aggressive_metrics[category]:  # Only include if both have data\n",
    "            categories.append(category)\n",
    "            baseline_scores.append(np.mean(values))\n",
    "            aggressive_scores.append(np.mean(aggressive_metrics[category]))\n",
    "    \n",
    "    if len(categories) >= 3:  # Only create radar chart if we have enough metrics\n",
    "        # Create radar chart\n",
    "        N = len(categories)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        ax.set_title('Multi-Dimensional Performance Comparison\\n(Higher = Better)', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Add baseline scores\n",
    "        baseline_values = baseline_scores + baseline_scores[:1]\n",
    "        ax.plot(angles, baseline_values, 'o-', linewidth=3, label='Baseline',\n",
    "                color=CURRICULA['baseline']['color'], alpha=0.8)\n",
    "        ax.fill(angles, baseline_values, alpha=0.15, color=CURRICULA['baseline']['color'])\n",
    "        \n",
    "        # Add aggressive scores\n",
    "        aggressive_values = aggressive_scores + aggressive_scores[:1]\n",
    "        ax.plot(angles, aggressive_values, 'o-', linewidth=3, label='Aggressive',\n",
    "                color=CURRICULA['aggressive']['color'], alpha=0.8)\n",
    "        ax.fill(angles, aggressive_values, alpha=0.15, color=CURRICULA['aggressive']['color'])\n",
    "        \n",
    "        # Customize chart\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories, fontweight='bold')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], alpha=0.7)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient data for radar chart (need at least 3 metrics with valid comparisons)\")\n",
    "\n",
    "# Generate comprehensive analysis\n",
    "print(\"Generating comprehensive analysis...\")\n",
    "unified_results = generate_comprehensive_analysis(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nGenerating performance radar chart...\")\n",
    "create_performance_radar_chart(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nComprehensive analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa1Jektc-j9y"
   },
   "source": [
    "## Results Export and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yait6iFw-j9y",
    "outputId": "66eaa819-6062-407a-c056-bc8b9a1adcd5"
   },
   "outputs": [],
   "source": [
    "# Export comprehensive results\n",
    "output_dir = Path('./ired_task_curriculum_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save task-specific summaries\n",
    "for task_key, summary_df in task_summaries.items():\n",
    "    summary_df.to_csv(output_dir / f'{task_key}_detailed_summary.csv', index=False)\n",
    "\n",
    "# Save unified results\n",
    "unified_results.to_csv(output_dir / 'unified_task_curriculum_results.csv', index=False)\n",
    "\n",
    "# Save improvements summary\n",
    "improvements_data = []\n",
    "for task_key, improvements in task_improvements.items():\n",
    "    for metric, value in improvements.items():\n",
    "        improvements_data.append({\n",
    "            'Task': TASKS[task_key]['name'],\n",
    "            'Task_Key': task_key,\n",
    "            'Metric': metric.replace('_', ' ').title(),\n",
    "            'Improvement_Percent': value\n",
    "        })\n",
    "\n",
    "improvements_df = pd.DataFrame(improvements_data)\n",
    "improvements_df.to_csv(output_dir / 'curriculum_improvements_summary.csv', index=False)\n",
    "\n",
    "# Create training results summary\n",
    "training_summary = []\n",
    "for task_key, task_results in training_results.items():\n",
    "    for curriculum_key, result_code in task_results.items():\n",
    "        training_summary.append({\n",
    "            'Task': TASKS[task_key]['name'],\n",
    "            'Task_Key': task_key,\n",
    "            'Curriculum': CURRICULA[curriculum_key]['name'],\n",
    "            'Curriculum_Key': curriculum_key,\n",
    "            'Exit_Code': result_code,\n",
    "            'Success': result_code == 0,\n",
    "            'Training_Steps': TASKS[task_key]['train_steps'],\n",
    "            'Model': TASKS[task_key]['model'],\n",
    "            'Dataset': TASKS[task_key]['dataset']\n",
    "        })\n",
    "\n",
    "training_df = pd.DataFrame(training_summary)\n",
    "training_df.to_csv(output_dir / 'training_execution_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS EXPORT AND FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“ Results saved to: {output_dir}\")\n",
    "print(\"   ðŸ“Š Data Files:\")\n",
    "for task_key in TASKS.keys():\n",
    "    print(f\"     â€¢ {task_key}_detailed_summary.csv - {TASKS[task_key]['name']} detailed metrics\")\n",
    "print(f\"     â€¢ unified_task_curriculum_results.csv - Combined results across all tasks\")\n",
    "print(f\"     â€¢ curriculum_improvements_summary.csv - Improvement percentages by metric\")\n",
    "print(f\"     â€¢ training_execution_summary.csv - Training execution status\")\n",
    "\n",
    "# Generate final executive summary\n",
    "successful_tasks = sum(1 for task_results in training_results.values() \n",
    "                      if all(r == 0 for r in task_results.values()))\n",
    "total_experiments = sum(len(task_results) for task_results in training_results.values())\n",
    "successful_experiments = sum(sum(1 for r in task_results.values() if r == 0) \n",
    "                           for task_results in training_results.values())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ§  IRED TASK CURRICULUM ANALYSIS - EXECUTIVE SUMMARY ðŸ§ \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EXPERIMENTAL OVERVIEW:\")\n",
    "print(f\"   â€¢ Tasks evaluated: {len(TASKS)} IRED reasoning tasks\")\n",
    "print(f\"   â€¢ Curricula compared: {len(CURRICULA)} approaches (Baseline vs Aggressive)\")\n",
    "print(f\"   â€¢ Total experiments: {total_experiments}\")\n",
    "print(f\"   â€¢ Successful completions: {successful_experiments}/{total_experiments} ({successful_experiments/total_experiments*100:.1f}%)\")\n",
    "print(f\"   â€¢ Fully completed tasks: {successful_tasks}/{len(TASKS)}\")\n",
    "\n",
    "# Calculate key findings\n",
    "identity_improvements = [task_improvements[task]['identity_error'] \n",
    "                        for task in task_improvements.keys() \n",
    "                        if task_improvements[task]['identity_error'] != 0]\n",
    "\n",
    "loss_improvements = [task_improvements[task]['total_loss'] \n",
    "                    for task in task_improvements.keys() \n",
    "                    if task_improvements[task]['total_loss'] != 0]\n",
    "\n",
    "if identity_improvements:\n",
    "    avg_identity_improvement = np.mean(identity_improvements)\n",
    "    positive_identity_improvements = sum(1 for imp in identity_improvements if imp > 0)\n",
    "    print(f\"\\nðŸ” IDENTITY ERROR FINDINGS:\")\n",
    "    print(f\"   â€¢ Average improvement: {avg_identity_improvement:+.1f}%\")\n",
    "    print(f\"   â€¢ Tasks with improvement: {positive_identity_improvements}/{len(identity_improvements)}\")\n",
    "    print(f\"   â€¢ Success rate: {positive_identity_improvements/len(identity_improvements)*100:.1f}%\")\n",
    "\n",
    "if loss_improvements:\n",
    "    avg_loss_improvement = np.mean(loss_improvements)\n",
    "    positive_loss_improvements = sum(1 for imp in loss_improvements if imp > 0)\n",
    "    print(f\"\\nðŸ“‰ TRAINING LOSS FINDINGS:\")\n",
    "    print(f\"   â€¢ Average improvement: {avg_loss_improvement:+.1f}%\")\n",
    "    print(f\"   â€¢ Tasks with improvement: {positive_loss_improvements}/{len(loss_improvements)}\")\n",
    "    print(f\"   â€¢ Success rate: {positive_loss_improvements/len(loss_improvements)*100:.1f}%\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nðŸ† FINAL RECOMMENDATION:\")\n",
    "if identity_improvements and len(identity_improvements) > 0:\n",
    "    identity_success_rate = sum(1 for imp in identity_improvements if imp > 0) / len(identity_improvements)\n",
    "    if identity_success_rate >= 0.5:\n",
    "        print(f\"   âœ… DEPLOY AGGRESSIVE CURRICULUM: Shows consistent identity error improvements\")\n",
    "        print(f\"      Success rate: {identity_success_rate*100:.0f}% of tasks improved\")\n",
    "    else:\n",
    "        print(f\"   âŒ STICK WITH BASELINE: Aggressive curriculum does not consistently improve performance\")\n",
    "        print(f\"      Success rate: {identity_success_rate*100:.0f}% of tasks improved\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  INCONCLUSIVE: Insufficient data for reliable recommendation\")\n",
    "    print(f\"      Consider longer training or additional validation metrics\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ TECHNICAL DETAILS:\")\n",
    "print(f\"   â€¢ Training steps per task: {list(TASKS.values())[0]['train_steps']}\")\n",
    "print(f\"   â€¢ Diffusion steps: {COMMON_ARGS['diffusion_steps']}\")\n",
    "print(f\"   â€¢ Validation interval: Every {COMMON_ARGS['csv_log_interval']} steps\")\n",
    "print(f\"   â€¢ Key metric: Identity error ||Pred @ Input - I||Â²\")\n",
    "print(f\"   â€¢ Models tested: {', '.join(set(task['model'] for task in TASKS.values()))}\")\n",
    "\n",
    "print(f\"\\nðŸ“š TASKS EVALUATED:\")\n",
    "for task_key, task_config in TASKS.items():\n",
    "    success = all(training_results[task_key][curr] == 0 for curr in CURRICULA.keys())\n",
    "    status = \"âœ…\" if success else \"âŒ\"\n",
    "    print(f\"   {status} {task_config['name']}: {task_config['description']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ IRED TASK CURRICULUM COMPARISON COMPLETED SUCCESSFULLY! ðŸš€\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ’¡ NEXT STEPS:\")\n",
    "print(f\"   1. Review detailed task-specific visualizations above\")\n",
    "print(f\"   2. Examine CSV files for numerical analysis\")\n",
    "print(f\"   3. Consider extending to OOD (out-of-distribution) versions if aggressive shows promise\")\n",
    "print(f\"   4. Validate findings with longer training runs if needed\")\n",
    "print(f\"   5. Test on additional IRED tasks if curriculum shows clear benefits\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
