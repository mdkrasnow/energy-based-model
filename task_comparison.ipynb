{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Final Consolidated Summary\n\ndef generate_final_summary_report(summary_df: pd.DataFrame, all_results: Dict):\n    \"\"\"Generate a final consolidated summary report of all task and curriculum results.\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üî¨ FINAL CONSOLIDATED SUMMARY REPORT\")\n    print(\"=\"*80)\n    \n    print(\"\\nüìä EXPERIMENTAL OVERVIEW:\")\n    print(f\"   ‚Ä¢ Total tasks evaluated: {len(TASKS)}\")\n    print(f\"   ‚Ä¢ Total curricula compared: {len(CURRICULA)}\")\n    print(f\"   ‚Ä¢ Total experimental conditions: {len(TASKS) * len(CURRICULA)}\")\n    \n    # Task performance overview\n    print(\"\\nüéØ TASK PERFORMANCE OVERVIEW:\")\n    for task_key, config in TASKS.items():\n        print(f\"   ‚Ä¢ {config['name']}: {config['description']}\")\n        print(f\"     Model: {config['model']}, Training Steps: {config['train_steps']}\")\n    \n    # Curriculum comparison overview\n    print(\"\\nüìà CURRICULUM COMPARISON OVERVIEW:\")\n    for curriculum_key, config in CURRICULA.items():\n        print(f\"   ‚Ä¢ {config['name']}: {config['description']}\")\n    \n    # Summary statistics if data is available\n    if not summary_df.empty:\n        print(\"\\nüìã PERFORMANCE SUMMARY:\")\n        print(summary_df.to_string(index=False))\n    else:\n        print(\"\\n‚ö†Ô∏è  No performance data available for consolidated summary\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìù FINAL CONSOLIDATED SUMMARY COMPLETED\")\n    print(\"=\"*80)\n\n# Generate the final consolidated summary if data is available\nif 'task_summaries' in locals() and task_summaries:\n    # Combine all task summaries into one DataFrame\n    all_summaries = []\n    for task_key, summary_df in task_summaries.items():\n        summary_copy = summary_df.copy()\n        summary_copy['task'] = task_key\n        all_summaries.append(summary_copy)\n    \n    if all_summaries:\n        consolidated_df = pd.concat(all_summaries, ignore_index=True)\n        generate_final_summary_report(consolidated_df, all_results)\n    else:\n        print(\"No task summaries available for consolidated report\")\nelse:\n    print(\"Task summaries not yet generated - run previous cells first\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def visualize_task_curriculum_comparison(all_results: Dict, summary_df: pd.DataFrame, task: str):\n    \"\"\"Create comprehensive visualizations comparing curriculum approaches for a specific task.\"\"\"\n    pass",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# IRED-Style Results Table Generation\n\ndef generate_ired_style_table(summary_df: pd.DataFrame, task: str):\n    \"\"\"Generate IRED-style results table for task comparison.\"\"\"\n    \n    # Create formatted results table matching IRED paper style\n    results_table = []\n    \n    for _, row in summary_df.iterrows():\n        curriculum_name = row['curriculum_name']\n        final_accuracy = row.get('final_accuracy', 0.0)\n        best_accuracy = row.get('best_accuracy', 0.0)\n        final_loss = row.get('final_total_loss', 0.0)\n        \n        results_table.append({\n            'Method': curriculum_name,\n            'Task': task,\n            'Final Acc (%)': f\"{final_accuracy:.2f}\",\n            'Best Acc (%)': f\"{best_accuracy:.2f}\",\n            'Final Loss': f\"{final_loss:.4f}\"\n        })\n    \n    # Convert to DataFrame for nice formatting\n    results_df = pd.DataFrame(results_table)\n    \n    print(f\"\\\\n=== IRED-Style Results Table for {task} ===\")\n    print(results_df.to_string(index=False))\n    \n    return results_df\n\n# Generate IRED-style tables for all tasks\nprint(\"Generating IRED-style results tables...\")\nfor task_key, summary_df in task_summaries.items():\n    task_name = TASKS[task_key]['name']\n    generate_ired_style_table(summary_df, task_name)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def generate_task_ranking(summary_df: pd.DataFrame):\n    \"\"\"Generate task ranking based on performance metrics.\"\"\"\n    \n    # Create ranking based on accuracy\n    ranking_data = []\n    \n    for _, row in summary_df.iterrows():\n        curriculum_name = row['curriculum_name']\n        final_accuracy = row.get('final_accuracy', 0.0)\n        best_accuracy = row.get('best_accuracy', 0.0)\n        final_loss = row.get('final_total_loss', 0.0)\n        \n        ranking_data.append({\n            'Curriculum': curriculum_name,\n            'Final Accuracy (%)': final_accuracy,\n            'Best Accuracy (%)': best_accuracy,\n            'Final Loss': final_loss,\n            'Score': final_accuracy  # Simple ranking by final accuracy\n        })\n    \n    # Sort by score (descending - higher accuracy is better)\n    ranking_df = pd.DataFrame(ranking_data)\n    ranking_df = ranking_df.sort_values('Score', ascending=False)\n    \n    print(\"\\\\nTask Performance Ranking:\")\n    print(ranking_df.to_string(index=False))\n    \n    return ranking_df",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzh7-Ram-j9r"
   },
   "source": [
    "# IRED Tasks: Baseline vs Aggressive Curriculum Comparison\n",
    "\n",
    "Comparative analysis of baseline training vs aggressive adversarial curriculum on core IRED reasoning tasks:\n",
    "- **Connectivity**: Graph connectivity reasoning (12x12 graphs, 0.1 edge probability)\n",
    "- **Sudoku**: Sudoku completion task (standard difficulty)\n",
    "\n",
    "This notebook evaluates whether aggressive curriculum learning improves identity error performance within limited training steps for reasoning tasks.\n",
    "\n",
    "**Key Research Question**: Does aggressive curriculum learning outperform baseline training on identity error metrics for IRED reasoning tasks?\n",
    "\n",
    "**Tasks Evaluated**: Easier versions (non-OOD) of connectivity and sudoku tasks as described in IRED paper by Yilun Du."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttlLgjdR-j9t",
    "outputId": "d4d35e3f-d98e-46d6-e01a-2a91434ecad3"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "%cd energy-based-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data\n",
    "!chmod +x download-rrn.sh\n",
    "!chmod +x download-satnet.sh\n",
    "!bash download-satnet.sh\n",
    "!bash download-rrn.sh\n",
    "%cd ..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUU1Z1rj-j9u"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAmsGBDQ-j9v"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZLi5DGT-j9v"
   },
   "source": [
    "## Task and Curriculum Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0MwXmDg-j9v",
    "outputId": "0f2b9706-161e-4bbc-bb09-6106e9b278cc"
   },
   "outputs": [],
   "source": "# Task-specific configurations based on IRED paper\nTASKS = {\n    'connectivity': {\n        'name': 'Graph Connectivity',\n        'description': 'Graph connectivity reasoning (12x12, p=0.1)',\n        'dataset': 'connectivity',\n        'model': 'gnn-conv-1d-v2',\n        'batch_size': 64,  # From train.py validation_batch_size\n        'train_steps': 600,  # Longer for reasoning tasks\n        'validation_interval': 200,  # Validate every 200 steps (milestones 1, 2, 3)\n        'extra_args': [],\n        'color': '#2E8B57',\n        'expected_metrics': ['accuracy', 'balanced_accuracy', 'precision', 'recall'],\n        'note': 'Fixed: Now uses proper validation dataset instead of training samples'\n    },\n    'sudoku': {\n        'name': 'Sudoku Completion', \n        'description': 'Sudoku puzzle completion task',\n        'dataset': 'sudoku',\n        'model': 'sudoku',\n        'batch_size': 64,  # From train.py validation_batch_size  \n        'train_steps': 600,\n        'validation_interval': 200,  # Validate every 200 steps (milestones 1, 2, 3)\n        'extra_args': ['--cond_mask', 'True'],  # Required for sudoku\n        'color': '#4169E1',\n        'expected_metrics': ['accuracy', 'consistency', 'board_accuracy']\n    }\n}\n\n# Only 2 curricula to compare: baseline vs aggressive\nCURRICULA = {\n    'baseline': {\n        'name': 'Baseline',\n        'description': 'No curriculum learning',\n        'args': ['--disable-curriculum', 'True'],\n        'color': '#1f77b4'\n    },\n    'aggressive': {\n        'name': 'Aggressive Curriculum', \n        'description': 'Rapid adversarial escalation',\n        'args': ['--curriculum-config', 'aggressive'],\n        'color': '#d62728'\n    }\n}\n\n# Common training parameters - Enhanced for validation tracking\nCOMMON_ARGS = {\n    'diffusion_steps': 10,\n    'supervise_energy_landscape': 'True',\n    'save_csv_logs': True,\n    'csv_log_interval': 10,  # Log training metrics every 10 steps\n}\n\nprint(f\"Testing {len(TASKS)} IRED reasoning tasks:\")\nfor key, config in TASKS.items():\n    print(f\"  ‚Ä¢ {config['name']}: {config['description']}\")\n    print(f\"    Model: {config['model']}, Steps: {config['train_steps']}\")\n    print(f\"    Validation every {config['validation_interval']} steps (3 evaluations total)\")\n    print(f\"    Expected metrics: {', '.join(config['expected_metrics'])}\")\n    if 'note' in config:\n        print(f\"    NOTE: {config['note']}\")\n\nprint(f\"\\nComparing {len(CURRICULA)} curriculum approaches:\")\nfor key, config in CURRICULA.items():\n    print(f\"  ‚Ä¢ {config['name']}: {config['description']}\")\n    \nprint(\"\\n‚ö†Ô∏è  VALIDATION FIX APPLIED:\")\nprint(\"  ‚Ä¢ Connectivity now uses proper validation dataset (not training samples)\")\nprint(\"  ‚Ä¢ Extra validation datasets (13x13, 15x15, etc.) run every 2 milestones\")\nprint(\"  ‚Ä¢ Output now displays 'Train Sample Evaluation' vs 'Validation' correctly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHwwUrR-j9w"
   },
   "source": [
    "## Training Command Builder and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBg0XbEp-j9w"
   },
   "outputs": [],
   "source": [
    "def build_task_training_command(task_key: str, curriculum_key: str) -> str:\n",
    "    \"\"\"Build training command for a specific task-curriculum combination with validation tracking.\"\"\"\n",
    "    task = TASKS[task_key]\n",
    "    curriculum = CURRICULA[curriculum_key]\n",
    "\n",
    "    # Set save_and_sample_every based on validation interval\n",
    "    save_and_sample_every = task.get('validation_interval', 10)\n",
    "\n",
    "    base_cmd = f\"\"\"python train.py \\\n",
    "        --dataset {task['dataset']} \\\n",
    "        --model {task['model']} \\\n",
    "        --batch_size {task['batch_size']} \\\n",
    "        --diffusion_steps {COMMON_ARGS['diffusion_steps']} \\\n",
    "        --supervise-energy-landscape {COMMON_ARGS['supervise_energy_landscape']} \\\n",
    "        --train-num-steps {task['train_steps']} \\\n",
    "        --save-csv-logs \\\n",
    "        --csv-log-interval {COMMON_ARGS['csv_log_interval']} \\\n",
    "        --csv-log-dir ./csv_logs_{task_key}_{curriculum_key} \"\"\" \n",
    "\n",
    "    # Add task-specific arguments\n",
    "    if task['extra_args']:\n",
    "        base_cmd += ' \\\\\\n        ' + ' \\\\\\n        '.join(task['extra_args'])\n",
    "\n",
    "    # Add curriculum arguments\n",
    "    if curriculum['args']:\n",
    "        base_cmd += ' \\\\\\n        ' + ' \\\\\\n        '.join(curriculum['args'])\n",
    "\n",
    "    return base_cmd\n",
    "\n",
    "def load_csv_data(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV data with error handling.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Debug: show columns and sample data\n",
    "            if len(df) > 0:\n",
    "                print(f\"  Loaded {csv_path.name}: {len(df)} rows, columns: {list(df.columns)[:5]}...\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: {csv_path} not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_accuracy_from_validation_csv(df: pd.DataFrame, task_key: str) -> Dict[str, List[Tuple[int, float]]]:\n",
    "    \"\"\"Extract accuracy metrics from validation CSV data.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return {}\n",
    "\n",
    "    metrics = {}\n",
    "    expected_metrics = TASKS[task_key].get('expected_metrics', ['accuracy'])\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_cols = ['step', 'metric_name', 'metric_value']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Warning: Missing required columns in validation CSV. Found: {df.columns.tolist()}\")\n",
    "        return metrics\n",
    "\n",
    "    # Extract each expected metric\n",
    "    for metric_name in expected_metrics:\n",
    "        metric_data = df[df['metric_name'] == metric_name].copy()\n",
    "        if not metric_data.empty:\n",
    "            # Sort by step and get (step, value) pairs\n",
    "            metric_data = metric_data.sort_values('step')\n",
    "            metrics[metric_name] = list(zip(metric_data['step'], metric_data['metric_value']))\n",
    "        else:\n",
    "            # Try alternative names (e.g., 'accuracy' might be stored as 'acc')\n",
    "            alt_names = {\n",
    "                'board_accuracy': ['board_acc', 'complete_boards'],\n",
    "                'balanced_accuracy': ['bal_acc', 'balanced_acc']\n",
    "            }\n",
    "            for alt_name in alt_names.get(metric_name, []):\n",
    "                metric_data = df[df['metric_name'] == alt_name].copy()\n",
    "                if not metric_data.empty:\n",
    "                    metric_data = metric_data.sort_values('step')\n",
    "                    metrics[metric_name] = list(zip(metric_data['step'], metric_data['metric_value']))\n",
    "                    break\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def safe_get_final_value(df: pd.DataFrame, column: str, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the final value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].iloc[-1])\n",
    "\n",
    "def safe_get_best_value(df: pd.DataFrame, column: str, minimize: bool = True, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the best (min/max) value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].min() if minimize else df[column].max())\n",
    "\n",
    "def safe_get_accuracy_metrics(metrics_dict: Dict[str, List[Tuple[int, float]]]) -> Dict[str, float]:\n",
    "    \"\"\"Extract final and best accuracy values from metrics dictionary.\"\"\"\n",
    "    results = {}\n",
    "    for metric_name, step_value_pairs in metrics_dict.items():\n",
    "        if step_value_pairs:\n",
    "            values = [v for _, v in step_value_pairs]\n",
    "            results[f'{metric_name}_final'] = values[-1]\n",
    "            results[f'{metric_name}_best'] = max(values)  # Accuracy metrics should be maximized\n",
    "            results[f'{metric_name}_steps'] = len(values)\n",
    "    return results\n",
    "\n",
    "def calculate_convergence_step(df: pd.DataFrame, column: str, threshold_pct: float = 0.9) -> int:\n",
    "    \"\"\"Calculate step where metric reaches threshold percentage of final improvement.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) < 5:\n",
    "        return -1\n",
    "\n",
    "    initial_val = df[column].iloc[:5].mean()\n",
    "    final_val = df[column].iloc[-5:].mean()\n",
    "    target_val = initial_val - threshold_pct * (initial_val - final_val)\n",
    "\n",
    "    below_target = df[df[column] <= target_val]\n",
    "    if not below_target.empty:\n",
    "        return int(below_target['step'].iloc[0])\n",
    "    else:\n",
    "        return int(df['step'].iloc[-1])\n",
    "\n",
    "print(\"Utility functions defined successfully!\")\n",
    "print(\"\\nExample command structure:\")\n",
    "print(build_task_training_command('connectivity', 'baseline')[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i95hNsVz-j9w"
   },
   "source": [
    "## Multi-Task Multi-Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i95hNsVz-j9w",
    "outputId": "2366f057-5bd2-43a1-9267-ef9745a36b8d"
   },
   "outputs": [],
   "source": "# Train all tasks with all curriculum configurations\nimport subprocess\nimport sys\n\ntraining_results = {}\n\n# Train each task\nfor task_key, task_config in TASKS.items():\n    print(f\"\\n{'='*80}\")\n    print(f\"TRAINING TASK: {task_config['name']}\")\n    print(f\"Description: {task_config['description']}\")\n    print(f\"Dataset: {task_config['dataset']}, Model: {task_config['model']}\")\n    print(f\"Training Steps: {task_config['train_steps']}, Batch Size: {task_config['batch_size']}\")\n    print(f\"{'='*80}\")\n    \n    task_results = {}\n    \n    for curriculum_key, curriculum_config in CURRICULA.items():\n        print(f\"\\n{'-'*60}\")\n        print(f\"Starting {task_config['name']} with {curriculum_config['name']}\")\n        print(f\"Curriculum Description: {curriculum_config['description']}\")\n        print(f\"{'-'*60}\")\n        \n        # Build training command\n        cmd = build_task_training_command(task_key, curriculum_key)\n        print(f\"\\nCommand: {cmd}\")\n        print(\"\\nTraining output:\")\n        print(\"-\" * 40)\n        \n        # Execute training with real-time output\n        try:\n            # Use subprocess to capture and display output in real-time\n            process = subprocess.Popen(\n                cmd,\n                shell=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n            \n            # Wait for process to complete\n            result = process.wait()\n            task_results[curriculum_key] = result\n            \n            if result == 0:\n                print(f\"‚úì {task_config['name']} with {curriculum_config['name']} completed successfully\")\n            else:\n                print(f\"‚úó {task_config['name']} with {curriculum_config['name']} failed with exit code {result}\")\n                \n        except Exception as e:\n            print(f\"‚úó Error during {task_config['name']} with {curriculum_config['name']} training: {e}\")\n            task_results[curriculum_key] = -1\n            \n        print(\"-\" * 40)\n    \n    training_results[task_key] = task_results\n\nprint(f\"\\n{'='*80}\")\nprint(\"ALL TASK TRAINING COMPLETED!\")\nprint(f\"{'='*80}\")\n\n# Print comprehensive summary\nprint(\"\\nTRAINING SUMMARY:\")\nprint(\"=\" * 40)\ntotal_successful = 0\ntotal_experiments = 0\n\nfor task_key, task_results in training_results.items():\n    task_name = TASKS[task_key]['name']\n    successful = sum(1 for result in task_results.values() if result == 0)\n    total = len(task_results)\n    total_successful += successful\n    total_experiments += total\n    \n    print(f\"\\n{task_name}: {successful}/{total} completed successfully\")\n    for curriculum_key, result in task_results.items():\n        curriculum_name = CURRICULA[curriculum_key]['name']\n        status = \"‚úì Success\" if result == 0 else \"‚úó Failed\"\n        print(f\"  {curriculum_name}: {status}\")\n\nsuccess_rate = (total_successful / total_experiments * 100) if total_experiments > 0 else 0\nprint(f\"\\nOVERALL SUCCESS RATE: {total_successful}/{total_experiments} ({success_rate:.1f}%)\")\n\nif success_rate >= 75:\n    print(\"üéØ Training pipeline working well! Proceeding to analysis...\")\nelif success_rate >= 50:\n    print(\"‚ö†Ô∏è Some training issues detected, but proceeding with available data...\")\nelse:\n    print(\"‚ùå Significant training issues detected. Check configurations and logs.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNqMLoV_-j9x"
   },
   "source": [
    "## Task-Specific Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "source": "def load_all_curriculum_results() -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n    \"\"\"Load curriculum results organized by task, then curriculum, then data type.\"\"\"\n    results = {}\n    \n    for task_key in TASKS.keys():\n        print(f\"Loading {task_key} curriculum results...\")\n        task_data = {}\n        \n        for curriculum_key in CURRICULA.keys():\n            csv_dir = Path(f\"./csv_logs_{task_key}_{curriculum_key}\")\n            \n            curriculum_data = {}\n            patterns = {\n                'training': 'training_metrics_*.csv',\n                'validation': 'validation_metrics_*.csv', \n                'energy': 'energy_metrics_*.csv',\n                'curriculum': 'curriculum_metrics_*.csv',\n                'robustness': 'robustness_metrics_*.csv'\n            }\n            \n            for key, pattern in patterns.items():\n                files = glob.glob(str(csv_dir / pattern))\n                if files:\n                    # Get most recent file\n                    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n                    curriculum_data[key] = load_csv_data(Path(latest_file))\n                else:\n                    curriculum_data[key] = None\n            \n            available = sum(1 for df in curriculum_data.values() if df is not None)\n            print(f\"  {curriculum_key}: {available}/5 data files found\")\n            task_data[curriculum_key] = curriculum_data\n        \n        results[task_key] = task_data\n    \n    return results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "YNPgrc2A-j9x",
    "outputId": "31e5a93b-e184-4817-f100-a0d58cd84c50"
   },
   "outputs": [],
   "source": [
    "def load_all_task_results() -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Load results organized by task, then curriculum, then data type.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for task_key in TASKS.keys():\n",
    "        print(f\"Loading {task_key} results...\")\n",
    "        task_data = {}\n",
    "        \n",
    "        for curriculum_key in CURRICULA.keys():\n",
    "            csv_dir = Path(f\"./csv_logs_{task_key}_{curriculum_key}\")\n",
    "            \n",
    "            curriculum_data = {}\n",
    "            patterns = {\n",
    "                'training': 'training_metrics_*.csv',\n",
    "                'validation': 'validation_metrics_*.csv', \n",
    "                'energy': 'energy_metrics_*.csv',\n",
    "                'curriculum': 'curriculum_metrics_*.csv',\n",
    "                'robustness': 'robustness_metrics_*.csv'\n",
    "            }\n",
    "            \n",
    "            for key, pattern in patterns.items():\n",
    "                files = glob.glob(str(csv_dir / pattern))\n",
    "                if files:\n",
    "                    # Get most recent file\n",
    "                    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n",
    "                    curriculum_data[key] = load_csv_data(Path(latest_file))\n",
    "                else:\n",
    "                    curriculum_data[key] = None\n",
    "            \n",
    "            available = sum(1 for df in curriculum_data.values() if df is not None)\n",
    "            print(f\"  {curriculum_key}: {available}/5 data files found\")\n",
    "            task_data[curriculum_key] = curriculum_data\n",
    "        \n",
    "        results[task_key] = task_data\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_task_metrics(task_key: str, task_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract key metrics for a specific task including accuracy metrics.\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for curriculum_key, data in task_data.items():\n",
    "        curriculum_config = CURRICULA[curriculum_key]\n",
    "        training_df = data.get('training')\n",
    "        validation_df = data.get('validation') \n",
    "        energy_df = data.get('energy')\n",
    "        \n",
    "        # Extract accuracy metrics from validation CSV\n",
    "        accuracy_metrics = extract_accuracy_from_validation_csv(validation_df, task_key)\n",
    "        accuracy_summary = safe_get_accuracy_metrics(accuracy_metrics)\n",
    "        \n",
    "        # Get main accuracy metric (first in expected_metrics list)\n",
    "        main_metric = TASKS[task_key]['expected_metrics'][0]\n",
    "        final_accuracy = accuracy_summary.get(f'{main_metric}_final', 0.0) * 100  # Convert to percentage\n",
    "        best_accuracy = accuracy_summary.get(f'{main_metric}_best', 0.0) * 100\n",
    "        \n",
    "        # Calculate convergence step for total loss\n",
    "        convergence_step = -1\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            convergence_step = calculate_convergence_step(training_df, 'total_loss')\n",
    "        \n",
    "        # Calculate accuracy convergence (step where 90% of final accuracy is reached)\n",
    "        accuracy_convergence_step = -1\n",
    "        if accuracy_metrics.get(main_metric):\n",
    "            steps, values = zip(*accuracy_metrics[main_metric])\n",
    "            if len(values) >= 5:\n",
    "                initial_acc = np.mean(values[:5])\n",
    "                final_acc = values[-1]\n",
    "                target_acc = initial_acc + 0.9 * (final_acc - initial_acc)\n",
    "                for i, (step, value) in enumerate(accuracy_metrics[main_metric]):\n",
    "                    if value >= target_acc:\n",
    "                        accuracy_convergence_step = step\n",
    "                        break\n",
    "        \n",
    "        metrics = {\n",
    "            'task': task_key,\n",
    "            'task_name': TASKS[task_key]['name'],\n",
    "            'curriculum': curriculum_key,\n",
    "            'curriculum_name': curriculum_config['name'],\n",
    "            'color': curriculum_config['color'],\n",
    "            \n",
    "            # Training metrics\n",
    "            'final_total_loss': safe_get_final_value(training_df, 'total_loss'),\n",
    "            'best_total_loss': safe_get_best_value(training_df, 'total_loss', minimize=True),\n",
    "            'final_energy_loss': safe_get_final_value(training_df, 'loss_energy'),\n",
    "            'final_denoise_loss': safe_get_final_value(training_df, 'loss_denoise'),\n",
    "            \n",
    "            # Key accuracy metrics\n",
    "            'final_accuracy': final_accuracy,\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'accuracy_convergence_step': accuracy_convergence_step,\n",
    "            \n",
    "            # Additional task-specific metrics\n",
    "            **{f'{k}': v * 100 for k, v in accuracy_summary.items() if k.endswith('_final')},\n",
    "            \n",
    "            # Performance metrics\n",
    "            'training_time': training_df['nn_time'].mean() if training_df is not None and 'nn_time' in training_df.columns else 0.0,\n",
    "            'convergence_step': convergence_step,\n",
    "            'validation_samples': accuracy_summary.get(f'{main_metric}_steps', 0),\n",
    "            \n",
    "            # Energy landscape metrics\n",
    "            'final_energy_margin': safe_get_final_value(energy_df, 'energy_margin'),\n",
    "            'curriculum_weight': safe_get_best_value(energy_df, 'curriculum_weight', minimize=False) if energy_df is not None and 'curriculum_weight' in energy_df.columns else 0.0\n",
    "        }\n",
    "        \n",
    "        # Store accuracy time series for plotting\n",
    "        metrics['accuracy_history'] = accuracy_metrics.get(main_metric, [])\n",
    "        \n",
    "        processed_data.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "def calculate_improvement_metrics(task_summary: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate improvement metrics for aggressive vs baseline.\"\"\"\n",
    "    baseline_row = task_summary[task_summary['curriculum'] == 'baseline']\n",
    "    aggressive_row = task_summary[task_summary['curriculum'] == 'aggressive']\n",
    "    \n",
    "    improvements = {}\n",
    "    \n",
    "    if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "        baseline = baseline_row.iloc[0]\n",
    "        aggressive = aggressive_row.iloc[0]\n",
    "        \n",
    "        # Loss improvement (lower is better)\n",
    "        if baseline['final_total_loss'] > 0 and aggressive['final_total_loss'] > 0:\n",
    "            improvements['total_loss'] = (baseline['final_total_loss'] - aggressive['final_total_loss']) / baseline['final_total_loss'] * 100\n",
    "        else:\n",
    "            improvements['total_loss'] = 0.0\n",
    "        \n",
    "        # Accuracy improvement (higher is better)\n",
    "        if baseline['final_accuracy'] > 0:\n",
    "            improvements['accuracy'] = (aggressive['final_accuracy'] - baseline['final_accuracy'])  # Absolute difference in percentage points\n",
    "            improvements['accuracy_relative'] = (aggressive['final_accuracy'] - baseline['final_accuracy']) / baseline['final_accuracy'] * 100\n",
    "        else:\n",
    "            improvements['accuracy'] = aggressive['final_accuracy'] - baseline['final_accuracy']\n",
    "            improvements['accuracy_relative'] = 0.0\n",
    "        \n",
    "        # Accuracy convergence speed (lower steps is better)\n",
    "        if baseline['accuracy_convergence_step'] > 0 and aggressive['accuracy_convergence_step'] > 0:\n",
    "            improvements['accuracy_convergence_speed'] = (baseline['accuracy_convergence_step'] - aggressive['accuracy_convergence_step']) / baseline['accuracy_convergence_step'] * 100\n",
    "        else:\n",
    "            improvements['accuracy_convergence_speed'] = 0.0\n",
    "        \n",
    "        # Training speed comparison (lower time is better)\n",
    "        if baseline['training_time'] > 0 and aggressive['training_time'] > 0:\n",
    "            improvements['training_speed'] = (baseline['training_time'] - aggressive['training_time']) / baseline['training_time'] * 100\n",
    "        else:\n",
    "            improvements['training_speed'] = 0.0\n",
    "        \n",
    "        # Convergence speed (lower steps is better)\n",
    "        if baseline['convergence_step'] > 0 and aggressive['convergence_step'] > 0:\n",
    "            improvements['convergence_speed'] = (baseline['convergence_step'] - aggressive['convergence_step']) / baseline['convergence_step'] * 100\n",
    "        else:\n",
    "            improvements['convergence_speed'] = 0.0\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "# Load all results\n",
    "print(\"Loading all task-curriculum results...\")\n",
    "all_results = load_all_task_results()\n",
    "\n",
    "# Process each task separately  \n",
    "task_summaries = {}\n",
    "task_improvements = {}\n",
    "\n",
    "print(\"\\nProcessing task data and calculating improvements...\")\n",
    "for task_key, task_data in all_results.items():\n",
    "    task_summaries[task_key] = extract_task_metrics(task_key, task_data)\n",
    "    task_improvements[task_key] = calculate_improvement_metrics(task_summaries[task_key])\n",
    "    \n",
    "    print(f\"\\n{TASKS[task_key]['name']} Summary:\")\n",
    "    display_cols = ['curriculum_name', 'final_total_loss', 'final_accuracy', 'best_accuracy', 'accuracy_convergence_step', 'validation_samples']\n",
    "    # Filter columns that exist\n",
    "    display_cols = [col for col in display_cols if col in task_summaries[task_key].columns]\n",
    "    if display_cols:\n",
    "        display(task_summaries[task_key][display_cols].round(2))\n",
    "    else:\n",
    "        print(\"  No data available yet - run training first\")\n",
    "    \n",
    "    # Print improvements\n",
    "    improvements = task_improvements[task_key]\n",
    "    if improvements:\n",
    "        print(f\"\\nImprovements (Aggressive vs Baseline):\")\n",
    "        print(f\"  Total Loss: {improvements.get('total_loss', 0):.1f}%\")\n",
    "        print(f\"  Accuracy: {improvements.get('accuracy', 0):+.1f} percentage points ({improvements.get('accuracy_relative', 0):+.1f}% relative)\")\n",
    "        print(f\"  Accuracy Convergence Speed: {improvements.get('accuracy_convergence_speed', 0):.1f}%\")\n",
    "\n",
    "print(f\"\\nLoaded and processed data for {len(task_summaries)} tasks\")\n",
    "print(\"Data loading and processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1wtufOg-j9x"
   },
   "source": [
    "## Task-Specific Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kkTaPFho-j9x",
    "outputId": "581b4dfa-db3a-49dd-9a32-1c3777b540e1"
   },
   "outputs": [],
   "source": [
    "def visualize_task_comparison(task_key: str, task_data: Dict, task_summary: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive visualizations for a single task with focus on accuracy.\"\"\"\n",
    "    task_config = TASKS[task_key]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle(f'{task_config[\"name\"]} - Baseline vs Aggressive Curriculum', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Training Loss Curves\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in task_data.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            curriculum_config = CURRICULA[curriculum_key]\n",
    "            # Apply smoothing for better visualization\n",
    "            window = min(10, len(training_df) // 20) if len(training_df) > 20 else 1\n",
    "            if window > 1:\n",
    "                smoothed_loss = training_df['total_loss'].rolling(window=window, min_periods=1).mean()\n",
    "            else:\n",
    "                smoothed_loss = training_df['total_loss']\n",
    "            \n",
    "            ax.plot(training_df['step'], smoothed_loss,\n",
    "                    color=curriculum_config['color'], label=curriculum_config['name'], \n",
    "                    linewidth=2, alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Training Loss Evolution (Smoothed)', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')  # Log scale for better loss visualization\n",
    "    \n",
    "    # 2. PRIMARY: Accuracy Evolution Over Time\n",
    "    ax = axes[0, 1]\n",
    "    accuracy_data_found = False\n",
    "    \n",
    "    for curriculum_idx, curriculum_key in enumerate(task_data.keys()):\n",
    "        curriculum_config = CURRICULA[curriculum_key]\n",
    "        summary_row = task_summary[task_summary['curriculum'] == curriculum_key]\n",
    "        \n",
    "        if len(summary_row) > 0 and 'accuracy_history' in summary_row.columns:\n",
    "            accuracy_history = summary_row.iloc[0]['accuracy_history']\n",
    "            if accuracy_history and len(accuracy_history) > 0:\n",
    "                steps, values = zip(*accuracy_history)\n",
    "                # Convert to percentage\n",
    "                values = [v * 100 for v in values]\n",
    "                ax.plot(steps, values,\n",
    "                        color=curriculum_config['color'], label=curriculum_config['name'],\n",
    "                        linewidth=3, alpha=0.9, marker='o', markersize=4)\n",
    "                accuracy_data_found = True\n",
    "                \n",
    "                # Add final value annotation\n",
    "                if len(values) > 0:\n",
    "                    ax.annotate(f'{values[-1]:.1f}%', \n",
    "                               xy=(steps[-1], values[-1]),\n",
    "                               xytext=(5, 5), textcoords='offset points',\n",
    "                               fontsize=10, fontweight='bold',\n",
    "                               color=curriculum_config['color'])\n",
    "    \n",
    "    if accuracy_data_found:\n",
    "        ax.set_title('Task Accuracy Evolution (PRIMARY METRIC)', fontweight='bold', color='darkred')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        # Add reference line at 90% accuracy\n",
    "        ax.axhline(y=90, color='green', linestyle='--', alpha=0.5, label='90% Target')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No accuracy data available\\n(Run training with validation enabled)', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy Evolution (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 3. Final Accuracy Comparison (Bar Chart)\n",
    "    ax = axes[1, 0]\n",
    "    curricula_names = task_summary['curriculum_name'].tolist()\n",
    "    accuracies = task_summary['final_accuracy'].tolist()\n",
    "    best_accuracies = task_summary['best_accuracy'].tolist() if 'best_accuracy' in task_summary.columns else accuracies\n",
    "    colors = task_summary['color'].tolist()\n",
    "    \n",
    "    if max(accuracies) > 0:\n",
    "        x = np.arange(len(curricula_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, accuracies, width, label='Final Accuracy',\n",
    "                      color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        bars2 = ax.bar(x + width/2, best_accuracies, width, label='Best Accuracy',\n",
    "                      color=colors, alpha=0.5, edgecolor='black', linewidth=2, hatch='//')\n",
    "        \n",
    "        ax.set_title('Accuracy Comparison (TASK PERFORMANCE)', fontweight='bold', color='darkred')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_xlabel('Curriculum')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(curricula_names)\n",
    "        ax.set_ylim(0, max(max(accuracies), max(best_accuracies)) * 1.2 if max(best_accuracies) > 0 else 100)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar1, bar2, acc, best in zip(bars1, bars2, accuracies, best_accuracies):\n",
    "            ax.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 1,\n",
    "                    f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "            ax.text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 1,\n",
    "                    f'{best:.1f}%', ha='center', va='bottom', fontweight='bold', style='italic')\n",
    "        \n",
    "        # Add improvement annotation\n",
    "        if len(accuracies) == 2:\n",
    "            baseline_idx = 0 if 'baseline' in curricula_names[0].lower() else 1\n",
    "            aggressive_idx = 1 - baseline_idx\n",
    "            \n",
    "            improvement = accuracies[aggressive_idx] - accuracies[baseline_idx]\n",
    "            color = 'green' if improvement > 0 else 'red'\n",
    "            ax.text(0.5, 0.95, f'Accuracy Gain: {improvement:+.1f} percentage points', \n",
    "                    ha='center', transform=ax.transAxes, fontweight='bold', fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No accuracy data available', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy Comparison (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 4. Accuracy vs Loss Trade-off\n",
    "    ax = axes[1, 1]\n",
    "    if 'final_accuracy' in task_summary.columns and 'final_total_loss' in task_summary.columns:\n",
    "        for _, row in task_summary.iterrows():\n",
    "            ax.scatter(row['final_total_loss'], row['final_accuracy'],\n",
    "                      s=200, c=[row['color']], alpha=0.7,\n",
    "                      edgecolors='black', linewidth=2,\n",
    "                      label=row['curriculum_name'])\n",
    "            # Add text annotation\n",
    "            ax.annotate(row['curriculum_name'],\n",
    "                       xy=(row['final_total_loss'], row['final_accuracy']),\n",
    "                       xytext=(5, 5), textcoords='offset points',\n",
    "                       fontsize=10)\n",
    "        \n",
    "        ax.set_title('Accuracy vs Loss Trade-off', fontweight='bold')\n",
    "        ax.set_xlabel('Final Total Loss (Lower is Better)')\n",
    "        ax.set_ylabel('Final Accuracy (%) (Higher is Better)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add ideal direction arrow\n",
    "        ax.annotate('', xy=(0.8, 0.9), xytext=(0.95, 0.75),\n",
    "                   xycoords='axes fraction',\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='green', alpha=0.5))\n",
    "        ax.text(0.75, 0.95, 'Better', transform=ax.transAxes,\n",
    "               fontsize=10, color='green', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for trade-off analysis', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy vs Loss Trade-off (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 5. Convergence Speed Comparison\n",
    "    ax = axes[2, 0]\n",
    "    conv_data = []\n",
    "    if 'accuracy_convergence_step' in task_summary.columns:\n",
    "        for _, row in task_summary.iterrows():\n",
    "            if row['accuracy_convergence_step'] > 0:\n",
    "                conv_data.append({\n",
    "                    'curriculum': row['curriculum_name'],\n",
    "                    'loss_conv': row.get('convergence_step', -1),\n",
    "                    'acc_conv': row['accuracy_convergence_step'],\n",
    "                    'color': row['color']\n",
    "                })\n",
    "    \n",
    "    if conv_data:\n",
    "        conv_df = pd.DataFrame(conv_data)\n",
    "        x = np.arange(len(conv_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, conv_df['loss_conv'], width, label='Loss Convergence',\n",
    "                      color=conv_df['color'], alpha=0.5, edgecolor='black')\n",
    "        bars2 = ax.bar(x + width/2, conv_df['acc_conv'], width, label='Accuracy Convergence',\n",
    "                      color=conv_df['color'], alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_title('Convergence Speed Comparison (Lower is Faster)', fontweight='bold')\n",
    "        ax.set_ylabel('Steps to Convergence')\n",
    "        ax.set_xlabel('Curriculum')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(conv_df['curriculum'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar1, bar2, loss_c, acc_c in zip(bars1, bars2, conv_df['loss_conv'], conv_df['acc_conv']):\n",
    "            if loss_c > 0:\n",
    "                ax.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 1,\n",
    "                        f'{int(loss_c)}', ha='center', va='bottom')\n",
    "            if acc_c > 0:\n",
    "                ax.text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 1,\n",
    "                        f'{int(acc_c)}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No convergence data available', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Convergence Speed (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 6. Task-Specific Metrics\n",
    "    ax = axes[2, 1]\n",
    "    task_specific_metrics = []\n",
    "    \n",
    "    # Get all metrics that end with '_final' except the main accuracy metric\n",
    "    main_metric = TASKS[task_key]['expected_metrics'][0]\n",
    "    for col in task_summary.columns:\n",
    "        if col.endswith('_final') and not col.startswith(main_metric):\n",
    "            metric_name = col.replace('_final', '').replace('_', ' ').title()\n",
    "            task_specific_metrics.append((metric_name, col))\n",
    "    \n",
    "    if task_specific_metrics:\n",
    "        metric_data = []\n",
    "        for _, row in task_summary.iterrows():\n",
    "            for metric_name, col in task_specific_metrics[:3]:  # Show top 3 additional metrics\n",
    "                if col in row and row[col] > 0:\n",
    "                    metric_data.append({\n",
    "                        'curriculum': row['curriculum_name'],\n",
    "                        'metric': metric_name,\n",
    "                        'value': row[col],\n",
    "                        'color': row['color']\n",
    "                    })\n",
    "        \n",
    "        if metric_data:\n",
    "            metric_df = pd.DataFrame(metric_data)\n",
    "            # Create grouped bar chart\n",
    "            metrics = metric_df['metric'].unique()\n",
    "            curricula = metric_df['curriculum'].unique()\n",
    "            \n",
    "            x = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            for i, curriculum in enumerate(curricula):\n",
    "                curr_data = metric_df[metric_df['curriculum'] == curriculum]\n",
    "                values = [curr_data[curr_data['metric'] == m]['value'].iloc[0] if len(curr_data[curr_data['metric'] == m]) > 0 else 0 \n",
    "                         for m in metrics]\n",
    "                color = CURRICULA[list(CURRICULA.keys())[i]]['color']\n",
    "                ax.bar(x + i * width - width/2, values, width, label=curriculum,\n",
    "                      color=color, alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            ax.set_title(f'Additional {task_config[\"name\"]} Metrics', fontweight='bold')\n",
    "            ax.set_ylabel('Value (%)')\n",
    "            ax.set_xlabel('Metric')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No additional metrics available', \n",
    "                    ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title('Additional Metrics (No Data)', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No additional metrics available', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Additional Metrics (No Data)', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_cross_task_comparison(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Compare aggressive curriculum performance across tasks with focus on accuracy.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Cross-Task Accuracy Performance: Aggressive vs Baseline Curriculum', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract data for comparison\n",
    "    task_names = []\n",
    "    baseline_losses = []\n",
    "    aggressive_losses = []\n",
    "    baseline_accuracies = []\n",
    "    aggressive_accuracies = []\n",
    "    baseline_best_acc = []\n",
    "    aggressive_best_acc = []\n",
    "    accuracy_improvements = []\n",
    "    accuracy_improvements_rel = []\n",
    "    \n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        task_names.append(TASKS[task_key]['name'])\n",
    "        \n",
    "        baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "        \n",
    "        if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "            baseline = baseline_row.iloc[0]\n",
    "            aggressive = aggressive_row.iloc[0]\n",
    "            \n",
    "            baseline_losses.append(baseline['final_total_loss'])\n",
    "            aggressive_losses.append(aggressive['final_total_loss'])\n",
    "            baseline_accuracies.append(baseline['final_accuracy'])\n",
    "            aggressive_accuracies.append(aggressive['final_accuracy'])\n",
    "            baseline_best_acc.append(baseline.get('best_accuracy', baseline['final_accuracy']))\n",
    "            aggressive_best_acc.append(aggressive.get('best_accuracy', aggressive['final_accuracy']))\n",
    "            \n",
    "            # Get improvements from pre-calculated data\n",
    "            improvements = task_improvements[task_key]\n",
    "            accuracy_improvements.append(improvements.get('accuracy', 0))\n",
    "            accuracy_improvements_rel.append(improvements.get('accuracy_relative', 0))\n",
    "        else:\n",
    "            baseline_losses.append(0)\n",
    "            aggressive_losses.append(0)\n",
    "            baseline_accuracies.append(0)\n",
    "            aggressive_accuracies.append(0)\n",
    "            baseline_best_acc.append(0)\n",
    "            aggressive_best_acc.append(0)\n",
    "            accuracy_improvements.append(0)\n",
    "            accuracy_improvements_rel.append(0)\n",
    "    \n",
    "    # 1. PRIMARY: Accuracy Comparison by Task\n",
    "    ax = axes[0, 0]\n",
    "    valid_acc_indices = [i for i, (b, a) in enumerate(zip(baseline_accuracies, aggressive_accuracies)) \n",
    "                         if b > 0 or a > 0]\n",
    "    \n",
    "    if valid_acc_indices:\n",
    "        valid_names = [task_names[i] for i in valid_acc_indices]\n",
    "        valid_baseline_acc = [baseline_accuracies[i] for i in valid_acc_indices]\n",
    "        valid_aggressive_acc = [aggressive_accuracies[i] for i in valid_acc_indices]\n",
    "        valid_x = np.arange(len(valid_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        baseline_bars = ax.bar(valid_x - width/2, valid_baseline_acc, width, label='Baseline', \n",
    "                              color=CURRICULA['baseline']['color'], alpha=0.7, edgecolor='black')\n",
    "        aggressive_bars = ax.bar(valid_x + width/2, valid_aggressive_acc, width, label='Aggressive', \n",
    "                                color=CURRICULA['aggressive']['color'], alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_title('Final Accuracy by Task (PRIMARY METRIC)', fontweight='bold', color='darkred')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_xlabel('Task')\n",
    "        ax.set_xticks(valid_x)\n",
    "        ax.set_xticklabels(valid_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(baseline_bars, valid_baseline_acc):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        for bar, val in zip(aggressive_bars, valid_aggressive_acc):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No accuracy data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy by Task (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 2. Accuracy Improvement (Percentage Points)\n",
    "    ax = axes[0, 1]\n",
    "    valid_improvements = []\n",
    "    for idx, (name, imp) in enumerate(zip(task_names, accuracy_improvements)):\n",
    "        if imp != 0 or baseline_accuracies[idx] > 0 or aggressive_accuracies[idx] > 0:\n",
    "            valid_improvements.append((name, imp))\n",
    "    \n",
    "    if valid_improvements:\n",
    "        names, improvements = zip(*valid_improvements) if valid_improvements else ([], [])\n",
    "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "        bars = ax.bar(names, improvements, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Accuracy Improvement (Percentage Points)', fontweight='bold', color='darkred')\n",
    "        ax.set_ylabel('Improvement (pp)')\n",
    "        ax.set_xlabel('Task')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, imp in zip(bars, improvements):\n",
    "            y_pos = bar.get_height() + (0.5 if imp > 0 else -0.5)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                    f'{imp:+.1f}pp', ha='center', va='bottom' if imp > 0 else 'top', \n",
    "                    fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No improvement data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy Improvement (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 3. Best Accuracy Achieved\n",
    "    ax = axes[0, 2]\n",
    "    valid_best_indices = [i for i, (b, a) in enumerate(zip(baseline_best_acc, aggressive_best_acc)) \n",
    "                          if b > 0 or a > 0]\n",
    "    \n",
    "    if valid_best_indices:\n",
    "        valid_names = [task_names[i] for i in valid_best_indices]\n",
    "        valid_baseline_best = [baseline_best_acc[i] for i in valid_best_indices]\n",
    "        valid_aggressive_best = [aggressive_best_acc[i] for i in valid_best_indices]\n",
    "        valid_x = np.arange(len(valid_names))\n",
    "        \n",
    "        baseline_bars = ax.bar(valid_x - width/2, valid_baseline_best, width, label='Baseline Best', \n",
    "                              color=CURRICULA['baseline']['color'], alpha=0.5, edgecolor='black', hatch='//')\n",
    "        aggressive_bars = ax.bar(valid_x + width/2, valid_aggressive_best, width, label='Aggressive Best', \n",
    "                                color=CURRICULA['aggressive']['color'], alpha=0.5, edgecolor='black', hatch='//')\n",
    "        \n",
    "        ax.set_title('Best Accuracy Achieved', fontweight='bold')\n",
    "        ax.set_ylabel('Best Accuracy (%)')\n",
    "        ax.set_xlabel('Task')\n",
    "        ax.set_xticks(valid_x)\n",
    "        ax.set_xticklabels(valid_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No best accuracy data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Best Accuracy (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 4. Loss Comparison (for reference)\n",
    "    ax = axes[1, 0]\n",
    "    valid_indices = [i for i, (b, a) in enumerate(zip(baseline_losses, aggressive_losses)) if b > 0 and a > 0]\n",
    "    \n",
    "    if valid_indices:\n",
    "        valid_names = [task_names[i] for i in valid_indices]\n",
    "        valid_baseline_losses = [baseline_losses[i] for i in valid_indices]\n",
    "        valid_aggressive_losses = [aggressive_losses[i] for i in valid_indices]\n",
    "        valid_x = np.arange(len(valid_names))\n",
    "        \n",
    "        baseline_bars = ax.bar(valid_x - width/2, valid_baseline_losses, width, label='Baseline', \n",
    "                              color=CURRICULA['baseline']['color'], alpha=0.7, edgecolor='black')\n",
    "        aggressive_bars = ax.bar(valid_x + width/2, valid_aggressive_losses, width, label='Aggressive', \n",
    "                                color=CURRICULA['aggressive']['color'], alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        ax.set_title('Total Loss by Task (Lower = Better)', fontweight='bold')\n",
    "        ax.set_ylabel('Total Loss')\n",
    "        ax.set_xlabel('Task')\n",
    "        ax.set_xticks(valid_x)\n",
    "        ax.set_xticklabels(valid_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No valid loss data', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Loss by Task (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 5. Accuracy vs Loss Scatter (All Tasks)\n",
    "    ax = axes[1, 1]\n",
    "    scatter_data = []\n",
    "    for i, task_name in enumerate(task_names):\n",
    "        if baseline_losses[i] > 0 and baseline_accuracies[i] >= 0:\n",
    "            scatter_data.append({\n",
    "                'loss': baseline_losses[i],\n",
    "                'accuracy': baseline_accuracies[i],\n",
    "                'task': task_name,\n",
    "                'curriculum': 'Baseline',\n",
    "                'color': CURRICULA['baseline']['color']\n",
    "            })\n",
    "        if aggressive_losses[i] > 0 and aggressive_accuracies[i] >= 0:\n",
    "            scatter_data.append({\n",
    "                'loss': aggressive_losses[i],\n",
    "                'accuracy': aggressive_accuracies[i],\n",
    "                'task': task_name,\n",
    "                'curriculum': 'Aggressive',\n",
    "                'color': CURRICULA['aggressive']['color']\n",
    "            })\n",
    "    \n",
    "    if scatter_data:\n",
    "        for item in scatter_data:\n",
    "            marker = 'o' if item['curriculum'] == 'Baseline' else '^'\n",
    "            ax.scatter(item['loss'], item['accuracy'], s=150, c=[item['color']], \n",
    "                      marker=marker, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "            # Add task label\n",
    "            ax.annotate(item['task'][:3], xy=(item['loss'], item['accuracy']),\n",
    "                       xytext=(2, 2), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        ax.set_title('Accuracy vs Loss Trade-off (All Tasks)', fontweight='bold')\n",
    "        ax.set_xlabel('Total Loss (Lower is Better)')\n",
    "        ax.set_ylabel('Accuracy (%) (Higher is Better)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=CURRICULA['baseline']['color'], alpha=0.7, label='Baseline'),\n",
    "            Patch(facecolor=CURRICULA['aggressive']['color'], alpha=0.7, label='Aggressive')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data for scatter plot', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Accuracy vs Loss (No Data)', fontweight='bold')\n",
    "    \n",
    "    # 6. Overall Summary Score\n",
    "    ax = axes[1, 2]\n",
    "    \n",
    "    # Create a summary score for each task (weighted: accuracy 70%, loss reduction 30%)\n",
    "    task_scores = []\n",
    "    score_names = []\n",
    "    \n",
    "    for task_key, improvements in task_improvements.items():\n",
    "        if any(imp != 0 for imp in improvements.values()):\n",
    "            # Weighted score: accuracy improvement matters more\n",
    "            score = (improvements.get('accuracy', 0) * 0.7 + \n",
    "                    improvements.get('total_loss', 0) * 0.3)\n",
    "            task_scores.append(score)\n",
    "            score_names.append(TASKS[task_key]['name'])\n",
    "    \n",
    "    if task_scores:\n",
    "        colors = ['green' if score > 0 else 'red' for score in task_scores]\n",
    "        bars = ax.bar(score_names, task_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Overall Performance Score\\n(70% Accuracy, 30% Loss)', fontweight='bold')\n",
    "        ax.set_ylabel('Composite Score')\n",
    "        ax.set_xlabel('Task')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, task_scores):\n",
    "            y_pos = bar.get_height() + (0.5 if score > 0 else -0.5)\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "                    f'{score:+.1f}', ha='center', va='bottom' if score > 0 else 'top', fontweight='bold')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add overall verdict\n",
    "        avg_score = np.mean(task_scores)\n",
    "        verdict_color = 'green' if avg_score > 0 else 'red'\n",
    "        ax.text(0.5, 0.95, f'Average Score: {avg_score:+.1f}', \n",
    "                ha='center', transform=ax.transAxes, fontweight='bold', fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=verdict_color, alpha=0.3))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No performance data available', ha='center', va='center', \n",
    "                transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Overall Performance (No Data)', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations for each task\n",
    "print(\"Generating task-specific visualizations with accuracy focus...\")\n",
    "for task_key, task_data in all_results.items():\n",
    "    print(f\"\\nGenerating visualizations for {TASKS[task_key]['name']}...\")\n",
    "    visualize_task_comparison(task_key, task_data, task_summaries[task_key])\n",
    "\n",
    "print(\"\\nGenerating cross-task comparison with accuracy focus...\")\n",
    "create_cross_task_comparison(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nAll visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwCctp-A-j9y"
   },
   "source": [
    "## Comprehensive Analysis and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N97tTHix-j9y",
    "outputId": "eb1787d1-fa57-4637-aabf-37893a596711"
   },
   "outputs": [],
   "source": [
    "def generate_comprehensive_analysis(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Generate detailed analysis of curriculum effectiveness with focus on accuracy.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE TASK-CURRICULUM ANALYSIS: ACCURACY FOCUS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüéØ PRIMARY METRIC: TASK ACCURACY PERFORMANCE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create accuracy-focused comparison table\n",
    "    accuracy_results = []\n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        for _, row in summary_df.iterrows():\n",
    "            result = {\n",
    "                'Task': TASKS[task_key]['name'],\n",
    "                'Curriculum': row['curriculum_name'],\n",
    "                'Final Accuracy (%)': f\"{row['final_accuracy']:.1f}\" if row['final_accuracy'] > 0 else 'N/A',\n",
    "                'Best Accuracy (%)': f\"{row.get('best_accuracy', 0):.1f}\" if row.get('best_accuracy', 0) > 0 else 'N/A',\n",
    "                'Acc. Conv. Step': f\"{int(row.get('accuracy_convergence_step', -1))}\" if row.get('accuracy_convergence_step', -1) > 0 else 'N/A',\n",
    "                'Validation Samples': f\"{int(row.get('validation_samples', 0))}\" if row.get('validation_samples', 0) > 0 else 'N/A',\n",
    "                'Final Loss': f\"{row['final_total_loss']:.4f}\" if row['final_total_loss'] > 0 else 'N/A'\n",
    "            }\n",
    "            accuracy_results.append(result)\n",
    "    \n",
    "    accuracy_df = pd.DataFrame(accuracy_results)\n",
    "    print(accuracy_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nüìä ACCURACY IMPROVEMENT ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Detailed accuracy analysis for each task\n",
    "    total_tasks = 0\n",
    "    successful_accuracy_improvements = 0\n",
    "    successful_loss_improvements = 0\n",
    "    accuracy_gains = []\n",
    "    \n",
    "    for task_key, summary_df in task_summaries.items():\n",
    "        task_name = TASKS[task_key]['name']\n",
    "        print(f\"\\n{task_name}:\")\n",
    "        print(f\"  Task Type: {TASKS[task_key]['description']}\")\n",
    "        \n",
    "        baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "        \n",
    "        if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "            total_tasks += 1\n",
    "            baseline = baseline_row.iloc[0]\n",
    "            aggressive = aggressive_row.iloc[0]\n",
    "            improvements = task_improvements[task_key]\n",
    "            \n",
    "            # PRIMARY: Accuracy Analysis\n",
    "            accuracy_improvement = improvements.get('accuracy', 0)\n",
    "            accuracy_improvement_rel = improvements.get('accuracy_relative', 0)\n",
    "            \n",
    "            print(f\"\\n  üéØ ACCURACY PERFORMANCE:\")\n",
    "            if baseline['final_accuracy'] > 0 or aggressive['final_accuracy'] > 0:\n",
    "                print(f\"     Baseline: {baseline['final_accuracy']:.1f}%\")\n",
    "                print(f\"     Aggressive: {aggressive['final_accuracy']:.1f}%\")\n",
    "                \n",
    "                if accuracy_improvement > 0:\n",
    "                    successful_accuracy_improvements += 1\n",
    "                    print(f\"     ‚úÖ IMPROVEMENT: +{accuracy_improvement:.1f} percentage points ({accuracy_improvement_rel:+.1f}% relative)\")\n",
    "                    accuracy_gains.append(accuracy_improvement)\n",
    "                elif accuracy_improvement < 0:\n",
    "                    print(f\"     ‚ùå REGRESSION: {accuracy_improvement:.1f} percentage points ({accuracy_improvement_rel:.1f}% relative)\")\n",
    "                    accuracy_gains.append(accuracy_improvement)\n",
    "                else:\n",
    "                    print(f\"     ‚ûñ NO CHANGE\")\n",
    "                \n",
    "                # Best accuracy comparison\n",
    "                if 'best_accuracy' in baseline and 'best_accuracy' in aggressive:\n",
    "                    best_improvement = aggressive['best_accuracy'] - baseline['best_accuracy']\n",
    "                    print(f\"     Best Achieved: Baseline {baseline['best_accuracy']:.1f}% ‚Üí Aggressive {aggressive['best_accuracy']:.1f}% ({best_improvement:+.1f}pp)\")\n",
    "            else:\n",
    "                print(f\"     ‚ö†Ô∏è  No accuracy data available\")\n",
    "            \n",
    "            # Accuracy convergence analysis\n",
    "            if baseline.get('accuracy_convergence_step', -1) > 0 and aggressive.get('accuracy_convergence_step', -1) > 0:\n",
    "                conv_improvement = improvements.get('accuracy_convergence_speed', 0)\n",
    "                print(f\"\\n  ‚è±Ô∏è  ACCURACY CONVERGENCE:\")\n",
    "                print(f\"     Baseline: {int(baseline['accuracy_convergence_step'])} steps\")\n",
    "                print(f\"     Aggressive: {int(aggressive['accuracy_convergence_step'])} steps\")\n",
    "                if conv_improvement > 0:\n",
    "                    print(f\"     ‚úÖ {conv_improvement:.1f}% faster convergence\")\n",
    "                elif conv_improvement < 0:\n",
    "                    print(f\"     ‚ùå {abs(conv_improvement):.1f}% slower convergence\")\n",
    "            \n",
    "            # Secondary: Loss Analysis (for reference)\n",
    "            loss_improvement = improvements.get('total_loss', 0)\n",
    "            if loss_improvement != 0:\n",
    "                if loss_improvement > 0:\n",
    "                    successful_loss_improvements += 1\n",
    "                print(f\"\\n  üìâ LOSS (Reference):\")\n",
    "                print(f\"     Baseline: {baseline['final_total_loss']:.4f}\")\n",
    "                print(f\"     Aggressive: {aggressive['final_total_loss']:.4f}\")\n",
    "                print(f\"     {'‚úÖ' if loss_improvement > 0 else '‚ùå'} Change: {loss_improvement:+.1f}%\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Missing training results for comparison\")\n",
    "    \n",
    "    # Overall accuracy-focused summary\n",
    "    print(\"\\nüèÜ ACCURACY-FOCUSED SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if total_tasks > 0:\n",
    "        accuracy_success_rate = successful_accuracy_improvements / total_tasks * 100\n",
    "        loss_success_rate = successful_loss_improvements / total_tasks * 100\n",
    "        \n",
    "        print(f\"üìà ACCURACY IMPROVEMENT STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Tasks with accuracy gains: {successful_accuracy_improvements}/{total_tasks} ({accuracy_success_rate:.0f}%)\")\n",
    "        \n",
    "        if accuracy_gains:\n",
    "            avg_accuracy_gain = np.mean(accuracy_gains)\n",
    "            max_gain = max(accuracy_gains)\n",
    "            min_gain = min(accuracy_gains)\n",
    "            print(f\"   ‚Ä¢ Average accuracy change: {avg_accuracy_gain:+.1f} percentage points\")\n",
    "            print(f\"   ‚Ä¢ Best improvement: {max_gain:+.1f} percentage points\")\n",
    "            print(f\"   ‚Ä¢ Worst change: {min_gain:+.1f} percentage points\")\n",
    "        \n",
    "        print(f\"\\nüìâ LOSS REDUCTION (Reference):\")\n",
    "        print(f\"   ‚Ä¢ Tasks with loss reduction: {successful_loss_improvements}/{total_tasks} ({loss_success_rate:.0f}%)\")\n",
    "        \n",
    "        # Task-specific insights\n",
    "        print(\"\\nüîç TASK-SPECIFIC INSIGHTS:\")\n",
    "        for task_key, improvements in task_improvements.items():\n",
    "            task_name = TASKS[task_key]['name']\n",
    "            acc_imp = improvements.get('accuracy', 0)\n",
    "            \n",
    "            if task_key == 'sudoku':\n",
    "                if acc_imp > 0:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: Curriculum helps with constraint satisfaction (+{acc_imp:.1f}pp)\")\n",
    "                elif acc_imp < 0:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: Baseline more stable for puzzle solving ({acc_imp:.1f}pp)\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: No significant difference detected\")\n",
    "                    \n",
    "            elif task_key == 'connectivity':\n",
    "                if acc_imp > 0:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: Curriculum improves graph reasoning (+{acc_imp:.1f}pp)\")\n",
    "                elif acc_imp < 0:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: Baseline better for connectivity detection ({acc_imp:.1f}pp)\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {task_name}: Similar performance across curricula\")\n",
    "        \n",
    "        # Final verdict based on accuracy\n",
    "        print(\"\\nüéØ ACCURACY-BASED RECOMMENDATION:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if accuracy_success_rate >= 75:\n",
    "            print(\"‚úÖ STRONG EVIDENCE: Aggressive curriculum significantly improves task accuracy\")\n",
    "            print(f\"   ‚Üí Deploy aggressive curriculum (Accuracy gains in {accuracy_success_rate:.0f}% of tasks)\")\n",
    "        elif accuracy_success_rate >= 50:\n",
    "            print(\"‚öñÔ∏è  MIXED EVIDENCE: Aggressive curriculum shows moderate accuracy benefits\")\n",
    "            print(f\"   ‚Üí Consider task-specific deployment (Accuracy gains in {accuracy_success_rate:.0f}% of tasks)\")\n",
    "        else:\n",
    "            print(\"‚ùå INSUFFICIENT EVIDENCE: Baseline performs better or equally on accuracy\")\n",
    "            print(f\"   ‚Üí Stick with baseline training (Accuracy gains in only {accuracy_success_rate:.0f}% of tasks)\")\n",
    "        \n",
    "        # Additional recommendations\n",
    "        print(\"\\nüí° KEY OBSERVATIONS:\")\n",
    "        if avg_accuracy_gain > 5:\n",
    "            print(\"   ‚Ä¢ Substantial accuracy improvements observed (+5pp average)\")\n",
    "        elif avg_accuracy_gain > 0:\n",
    "            print(\"   ‚Ä¢ Modest accuracy improvements observed\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ No clear accuracy advantage for aggressive curriculum\")\n",
    "        \n",
    "        if any(imp.get('accuracy_convergence_speed', 0) > 20 for imp in task_improvements.values()):\n",
    "            print(\"   ‚Ä¢ Aggressive curriculum shows faster accuracy convergence in some tasks\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Insufficient data for accuracy assessment\")\n",
    "    \n",
    "    # Experimental details\n",
    "    print(f\"\\nüìã EXPERIMENTAL CONFIGURATION:\")\n",
    "    print(f\"   ‚Ä¢ Tasks evaluated: {', '.join([TASKS[k]['name'] for k in TASKS.keys()])}\")\n",
    "    print(f\"   ‚Ä¢ Primary metric: Task-specific accuracy (%)\")\n",
    "    print(f\"   ‚Ä¢ Training steps: {list(TASKS.values())[0]['train_steps']} per task\")\n",
    "    print(f\"   ‚Ä¢ Validation frequency: Every {list(TASKS.values())[0].get('validation_interval', 10)} steps\")\n",
    "    print(f\"   ‚Ä¢ Diffusion steps: {COMMON_ARGS['diffusion_steps']}\")\n",
    "    \n",
    "    return accuracy_df\n",
    "\n",
    "def create_accuracy_summary_report(task_summaries: Dict, task_improvements: Dict):\n",
    "    \"\"\"Create a concise accuracy-focused summary report.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä ACCURACY PERFORMANCE REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for task_key in TASKS.keys():\n",
    "        if task_key in task_summaries and task_key in task_improvements:\n",
    "            summary_df = task_summaries[task_key]\n",
    "            improvements = task_improvements[task_key]\n",
    "            \n",
    "            baseline_row = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "            aggressive_row = summary_df[summary_df['curriculum'] == 'aggressive']\n",
    "            \n",
    "            if len(baseline_row) > 0 and len(aggressive_row) > 0:\n",
    "                summary_data.append({\n",
    "                    'Task': TASKS[task_key]['name'],\n",
    "                    'Baseline Acc (%)': f\"{baseline_row.iloc[0]['final_accuracy']:.1f}\",\n",
    "                    'Aggressive Acc (%)': f\"{aggressive_row.iloc[0]['final_accuracy']:.1f}\",\n",
    "                    'Improvement (pp)': f\"{improvements.get('accuracy', 0):+.1f}\",\n",
    "                    'Winner': 'üèÜ Aggressive' if improvements.get('accuracy', 0) > 0 else 'üèÜ Baseline' if improvements.get('accuracy', 0) < 0 else 'ü§ù Tie'\n",
    "                })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_table = pd.DataFrame(summary_data)\n",
    "        print(\"\\nACCURACY COMPARISON TABLE:\")\n",
    "        print(summary_table.to_string(index=False))\n",
    "        \n",
    "        # Calculate overall winner\n",
    "        aggressive_wins = sum(1 for d in summary_data if 'Aggressive' in d['Winner'])\n",
    "        baseline_wins = sum(1 for d in summary_data if 'Baseline' in d['Winner'])\n",
    "        \n",
    "        print(f\"\\nüìà OVERALL ACCURACY VERDICT:\")\n",
    "        print(f\"   Aggressive wins: {aggressive_wins}/{len(summary_data)} tasks\")\n",
    "        print(f\"   Baseline wins: {baseline_wins}/{len(summary_data)} tasks\")\n",
    "        \n",
    "        if aggressive_wins > baseline_wins:\n",
    "            print(f\"\\n   üèÜ WINNER: Aggressive Curriculum (Better accuracy in {aggressive_wins}/{len(summary_data)} tasks)\")\n",
    "        elif baseline_wins > aggressive_wins:\n",
    "            print(f\"\\n   üèÜ WINNER: Baseline Training (Better accuracy in {baseline_wins}/{len(summary_data)} tasks)\")\n",
    "        else:\n",
    "            print(f\"\\n   ü§ù TIE: Equal performance across tasks\")\n",
    "    else:\n",
    "        print(\"No comparison data available\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate comprehensive accuracy-focused analysis\n",
    "print(\"Generating comprehensive accuracy-focused analysis...\")\n",
    "accuracy_results = generate_comprehensive_analysis(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nGenerating accuracy summary report...\")\n",
    "create_accuracy_summary_report(task_summaries, task_improvements)\n",
    "\n",
    "print(\"\\nComprehensive accuracy analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa1Jektc-j9y"
   },
   "source": [
    "## Results Export and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yait6iFw-j9y",
    "outputId": "66eaa819-6062-407a-c056-bc8b9a1adcd5"
   },
   "outputs": [],
   "source": [
    "# Export comprehensive results\n",
    "output_dir = Path('./ired_task_curriculum_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save task-specific summaries\n",
    "for task_key, summary_df in task_summaries.items():\n",
    "    summary_df.to_csv(output_dir / f'{task_key}_detailed_summary.csv', index=False)\n",
    "\n",
    "# Save unified results\n",
    "unified_results.to_csv(output_dir / 'unified_task_curriculum_results.csv', index=False)\n",
    "\n",
    "# Save improvements summary\n",
    "improvements_data = []\n",
    "for task_key, improvements in task_improvements.items():\n",
    "    for metric, value in improvements.items():\n",
    "        improvements_data.append({\n",
    "            'Task': TASKS[task_key]['name'],\n",
    "            'Task_Key': task_key,\n",
    "            'Metric': metric.replace('_', ' ').title(),\n",
    "            'Improvement_Percent': value\n",
    "        })\n",
    "\n",
    "improvements_df = pd.DataFrame(improvements_data)\n",
    "improvements_df.to_csv(output_dir / 'curriculum_improvements_summary.csv', index=False)\n",
    "\n",
    "# Create training results summary\n",
    "training_summary = []\n",
    "for task_key, task_results in training_results.items():\n",
    "    for curriculum_key, result_code in task_results.items():\n",
    "        training_summary.append({\n",
    "            'Task': TASKS[task_key]['name'],\n",
    "            'Task_Key': task_key,\n",
    "            'Curriculum': CURRICULA[curriculum_key]['name'],\n",
    "            'Curriculum_Key': curriculum_key,\n",
    "            'Exit_Code': result_code,\n",
    "            'Success': result_code == 0,\n",
    "            'Training_Steps': TASKS[task_key]['train_steps'],\n",
    "            'Model': TASKS[task_key]['model'],\n",
    "            'Dataset': TASKS[task_key]['dataset']\n",
    "        })\n",
    "\n",
    "training_df = pd.DataFrame(training_summary)\n",
    "training_df.to_csv(output_dir / 'training_execution_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS EXPORT AND FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {output_dir}\")\n",
    "print(\"   üìä Data Files:\")\n",
    "for task_key in TASKS.keys():\n",
    "    print(f\"     ‚Ä¢ {task_key}_detailed_summary.csv - {TASKS[task_key]['name']} detailed metrics\")\n",
    "print(f\"     ‚Ä¢ unified_task_curriculum_results.csv - Combined results across all tasks\")\n",
    "print(f\"     ‚Ä¢ curriculum_improvements_summary.csv - Improvement percentages by metric\")\n",
    "print(f\"     ‚Ä¢ training_execution_summary.csv - Training execution status\")\n",
    "\n",
    "# Generate final executive summary\n",
    "successful_tasks = sum(1 for task_results in training_results.values() \n",
    "                      if all(r == 0 for r in task_results.values()))\n",
    "total_experiments = sum(len(task_results) for task_results in training_results.values())\n",
    "successful_experiments = sum(sum(1 for r in task_results.values() if r == 0) \n",
    "                           for task_results in training_results.values())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† IRED TASK CURRICULUM ANALYSIS - EXECUTIVE SUMMARY üß†\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüéØ EXPERIMENTAL OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Tasks evaluated: {len(TASKS)} IRED reasoning tasks\")\n",
    "print(f\"   ‚Ä¢ Curricula compared: {len(CURRICULA)} approaches (Baseline vs Aggressive)\")\n",
    "print(f\"   ‚Ä¢ Total experiments: {total_experiments}\")\n",
    "print(f\"   ‚Ä¢ Successful completions: {successful_experiments}/{total_experiments} ({successful_experiments/total_experiments*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Fully completed tasks: {successful_tasks}/{len(TASKS)}\")\n",
    "\n",
    "# Calculate key findings\n",
    "loss_improvements = [task_improvements[task]['total_loss'] \n",
    "                    for task in task_improvements.keys() \n",
    "                    if task_improvements[task]['total_loss'] != 0]\n",
    "\n",
    "accuracy_improvements = [task_improvements[task]['accuracy'] \n",
    "                        for task in task_improvements.keys() \n",
    "                        if task_improvements[task]['accuracy'] != 0]\n",
    "\n",
    "if loss_improvements:\n",
    "    avg_loss_improvement = np.mean(loss_improvements)\n",
    "    positive_loss_improvements = sum(1 for imp in loss_improvements if imp > 0)\n",
    "    print(f\"\\nüìâ TRAINING LOSS FINDINGS:\")\n",
    "    print(f\"   ‚Ä¢ Average improvement: {avg_loss_improvement:+.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Tasks with improvement: {positive_loss_improvements}/{len(loss_improvements)}\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {positive_loss_improvements/len(loss_improvements)*100:.1f}%\")\n",
    "\n",
    "if accuracy_improvements:\n",
    "    avg_accuracy_improvement = np.mean(accuracy_improvements)\n",
    "    positive_accuracy_improvements = sum(1 for imp in accuracy_improvements if imp > 0)\n",
    "    print(f\"\\nüìà ACCURACY FINDINGS:\")\n",
    "    print(f\"   ‚Ä¢ Average improvement: {avg_accuracy_improvement:+.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Tasks with improvement: {positive_accuracy_improvements}/{len(accuracy_improvements)}\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {positive_accuracy_improvements/len(accuracy_improvements)*100:.1f}%\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nüèÜ FINAL RECOMMENDATION:\")\n",
    "overall_success_indicators = []\n",
    "if loss_improvements:\n",
    "    loss_success_rate = sum(1 for imp in loss_improvements if imp > 0) / len(loss_improvements)\n",
    "    overall_success_indicators.append(loss_success_rate)\n",
    "if accuracy_improvements:\n",
    "    accuracy_success_rate = sum(1 for imp in accuracy_improvements if imp > 0) / len(accuracy_improvements)\n",
    "    overall_success_indicators.append(accuracy_success_rate)\n",
    "\n",
    "if overall_success_indicators:\n",
    "    overall_success_rate = np.mean(overall_success_indicators)\n",
    "    if overall_success_rate >= 0.5:\n",
    "        print(f\"   ‚úÖ DEPLOY AGGRESSIVE CURRICULUM: Shows consistent improvements\")\n",
    "        print(f\"      Success rate: {overall_success_rate*100:.0f}% of metrics improved\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå STICK WITH BASELINE: Aggressive curriculum does not consistently improve performance\")\n",
    "        print(f\"      Success rate: {overall_success_rate*100:.0f}% of metrics improved\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  INCONCLUSIVE: Insufficient data for reliable recommendation\")\n",
    "    print(f\"      Consider longer training or additional validation metrics\")\n",
    "\n",
    "print(f\"\\nüìã TECHNICAL DETAILS:\")\n",
    "print(f\"   ‚Ä¢ Training steps per task: {list(TASKS.values())[0]['train_steps']}\")\n",
    "print(f\"   ‚Ä¢ Diffusion steps: {COMMON_ARGS['diffusion_steps']}\")\n",
    "print(f\"   ‚Ä¢ Validation interval: Every {COMMON_ARGS['csv_log_interval']} steps\")\n",
    "print(f\"   ‚Ä¢ Key metrics: Total Loss, Task Accuracy\")\n",
    "print(f\"   ‚Ä¢ Models tested: {', '.join(set(task['model'] for task in TASKS.values()))}\")\n",
    "\n",
    "print(f\"\\nüìö TASKS EVALUATED:\")\n",
    "for task_key, task_config in TASKS.items():\n",
    "    success = all(training_results[task_key][curr] == 0 for curr in CURRICULA.keys())\n",
    "    status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "    print(f\"   {status} {task_config['name']}: {task_config['description']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ IRED TASK CURRICULUM COMPARISON COMPLETED SUCCESSFULLY! üöÄ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° NEXT STEPS:\")\n",
    "print(f\"   1. Review detailed task-specific visualizations above\")\n",
    "print(f\"   2. Examine CSV files for numerical analysis\")\n",
    "print(f\"   3. Consider extending to OOD (out-of-distribution) versions if aggressive shows promise\")\n",
    "print(f\"   4. Validate findings with longer training runs if needed\")\n",
    "print(f\"   5. Test on additional IRED tasks if curriculum shows clear benefits\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}