{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRED Energy-Based Model Training on Google Colab (FREE T4 GPU)\n",
    "\n",
    "This notebook implements the **IRED (Iterative Reasoning through Energy Diffusion)** paper's training setup on Google Colab's free T4 GPU.\n",
    "\n",
    "## Paper Reference\n",
    "Du et al. trained for **100,000 iterations on a single NVIDIA RTX 2080 with batch size 512** using Adam optimizer.\n",
    "\n",
    "## Key Settings for IRED (not diffusion baseline):\n",
    "1. **DO NOT** use `--diffusion_steps` flag\n",
    "2. **DO** use `--use-innerloop-opt True` and `--supervise-energy-landscape True`\n",
    "3. Set `--data-workers 2` to avoid DataLoader stalls\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload this notebook to [Google Colab](https://colab.research.google.com)\n",
    "2. Go to Runtime → Change runtime type → GPU → T4\n",
    "3. Run the cells below\n",
    "\n",
    "## Training Time Estimates:\n",
    "- Paper's 100k iterations @ batch 512 = 51.2M examples seen\n",
    "- With batch 2048 (4x larger): ~25k iterations for equivalent training\n",
    "- **Estimated time on T4: ~1.3 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate==1.10.1\n",
    "!pip install -q einops==0.8.1\n",
    "!pip install -q ema_pytorch==0.7.7\n",
    "!pip install -q tabulate==0.9.0\n",
    "!pip install -q tqdm==4.67.1\n",
    "!pip install -q wandb  # Optional for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if needed\n",
    "!mkdir -p data\n",
    "# Add your data download commands here\n",
    "# Example: !wget -O data/dataset.tar.gz https://example.com/dataset.tar.gz\n",
    "# !tar -xzf data/dataset.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoint directory in Google Drive for persistence\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Make sure the target directories exist in Google Drive\n",
    "os.makedirs('/content/drive/MyDrive/ebm_checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/ebm_logs', exist_ok=True)\n",
    "\n",
    "# If symlinks are not supported, fall back to removing and recreating directories\n",
    "# Remove existing local directories if they exist\n",
    "# if os.path.islink('./checkpoints') or os.path.exists('./checkpoints'):\n",
    "#     shutil.rmtree('./checkpoints')\n",
    "# if os.path.islink('./logs') or os.path.exists('./logs'):\n",
    "#     shutil.rmtree('./logs')\n",
    "\n",
    "# Create new directories that point to Google Drive using bind mount\n",
    "!rm -rf ./checkpoints\n",
    "!rm -rf ./logs\n",
    "!mkdir ./checkpoints\n",
    "!mkdir ./logs\n",
    "!mount --bind /content/drive/MyDrive/ebm_checkpoints ./checkpoints\n",
    "!mount --bind /content/drive/MyDrive/ebm_logs ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - IRED paper settings\n",
    "DATASET = \"inverse\"  # Paper uses inverse task\n",
    "MODEL = \"mlp\"  # Default model architecture\n",
    "BATCH_SIZE = 2048  # Can use 2048 (4x paper's 512) for efficiency\n",
    "RANK = 20  # Rank for matrix datasets\n",
    "NUM_WORKERS = 2  # DataLoader suggestion from log\n",
    "\n",
    "# IMPORTANT: For true IRED (not diffusion baseline), we:\n",
    "# 1. DO NOT set diffusion_steps (omit it)\n",
    "# 2. DO set use_innerloop_opt and supervise_energy_landscape flags\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(\"  Mode: IRED (with inner loop optimization)\")\n",
    "print(\"\")\n",
    "print(\"Training time estimates:\")\n",
    "print(\"  Paper's 100k iterations @ batch 512 = 51.2M examples\")\n",
    "print(\"  With batch 2048: ~25k iterations for same examples\")\n",
    "print(\"  Estimated time: ~1.3 hours on T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IRED training (matching the paper's approach)\n",
    "# This runs the actual IRED algorithm, NOT the diffusion baseline\n",
    "%cd /content/energy-based-model\n",
    "!python train.py \\\n",
    "    --dataset {DATASET} \\\n",
    "    --model {MODEL} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --rank {RANK} \\\n",
    "    --data-workers {NUM_WORKERS} \\\n",
    "    --use-innerloop-opt True \\\n",
    "    --supervise-energy-landscape True\n",
    "\n",
    "# Note: We intentionally DO NOT include --diffusion_steps for IRED\n",
    "# The paper trains for 100k iterations at batch 512\n",
    "# With batch 2048 (4x larger), ~25k iterations sees similar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative training configurations\n",
    "\n",
    "# Option 1: IRED with wrapper script (cleaner interface)\n",
    "!python train_wrapper.py \\\n",
    "    --dataset {DATASET} \\\n",
    "    --model {MODEL} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --num_workers {NUM_WORKERS} \\\n",
    "    --use_innerloop_opt \\\n",
    "    --supervise_energy_landscape \\\n",
    "    --checkpoint_dir ./checkpoints \\\n",
    "    --log_dir ./logs\n",
    "\n",
    "# Option 2: Paper's exact settings (batch 512, longer training)\n",
    "# !python train.py \\\n",
    "#     --dataset inverse \\\n",
    "#     --model mlp \\\n",
    "#     --batch_size 512 \\\n",
    "#     --rank 20 \\\n",
    "#     --data-workers 2 \\\n",
    "#     --use-innerloop-opt True \\\n",
    "#     --supervise-energy-landscape True\n",
    "\n",
    "# Option 3: IRED with ANM for enhanced performance\n",
    "# !python train_wrapper.py \\\n",
    "#     --dataset {DATASET} \\\n",
    "#     --model {MODEL} \\\n",
    "#     --batch_size {BATCH_SIZE} \\\n",
    "#     --num_workers {NUM_WORKERS} \\\n",
    "#     --use_innerloop_opt \\\n",
    "#     --supervise_energy_landscape \\\n",
    "#     --use_anm \\\n",
    "#     --anm_steps 10 \\\n",
    "#     --anm_loss_weight 0.5\n",
    "\n",
    "# Option 4: Diffusion baseline (for comparison)\n",
    "# !python train.py \\\n",
    "#     --dataset {DATASET} \\\n",
    "#     --model {MODEL} \\\n",
    "#     --batch_size {BATCH_SIZE} \\\n",
    "#     --diffusion_steps 10 \\\n",
    "#     --rank {RANK} \\\n",
    "#     --data-workers {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage during training\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results to local machine\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Zip checkpoints and logs\n",
    "shutil.make_archive('training_results', 'zip', '.', 'checkpoints')\n",
    "files.download('training_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab:\n",
    "\n",
    "1. **Session Time Limits**: Free Colab has a 12-hour maximum runtime. Save checkpoints frequently!\n",
    "2. **GPU Limits**: You get about 8-12 hours of GPU per day on the free tier\n",
    "3. **Persistent Storage**: Always save important files to Google Drive\n",
    "4. **Idle Timeout**: Colab disconnects after 90 minutes of inactivity\n",
    "5. **Keep Alive**: Use this JavaScript in browser console to prevent disconnection:\n",
    "```javascript\n",
    "function ClickConnect(){\n",
    "    console.log(\"Keeping alive...\");\n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect, 60000)\n",
    "```\n",
    "\n",
    "## Alternative: Colab Pro\n",
    "- $10/month for faster GPUs (V100), longer runtimes, and more RAM\n",
    "- No quota requirements, instant access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
