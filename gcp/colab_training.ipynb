{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRED vs IRED+ANM Evaluation on Google Colab\n",
    "\n",
    "This notebook evaluates the **IRED (Iterative Reasoning through Energy Diffusion)** base implementation against the enhanced **IRED+ANM (Adversarial Negative Mining)** version.\n",
    "\n",
    "## Key Features:\n",
    "- Side-by-side comparison of IRED base vs IRED+ANM\n",
    "- Configurable number of training iterations\n",
    "- Comprehensive metrics tracking (margins, hardness, energy landscape)\n",
    "- Visualization of training dynamics and performance\n",
    "\n",
    "## Paper Reference\n",
    "Du et al. trained for **100,000 iterations on a single NVIDIA RTX 2080 with batch size 512** using Adam optimizer.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload this notebook to [Google Colab](https://colab.research.google.com)\n",
    "2. Go to Runtime → Change runtime type → GPU → T4\n",
    "3. Configure NUM_ITERATIONS below\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TRAINING CONFIGURATION =============\n",
    "# Easily change the number of iterations here\n",
    "NUM_ITERATIONS = 2000  # Options: 1000 (quick test), 5000 (fast eval), 25000 (medium), 100000 (full)\n",
    "\n",
    "# Dataset and model configuration\n",
    "DATASET = \"inverse\"  # Paper uses inverse task\n",
    "MODEL = \"mlp\"  # Default model architecture\n",
    "BATCH_SIZE = 2048  # 4x paper's 512 for efficiency\n",
    "RANK = 20  # Rank for matrix datasets\n",
    "NUM_WORKERS = 2  # DataLoader workers\n",
    "\n",
    "# ANM-specific parameters\n",
    "ANM_STEPS = 10  # Number of adversarial optimization steps\n",
    "ANM_LOSS_WEIGHT = 0.5  # Weight for energy loss when ANM is active\n",
    "ANM_STEP_MULT = 1.0  # Step size multiplier for ANM\n",
    "ANM_ADAPTIVE = False  # Enable timestep-aware ANM\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Number of iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Estimated time: {NUM_ITERATIONS/100000 * 5:.1f} hours on T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate==1.10.1\n",
    "!pip install -q einops==0.8.1\n",
    "!pip install -q ema_pytorch==0.7.7\n",
    "!pip install -q tabulate==0.9.0\n",
    "!pip install -q tqdm==4.67.1\n",
    "!pip install -q wandb  # Optional for logging\n",
    "!pip install -q matplotlib seaborn pandas  # For visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Experiment Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment directories with timestamps\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "experiment_name = f\"ired_vs_anm_{NUM_ITERATIONS}iters_{timestamp}\"\n",
    "\n",
    "# Base directories in Google Drive\n",
    "base_dir = f'/content/drive/MyDrive/ebm_experiments/{experiment_name}'\n",
    "ired_base_dir = f'{base_dir}/ired_base'\n",
    "ired_anm_dir = f'{base_dir}/ired_anm'\n",
    "comparison_dir = f'{base_dir}/comparison'\n",
    "\n",
    "# Create all directories\n",
    "for dir_path in [ired_base_dir, ired_anm_dir, comparison_dir]:\n",
    "    os.makedirs(f'{dir_path}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{dir_path}/logs', exist_ok=True)\n",
    "    os.makedirs(f'{dir_path}/metrics', exist_ok=True)\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Results will be saved to: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ANM Documentation\n",
    "\n",
    "### What is Adversarial Negative Mining (ANM)?\n",
    "\n",
    "ANM enhances IRED by generating **harder negative samples** through adversarial optimization. Instead of using random corruptions, ANM:\n",
    "1. Starts with a noisy sample\n",
    "2. Optimizes it to minimize energy (making it \"plausible but wrong\")\n",
    "3. Uses these hard negatives to better shape the energy landscape\n",
    "\n",
    "### Key Metrics to Track:\n",
    "\n",
    "- **Margin** ($m = E_{\\theta}(x, \\tilde{y}^-) - E_{\\theta}(x, \\tilde{y}^+)$): Should increase over training\n",
    "- **Hardness** ($p_{hard} = P[E(y^-) < E(y^+)]$): Fraction of hard negatives, should start high and decrease\n",
    "- **Gradient Share Ratio**: Balance between MSE and contrastive loss gradients\n",
    "- **Energy Reduction**: How much ANM reduces energy during optimization\n",
    "\n",
    "### Expected Behavior:\n",
    "\n",
    "If ANM is working correctly:\n",
    "- Faster early improvement in loss\n",
    "- Higher final margins on validation set\n",
    "- More stable energy landscape\n",
    "- Better generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modified Training Script with Metrics Tracking\n",
    "\n",
    "We'll create a modified training script that logs the metrics we need for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metrics tracking wrapper script\n",
    "metrics_script = '''\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/energy-based-model')\n",
    "os.chdir('/content/energy-based-model')\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "# Import the training components\n",
    "from train import *\n",
    "\n",
    "def track_metrics(trainer, metrics_file):\n",
    "    \"\"\"Hook to track additional metrics during training\"\"\"\n",
    "    metrics = {\n",
    "        'iteration': trainer.step,\n",
    "        'timestamp': time.time(),\n",
    "    }\n",
    "    \n",
    "    # Get loss values if available\n",
    "    if hasattr(trainer, 'last_loss'):\n",
    "        metrics['loss'] = float(trainer.last_loss)\n",
    "    \n",
    "    # Track ANM-specific metrics if available\n",
    "    if hasattr(trainer.diffusion, 'last_anm_stats'):\n",
    "        metrics.update(trainer.diffusion.last_anm_stats)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(metrics_file, 'a') as f:\n",
    "        f.write(json.dumps(metrics) + '\\\\n')\n",
    "\n",
    "def run_training_with_metrics(config_name, use_anm, num_iterations, output_dir):\n",
    "    \"\"\"Run training with metrics tracking\"\"\"\n",
    "    \n",
    "    # Set up FLAGS\n",
    "    FLAGS = argparse.Namespace(\n",
    "        dataset='{DATASET}',\n",
    "        model='{MODEL}',\n",
    "        batch_size={BATCH_SIZE},\n",
    "        rank={RANK},\n",
    "        data_workers={NUM_WORKERS},\n",
    "        diffusion_steps=10,\n",
    "        supervise_energy_landscape=True,\n",
    "        use_innerloop_opt=True,\n",
    "        use_anm=use_anm,\n",
    "        anm_steps={ANM_STEPS} if use_anm else 0,\n",
    "        anm_step_mult={ANM_STEP_MULT} if use_anm else 1.0,\n",
    "        anm_loss_weight={ANM_LOSS_WEIGHT} if use_anm else 0.0,\n",
    "        anm_adaptive={ANM_ADAPTIVE} if use_anm else False,\n",
    "        cond_mask=False,\n",
    "        evaluate=False,\n",
    "        latent=False,\n",
    "        ood=False,\n",
    "        baseline=False,\n",
    "        load_milestone=None\n",
    "    )\n",
    "    \n",
    "    # Load dataset and model (same as train.py)\n",
    "    dataset = Inverse(\"train\", FLAGS.rank, FLAGS.ood)\n",
    "    validation_dataset = dataset\n",
    "    metric = 'mse'\n",
    "    \n",
    "    model = EBM(\n",
    "        inp_dim=dataset.inp_dim,\n",
    "        out_dim=dataset.out_dim,\n",
    "    )\n",
    "    model = DiffusionWrapper(model)\n",
    "    \n",
    "    # Set up diffusion with ANM if enabled\n",
    "    kwargs = {'continuous': True}\n",
    "    if use_anm:\n",
    "        kwargs.update({\n",
    "            'use_anm': True,\n",
    "            'anm_steps': FLAGS.anm_steps,\n",
    "            'anm_step_mult': FLAGS.anm_step_mult,\n",
    "            'anm_loss_weight': FLAGS.anm_loss_weight,\n",
    "            'anm_adaptive': FLAGS.anm_adaptive\n",
    "        })\n",
    "    \n",
    "    diffusion = GaussianDiffusion1D(\n",
    "        model,\n",
    "        seq_length=32,\n",
    "        objective='pred_noise',\n",
    "        timesteps=FLAGS.diffusion_steps,\n",
    "        sampling_timesteps=FLAGS.diffusion_steps,\n",
    "        supervise_energy_landscape=FLAGS.supervise_energy_landscape,\n",
    "        use_innerloop_opt=FLAGS.use_innerloop_opt,\n",
    "        show_inference_tqdm=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Set up trainer with custom number of iterations\n",
    "    trainer = Trainer1D(\n",
    "        diffusion,\n",
    "        dataset,\n",
    "        train_batch_size=FLAGS.batch_size,\n",
    "        validation_batch_size=256,\n",
    "        train_lr=1e-4,\n",
    "        train_num_steps=num_iterations,  # Use our configurable iterations\n",
    "        gradient_accumulate_every=1,\n",
    "        ema_decay=0.995,\n",
    "        data_workers=FLAGS.data_workers,\n",
    "        amp=False,\n",
    "        metric=metric,\n",
    "        results_folder=f'{output_dir}/checkpoints',\n",
    "        cond_mask=FLAGS.cond_mask,\n",
    "        validation_dataset=validation_dataset,\n",
    "        extra_validation_datasets={},\n",
    "        extra_validation_every_mul=10,\n",
    "        save_and_sample_every=max(1000, num_iterations // 10),\n",
    "        evaluate_first=False,\n",
    "        latent=False,\n",
    "        autoencode_model=None\n",
    "    )\n",
    "    \n",
    "    # Add metrics tracking hook\n",
    "    metrics_file = f'{output_dir}/metrics/training_metrics.jsonl'\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Starting {config_name} training for {num_iterations} iterations...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Training complete! Results saved to {output_dir}\")\n",
    "    return trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    config_name = sys.argv[1]\n",
    "    use_anm = sys.argv[2] == 'True'\n",
    "    num_iterations = int(sys.argv[3])\n",
    "    output_dir = sys.argv[4]\n",
    "    \n",
    "    run_training_with_metrics(config_name, use_anm, num_iterations, output_dir)\n",
    "'''\n",
    "\n",
    "# Save the script\n",
    "with open('/content/train_with_metrics.py', 'w') as f:\n",
    "    f.write(metrics_script.format(\n",
    "        DATASET=DATASET,\n",
    "        MODEL=MODEL,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        RANK=RANK,\n",
    "        NUM_WORKERS=NUM_WORKERS,\n",
    "        ANM_STEPS=ANM_STEPS,\n",
    "        ANM_LOSS_WEIGHT=ANM_LOSS_WEIGHT,\n",
    "        ANM_STEP_MULT=ANM_STEP_MULT,\n",
    "        ANM_ADAPTIVE=str(ANM_ADAPTIVE)\n",
    "    ))\n",
    "\n",
    "print(\"Metrics tracking script created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train IRED Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IRED base model (without ANM)\n",
    "print(\"=\"*60)\n",
    "print(\"Training IRED Base Model (without ANM)\")\n",
    "print(f\"Iterations: {NUM_ITERATIONS}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Change to the repository directory\n",
    "%cd /content/energy-based-model\n",
    "\n",
    "# Run training\n",
    "!python /content/train_with_metrics.py \"IRED_Base\" False {NUM_ITERATIONS} \"{ired_base_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train IRED+ANM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IRED+ANM model\n",
    "print(\"=\"*60)\n",
    "print(\"Training IRED+ANM Model (with Adversarial Negative Mining)\")\n",
    "print(f\"Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"ANM Steps: {ANM_STEPS}\")\n",
    "print(f\"ANM Loss Weight: {ANM_LOSS_WEIGHT}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Change to the repository directory\n",
    "%cd /content/energy-based-model\n",
    "\n",
    "# Run training\n",
    "!python /content/train_with_metrics.py \"IRED_ANM\" True {NUM_ITERATIONS} \"{ired_anm_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load metrics from jsonl files\n",
    "def load_metrics(metrics_file):\n",
    "    metrics = []\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    metrics.append(json.loads(line))\n",
    "                except:\n",
    "                    pass\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Load metrics for both models\n",
    "ired_base_metrics = load_metrics(f'{ired_base_dir}/metrics/training_metrics.jsonl')\n",
    "ired_anm_metrics = load_metrics(f'{ired_anm_dir}/metrics/training_metrics.jsonl')\n",
    "\n",
    "print(f\"IRED Base: {len(ired_base_metrics)} metric entries\")\n",
    "print(f\"IRED+ANM: {len(ired_anm_metrics)} metric entries\")\n",
    "\n",
    "# Display sample metrics\n",
    "if len(ired_base_metrics) > 0:\n",
    "    print(\"\\nIRED Base - Last 5 entries:\")\n",
    "    print(ired_base_metrics.tail())\n",
    "\n",
    "if len(ired_anm_metrics) > 0:\n",
    "    print(\"\\nIRED+ANM - Last 5 entries:\")\n",
    "    print(ired_anm_metrics.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Training Loss Comparison\n",
    "if 'loss' in ired_base_metrics.columns and 'loss' in ired_anm_metrics.columns:\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(ired_base_metrics['iteration'], ired_base_metrics['loss'], \n",
    "            label='IRED Base', alpha=0.8, linewidth=2)\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_metrics['loss'], \n",
    "            label='IRED+ANM', alpha=0.8, linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: ANM Energy Reduction (if available)\n",
    "if 'anm_energy_reduction' in ired_anm_metrics.columns:\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_metrics['anm_energy_reduction'], \n",
    "            color='orange', alpha=0.8, linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Energy Reduction')\n",
    "    ax.set_title('ANM Energy Reduction During Mining')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: ANM Optimization Movement\n",
    "if 'anm_optimization_movement' in ired_anm_metrics.columns:\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_metrics['anm_optimization_movement'], \n",
    "            color='green', alpha=0.8, linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Movement')\n",
    "    ax.set_title('ANM Optimization Movement')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Loss Smoothed Comparison\n",
    "if 'loss' in ired_base_metrics.columns and 'loss' in ired_anm_metrics.columns:\n",
    "    ax = axes[1, 0]\n",
    "    # Apply rolling mean for smoother curves\n",
    "    window = max(10, NUM_ITERATIONS // 100)\n",
    "    ired_base_smooth = ired_base_metrics['loss'].rolling(window=window, min_periods=1).mean()\n",
    "    ired_anm_smooth = ired_anm_metrics['loss'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    ax.plot(ired_base_metrics['iteration'], ired_base_smooth, \n",
    "            label='IRED Base (smoothed)', alpha=0.8, linewidth=2)\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_smooth, \n",
    "            label='IRED+ANM (smoothed)', alpha=0.8, linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss (smoothed)')\n",
    "    ax.set_title(f'Smoothed Loss (window={window})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Final Energy Comparison\n",
    "if 'anm_final_energy' in ired_anm_metrics.columns and 'anm_initial_energy' in ired_anm_metrics.columns:\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_metrics['anm_initial_energy'], \n",
    "            label='Initial Energy', alpha=0.6, linewidth=1)\n",
    "    ax.plot(ired_anm_metrics['iteration'], ired_anm_metrics['anm_final_energy'], \n",
    "            label='Final Energy (after ANM)', alpha=0.8, linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Energy')\n",
    "    ax.set_title('ANM Energy Before/After Optimization')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Training Progress\n",
    "ax = axes[1, 2]\n",
    "progress_data = pd.DataFrame({\n",
    "    'Model': ['IRED Base', 'IRED+ANM'],\n",
    "    'Iterations': [len(ired_base_metrics), len(ired_anm_metrics)],\n",
    "    'Target': [NUM_ITERATIONS, NUM_ITERATIONS]\n",
    "})\n",
    "x = range(len(progress_data))\n",
    "width = 0.35\n",
    "ax.bar([i - width/2 for i in x], progress_data['Iterations'], width, label='Completed', color='green', alpha=0.7)\n",
    "ax.bar([i + width/2 for i in x], progress_data['Target'] - progress_data['Iterations'], width, \n",
    "       bottom=progress_data['Iterations'], label='Remaining', color='gray', alpha=0.3)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(progress_data['Model'])\n",
    "ax.set_ylabel('Iterations')\n",
    "ax.set_title('Training Progress')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{comparison_dir}/training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plots saved to {comparison_dir}/training_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "summary = {}\n",
    "\n",
    "# IRED Base statistics\n",
    "if len(ired_base_metrics) > 0 and 'loss' in ired_base_metrics.columns:\n",
    "    summary['IRED Base'] = {\n",
    "        'Final Loss': ired_base_metrics['loss'].iloc[-1] if len(ired_base_metrics) > 0 else None,\n",
    "        'Min Loss': ired_base_metrics['loss'].min(),\n",
    "        'Mean Loss': ired_base_metrics['loss'].mean(),\n",
    "        'Loss Std': ired_base_metrics['loss'].std(),\n",
    "        'Total Iterations': len(ired_base_metrics)\n",
    "    }\n",
    "\n",
    "# IRED+ANM statistics\n",
    "if len(ired_anm_metrics) > 0 and 'loss' in ired_anm_metrics.columns:\n",
    "    summary['IRED+ANM'] = {\n",
    "        'Final Loss': ired_anm_metrics['loss'].iloc[-1] if len(ired_anm_metrics) > 0 else None,\n",
    "        'Min Loss': ired_anm_metrics['loss'].min(),\n",
    "        'Mean Loss': ired_anm_metrics['loss'].mean(),\n",
    "        'Loss Std': ired_anm_metrics['loss'].std(),\n",
    "        'Total Iterations': len(ired_anm_metrics)\n",
    "    }\n",
    "    \n",
    "    # Add ANM-specific statistics\n",
    "    if 'anm_energy_reduction' in ired_anm_metrics.columns:\n",
    "        summary['IRED+ANM']['Mean Energy Reduction'] = ired_anm_metrics['anm_energy_reduction'].mean()\n",
    "        summary['IRED+ANM']['Max Energy Reduction'] = ired_anm_metrics['anm_energy_reduction'].max()\n",
    "    \n",
    "    if 'anm_optimization_movement' in ired_anm_metrics.columns:\n",
    "        summary['IRED+ANM']['Mean ANM Movement'] = ired_anm_metrics['anm_optimization_movement'].mean()\n",
    "\n",
    "# Display summary\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Save summary to file\n",
    "summary_df.to_csv(f'{comparison_dir}/summary_statistics.csv')\n",
    "with open(f'{comparison_dir}/summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to {comparison_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'IRED Base' in summary and 'IRED+ANM' in summary:\n",
    "    base_final = summary['IRED Base'].get('Final Loss', float('inf'))\n",
    "    anm_final = summary['IRED+ANM'].get('Final Loss', float('inf'))\n",
    "    \n",
    "    if base_final and anm_final and base_final != float('inf') and anm_final != float('inf'):\n",
    "        improvement = (base_final - anm_final) / base_final * 100\n",
    "        \n",
    "        print(f\"\\nFinal Loss Comparison:\")\n",
    "        print(f\"  IRED Base:  {base_final:.6f}\")\n",
    "        print(f\"  IRED+ANM:   {anm_final:.6f}\")\n",
    "        print(f\"  Improvement: {improvement:.2f}%\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"\\n✅ ANM shows {improvement:.2f}% improvement in final loss!\")\n",
    "        elif improvement < 0:\n",
    "            print(f\"\\n⚠️ ANM shows {-improvement:.2f}% degradation in final loss.\")\n",
    "        else:\n",
    "            print(f\"\\n➡️ ANM shows similar performance to base IRED.\")\n",
    "    \n",
    "    # Compare convergence speed\n",
    "    if len(ired_base_metrics) > 100 and len(ired_anm_metrics) > 100:\n",
    "        # Check loss at 25%, 50%, 75% of training\n",
    "        checkpoints = [0.25, 0.5, 0.75]\n",
    "        \n",
    "        print(f\"\\nConvergence Speed (Loss at checkpoints):\")\n",
    "        for checkpoint in checkpoints:\n",
    "            idx = int(len(ired_base_metrics) * checkpoint)\n",
    "            base_loss = ired_base_metrics['loss'].iloc[min(idx, len(ired_base_metrics)-1)]\n",
    "            anm_loss = ired_anm_metrics['loss'].iloc[min(idx, len(ired_anm_metrics)-1)]\n",
    "            print(f\"  At {checkpoint*100:.0f}% training:\")\n",
    "            print(f\"    IRED Base: {base_loss:.6f}\")\n",
    "            print(f\"    IRED+ANM:  {anm_loss:.6f}\")\n",
    "            print(f\"    Difference: {(base_loss - anm_loss):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive report\n",
    "report = {\n",
    "    'experiment': experiment_name,\n",
    "    'configuration': {\n",
    "        'num_iterations': NUM_ITERATIONS,\n",
    "        'dataset': DATASET,\n",
    "        'model': MODEL,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'rank': RANK,\n",
    "        'anm_steps': ANM_STEPS,\n",
    "        'anm_loss_weight': ANM_LOSS_WEIGHT,\n",
    "        'anm_step_mult': ANM_STEP_MULT,\n",
    "        'anm_adaptive': ANM_ADAPTIVE\n",
    "    },\n",
    "    'summary': summary,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save report\n",
    "with open(f'{comparison_dir}/experiment_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Experiment report saved to {comparison_dir}/experiment_report.json\")\n",
    "\n",
    "# Zip results for download\n",
    "import shutil\n",
    "zip_path = f'/content/{experiment_name}_results'\n",
    "shutil.make_archive(zip_path, 'zip', base_dir)\n",
    "\n",
    "print(f\"\\nResults archived at: {zip_path}.zip\")\n",
    "print(f\"Size: {os.path.getsize(zip_path + '.zip') / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "files.download(f'{zip_path}.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Test Commands\n",
    "\n",
    "For manual testing, you can also run these commands directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test commands (uncomment to run)\n",
    "\n",
    "# Test IRED Base (1000 iterations)\n",
    "# !cd /content/energy-based-model && python train.py --dataset inverse --model mlp --batch_size 2048 --rank 20 --data-workers 2 --use-innerloop-opt True --supervise-energy-landscape True\n",
    "\n",
    "# Test IRED+ANM (1000 iterations)\n",
    "# !cd /content/energy-based-model && python train.py --dataset inverse --model mlp --batch_size 2048 --rank 20 --data-workers 2 --use-innerloop-opt True --supervise-energy-landscape True --use-anm True --anm-steps 10 --anm-loss-weight 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab:\n",
    "\n",
    "1. **Session Time Limits**: Free Colab has a 12-hour maximum runtime. Save checkpoints frequently!\n",
    "2. **GPU Limits**: You get about 8-12 hours of GPU per day on the free tier\n",
    "3. **Persistent Storage**: Always save important files to Google Drive\n",
    "4. **Idle Timeout**: Colab disconnects after 90 minutes of inactivity\n",
    "5. **Keep Alive**: Use this JavaScript in browser console to prevent disconnection:\n",
    "```javascript\n",
    "function ClickConnect(){\n",
    "    console.log(\"Keeping alive...\");\n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect, 60000)\n",
    "```\n",
    "\n",
    "## Interpreting Results:\n",
    "\n",
    "### Good ANM Performance Indicators:\n",
    "- **Lower final loss** compared to base IRED\n",
    "- **Faster early convergence** (steeper loss curve at beginning)\n",
    "- **Consistent energy reduction** during ANM optimization\n",
    "- **Moderate optimization movement** (not too small, not too large)\n",
    "\n",
    "### Warning Signs:\n",
    "- **Flat or increasing loss** → Check learning rate, may need adjustment\n",
    "- **Very small energy reduction** → ANM may not be finding hard negatives\n",
    "- **Unstable loss curves** → Reduce ANM step size or loss weight\n",
    "- **No difference from base** → Check if ANM is actually being used\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Tune ANM parameters** if results are not satisfactory:\n",
    "   - Increase `ANM_STEPS` for more thorough optimization\n",
    "   - Adjust `ANM_LOSS_WEIGHT` to balance MSE vs contrastive loss\n",
    "   - Try `ANM_ADAPTIVE=True` for timestep-aware optimization\n",
    "\n",
    "2. **Run longer experiments** for more conclusive results\n",
    "3. **Test on different datasets** to verify generalization\n",
    "4. **Implement additional metrics** from the documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
