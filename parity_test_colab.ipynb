{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANS Parity Tests - Comprehensive Comparison\n",
    "\n",
    "This notebook runs parity tests to identify if SANS performance issues are in the weighting mechanism or deeper in the implementation.\n",
    "\n",
    "## Test Configurations:\n",
    "1. **Baseline iREM** - Original implementation without SANS\n",
    "2. **SANS Normal** - SANS with α=2.0, K=16, temperature schedule\n",
    "3. **SANS Parity (α=0)** - SANS with uniform weighting (α=0, K=16)\n",
    "\n",
    "## Key Questions:\n",
    "- Does SANS(α=0) match baseline performance? (Should be within ±1-2%)\n",
    "- If not, the SANS codepath has structural differences beyond weighting\n",
    "- Are the negative terms computed equivalently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    if 'T4' in gpu_name:\n",
    "        print(\"✓ T4 GPU detected\")\n",
    "        batch_size = 256\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"✓ V100 GPU detected\")\n",
    "        batch_size = 512\n",
    "    else:\n",
    "        print(f\"✓ {gpu_name} detected\")\n",
    "        batch_size = 128\n",
    "else:\n",
    "    print(\"⚠ No GPU detected - running on CPU (will be slower)\")\n",
    "    batch_size = 32\n",
    "\n",
    "print(f\"\\nUsing batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository or use existing files\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "\n",
    "import os\n",
    "if not os.path.exists('energy-based-model'):\n",
    "    print(\"Repository not found. Please either:\")\n",
    "    print(\"1. Update the git clone URL with your repository\")\n",
    "    print(\"2. Upload the energy-based-model folder to Colab\")\n",
    "    os.makedirs('energy-based-model', exist_ok=True)\n",
    "\n",
    "os.chdir('energy-based-model')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate ema-pytorch einops tabulate tqdm matplotlib seaborn pandas\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Modules and Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Import training modules\n",
    "from diffusion_lib.denoising_diffusion_pytorch_1d import GaussianDiffusion1D, Trainer1D\n",
    "from models import EBM, DiffusionWrapper\n",
    "from dataset import Inverse, Addition, LowRankDataset\n",
    "from irem_lib.irem import Trainer1D as iREMTrainer1D\n",
    "\n",
    "print(\"✓ Modules imported successfully\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    # Dataset parameters\n",
    "    'dataset': 'inverse',\n",
    "    'rank': 10,\n",
    "    'ood': False,\n",
    "    \n",
    "    # Model parameters\n",
    "    'model': 'mlp',\n",
    "    'diffusion_steps': 10,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_steps': 5000,  # Run for 5k steps\n",
    "    'val_every': 250,\n",
    "    'save_every': 1000,\n",
    "    \n",
    "    # Common parameters\n",
    "    'supervise_energy_landscape': True,\n",
    "    'use_innerloop_opt': False,\n",
    "    'data_workers': 2,\n",
    "    'ema_decay': 0.995,\n",
    "    'gradient_accumulate_every': 1,\n",
    "    'amp': False,\n",
    "    \n",
    "    # Base results directory\n",
    "    'results_dir': 'parity_test_results',\n",
    "}\n",
    "\n",
    "# Test configurations for parity testing\n",
    "TEST_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'sans_enabled': False,\n",
    "        'description': 'Baseline iREM (no SANS)',\n",
    "        'color': 'blue',\n",
    "        'marker': 'o'\n",
    "    },\n",
    "    'sans_normal': {\n",
    "        'sans_enabled': True,\n",
    "        'sans_temp': 2.0,\n",
    "        'sans_num_negs': 16,\n",
    "        'sans_temp_schedule': True,\n",
    "        'description': 'SANS (α=2.0, K=16, scheduled)',\n",
    "        'color': 'red',\n",
    "        'marker': 's'\n",
    "    },\n",
    "    'sans_alpha0': {\n",
    "        'sans_enabled': True,\n",
    "        'sans_temp': 0.0,  # Force uniform weighting\n",
    "        'sans_num_negs': 16,\n",
    "        'sans_temp_schedule': False,  # No schedule with α=0\n",
    "        'description': 'SANS Parity (α=0, K=16, uniform)',\n",
    "        'color': 'green',\n",
    "        'marker': '^'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Test Configurations:\")\n",
    "print(\"=\"*60)\n",
    "for name, config in TEST_CONFIGS.items():\n",
    "    print(f\"  {name}: {config['description']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "if BASE_CONFIG['dataset'] == 'inverse':\n",
    "    dataset = Inverse(\"train\", BASE_CONFIG['rank'], BASE_CONFIG['ood'])\n",
    "elif BASE_CONFIG['dataset'] == 'addition':\n",
    "    dataset = Addition(\"train\", BASE_CONFIG['rank'], BASE_CONFIG['ood'])\n",
    "elif BASE_CONFIG['dataset'] == 'lowrank':\n",
    "    dataset = LowRankDataset(\"train\", BASE_CONFIG['rank'], BASE_CONFIG['ood'])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {BASE_CONFIG['dataset']}\")\n",
    "\n",
    "validation_dataset = dataset\n",
    "metric = 'mse'\n",
    "\n",
    "print(f\"Dataset: {BASE_CONFIG['dataset']}\")\n",
    "print(f\"  Input dimension: {dataset.inp_dim}\")\n",
    "print(f\"  Output dimension: {dataset.out_dim}\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unified Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(config_name, test_config, base_config, dataset, validation_dataset, capture_output=True):\n",
    "    \"\"\"\n",
    "    Run training for a specific configuration\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of the configuration (e.g., 'baseline', 'sans_normal')\n",
    "        test_config: Specific test configuration parameters\n",
    "        base_config: Base configuration parameters\n",
    "        dataset: Training dataset\n",
    "        validation_dataset: Validation dataset\n",
    "        capture_output: Whether to capture console output for debugging\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results and captured output\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RUNNING: {test_config['description']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = Path(base_config['results_dir']) / config_name\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Merge configurations\n",
    "    config = {**base_config, **test_config}\n",
    "    \n",
    "    # Save configuration\n",
    "    with open(results_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Create model\n",
    "    model = EBM(\n",
    "        inp_dim=dataset.inp_dim,\n",
    "        out_dim=dataset.out_dim,\n",
    "    )\n",
    "    model = DiffusionWrapper(model)\n",
    "    \n",
    "    # Capture output if requested\n",
    "    captured_output = \"\"\n",
    "    if capture_output:\n",
    "        output_buffer = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        if config.get('sans_enabled', False):\n",
    "            # SANS configuration\n",
    "            print(f\"  SANS Config: α={config.get('sans_temp', 1.0)}, K={config.get('sans_num_negs', 16)}, \"\n",
    "                  f\"schedule={config.get('sans_temp_schedule', True)}\")\n",
    "            \n",
    "            # Create diffusion with SANS\n",
    "            diffusion = GaussianDiffusion1D(\n",
    "                model,\n",
    "                seq_length=32,\n",
    "                objective='pred_noise',\n",
    "                timesteps=config['diffusion_steps'],\n",
    "                sampling_timesteps=config['diffusion_steps'],\n",
    "                supervise_energy_landscape=config['supervise_energy_landscape'],\n",
    "                use_innerloop_opt=config['use_innerloop_opt'],\n",
    "                show_inference_tqdm=False,\n",
    "                # SANS parameters\n",
    "                sans_enabled=True,\n",
    "                sans_num_negs=config.get('sans_num_negs', 16),\n",
    "                sans_temp=config.get('sans_temp', 2.0),\n",
    "                sans_temp_schedule=config.get('sans_temp_schedule', True),\n",
    "                sans_chunk=config.get('sans_chunk', 0),\n",
    "                continuous=True  # For inverse dataset\n",
    "            )\n",
    "            \n",
    "            # Create SANS trainer\n",
    "            if capture_output:\n",
    "                with redirect_stdout(output_buffer):\n",
    "                    trainer = Trainer1D(\n",
    "                        diffusion,\n",
    "                        dataset,\n",
    "                        train_batch_size=config['batch_size'],\n",
    "                        validation_batch_size=min(256, config['batch_size']),\n",
    "                        train_lr=config['learning_rate'],\n",
    "                        train_num_steps=config['num_steps'],\n",
    "                        gradient_accumulate_every=config['gradient_accumulate_every'],\n",
    "                        ema_decay=config['ema_decay'],\n",
    "                        data_workers=config['data_workers'],\n",
    "                        amp=config['amp'],\n",
    "                        metric='mse',\n",
    "                        results_folder=str(results_dir),\n",
    "                        cond_mask=False,\n",
    "                        validation_dataset=validation_dataset,\n",
    "                        save_and_sample_every=config['save_every'],\n",
    "                        evaluate_first=False\n",
    "                    )\n",
    "                    trainer.train()\n",
    "            else:\n",
    "                trainer = Trainer1D(\n",
    "                    diffusion,\n",
    "                    dataset,\n",
    "                    train_batch_size=config['batch_size'],\n",
    "                    validation_batch_size=min(256, config['batch_size']),\n",
    "                    train_lr=config['learning_rate'],\n",
    "                    train_num_steps=config['num_steps'],\n",
    "                    gradient_accumulate_every=config['gradient_accumulate_every'],\n",
    "                    ema_decay=config['ema_decay'],\n",
    "                    data_workers=config['data_workers'],\n",
    "                    amp=config['amp'],\n",
    "                    metric='mse',\n",
    "                    results_folder=str(results_dir),\n",
    "                    cond_mask=False,\n",
    "                    validation_dataset=validation_dataset,\n",
    "                    save_and_sample_every=config['save_every'],\n",
    "                    evaluate_first=False\n",
    "                )\n",
    "                trainer.train()\n",
    "        else:\n",
    "            # Baseline iREM configuration\n",
    "            print(\"  Using baseline iREM (no SANS)\")\n",
    "            \n",
    "            if capture_output:\n",
    "                with redirect_stdout(output_buffer):\n",
    "                    trainer = iREMTrainer1D(\n",
    "                        model,\n",
    "                        dataset,\n",
    "                        train_batch_size=config['batch_size'],\n",
    "                        validation_batch_size=min(256, config['batch_size']),\n",
    "                        train_lr=config['learning_rate'],\n",
    "                        train_num_steps=config['num_steps'],\n",
    "                        gradient_accumulate_every=config['gradient_accumulate_every'],\n",
    "                        ema_decay=config['ema_decay'],\n",
    "                        data_workers=config['data_workers'],\n",
    "                        amp=config['amp'],\n",
    "                        metric='mse',\n",
    "                        results_folder=str(results_dir),\n",
    "                        cond_mask=False,\n",
    "                        validation_dataset=validation_dataset,\n",
    "                        save_and_sample_every=config['save_every'],\n",
    "                        evaluate_first=False\n",
    "                    )\n",
    "                    trainer.train()\n",
    "            else:\n",
    "                trainer = iREMTrainer1D(\n",
    "                    model,\n",
    "                    dataset,\n",
    "                    train_batch_size=config['batch_size'],\n",
    "                    validation_batch_size=min(256, config['batch_size']),\n",
    "                    train_lr=config['learning_rate'],\n",
    "                    train_num_steps=config['num_steps'],\n",
    "                    gradient_accumulate_every=config['gradient_accumulate_every'],\n",
    "                    ema_decay=config['ema_decay'],\n",
    "                    data_workers=config['data_workers'],\n",
    "                    amp=config['amp'],\n",
    "                    metric='mse',\n",
    "                    results_folder=str(results_dir),\n",
    "                    cond_mask=False,\n",
    "                    validation_dataset=validation_dataset,\n",
    "                    save_and_sample_every=config['save_every'],\n",
    "                    evaluate_first=False\n",
    "                )\n",
    "                trainer.train()\n",
    "        \n",
    "        print(f\"\\n✓ {config_name} training complete\")\n",
    "        \n",
    "        if capture_output:\n",
    "            captured_output = output_buffer.getvalue()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Training error for {config_name}: {str(e)}\")\n",
    "        if capture_output:\n",
    "            captured_output = output_buffer.getvalue()\n",
    "    \n",
    "    return {\n",
    "        'config_name': config_name,\n",
    "        'results_dir': results_dir,\n",
    "        'captured_output': captured_output\n",
    "    }\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run All Test Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all test configurations\n",
    "results = {}\n",
    "\n",
    "for config_name, test_config in TEST_CONFIGS.items():\n",
    "    result = run_training(\n",
    "        config_name=config_name,\n",
    "        test_config=test_config,\n",
    "        base_config=BASE_CONFIG,\n",
    "        dataset=dataset,\n",
    "        validation_dataset=validation_dataset,\n",
    "        capture_output=(config_name.startswith('sans'))  # Capture output for SANS runs\n",
    "    )\n",
    "    results[config_name] = result\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TRAINING RUNS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract SANS Debug Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display SANS debug information\n",
    "for config_name, result in results.items():\n",
    "    if 'sans' in config_name and result['captured_output']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DEBUG OUTPUT: {TEST_CONFIGS[config_name]['description']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Extract SANS debug lines\n",
    "        debug_lines = []\n",
    "        for line in result['captured_output'].split('\\n'):\n",
    "            if '[SANS Debug]' in line or any(x in line for x in ['Neg energy:', 'Real energy:', 'Weight', 'Corr', 'Alpha', 'Loss components:']):\n",
    "                debug_lines.append(line)\n",
    "        \n",
    "        if debug_lines:\n",
    "            print(\"\\nSample Debug Output (first occurrence):\")\n",
    "            # Find first complete debug block\n",
    "            for i, line in enumerate(debug_lines):\n",
    "                if '[SANS Debug]' in line and i + 10 < len(debug_lines):\n",
    "                    for j in range(min(11, len(debug_lines) - i)):\n",
    "                        print(debug_lines[i + j])\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No debug output captured (may need to increase logging frequency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Analyze Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from all runs\n",
    "metrics_data = {}\n",
    "\n",
    "for config_name, result in results.items():\n",
    "    metrics_path = result['results_dir'] / 'metrics.csv'\n",
    "    \n",
    "    if metrics_path.exists():\n",
    "        df = pd.read_csv(metrics_path)\n",
    "        df['config'] = config_name\n",
    "        df['description'] = TEST_CONFIGS[config_name]['description']\n",
    "        metrics_data[config_name] = df\n",
    "        print(f\"✓ Loaded metrics for {config_name}: {len(df)} steps\")\n",
    "    else:\n",
    "        print(f\"⚠ No metrics file for {config_name}\")\n",
    "\n",
    "# Combine all metrics\n",
    "if metrics_data:\n",
    "    all_metrics = pd.concat(list(metrics_data.values()), ignore_index=True)\n",
    "    print(f\"\\n✓ Total metrics loaded: {len(all_metrics)} rows\")\n",
    "else:\n",
    "    print(\"\\n⚠ No metrics data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parity Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parity comparison table\n",
    "def get_metrics_at_step(df, step, window=50):\n",
    "    \"\"\"Get metrics near a specific step with averaging window\"\"\"\n",
    "    mask = (df['step'] >= step - window) & (df['step'] <= step + window)\n",
    "    if mask.sum() > 0:\n",
    "        return {\n",
    "            'loss': df.loc[mask, 'loss'].mean(),\n",
    "            'val_loss': df.loc[mask, 'val_loss'].dropna().mean() if 'val_loss' in df else None,\n",
    "            'actual_step': df.loc[mask, 'step'].mean()\n",
    "        }\n",
    "    return {'loss': None, 'val_loss': None, 'actual_step': None}\n",
    "\n",
    "# Build comparison table\n",
    "comparison_steps = [1000, 5000]\n",
    "comparison_data = []\n",
    "\n",
    "for step in comparison_steps:\n",
    "    row = {'Step': step}\n",
    "    \n",
    "    for config_name in ['baseline', 'sans_normal', 'sans_alpha0']:\n",
    "        if config_name in metrics_data:\n",
    "            metrics = get_metrics_at_step(metrics_data[config_name], step)\n",
    "            \n",
    "            if metrics['loss'] is not None:\n",
    "                row[f\"{config_name}_loss\"] = metrics['loss']\n",
    "                \n",
    "                # Calculate difference from baseline\n",
    "                if config_name != 'baseline' and 'baseline' in metrics_data:\n",
    "                    baseline_metrics = get_metrics_at_step(metrics_data['baseline'], step)\n",
    "                    if baseline_metrics['loss'] is not None:\n",
    "                        diff = (metrics['loss'] - baseline_metrics['loss']) / baseline_metrics['loss'] * 100\n",
    "                        row[f\"{config_name}_diff%\"] = diff\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create and display comparison table\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARITY TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLoss Comparison at Key Steps:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Format the table for display\n",
    "display_data = []\n",
    "for _, row in comparison_df.iterrows():\n",
    "    display_row = [f\"Step {int(row['Step'])}\"]\n",
    "    \n",
    "    # Baseline\n",
    "    if 'baseline_loss' in row:\n",
    "        display_row.append(f\"{row['baseline_loss']:.6f}\")\n",
    "    else:\n",
    "        display_row.append(\"-\")\n",
    "    \n",
    "    # SANS Normal\n",
    "    if 'sans_normal_loss' in row:\n",
    "        display_row.append(f\"{row['sans_normal_loss']:.6f}\")\n",
    "        if 'sans_normal_diff%' in row:\n",
    "            display_row.append(f\"{row['sans_normal_diff%']:+.1f}%\")\n",
    "        else:\n",
    "            display_row.append(\"-\")\n",
    "    else:\n",
    "        display_row.extend([\"-\", \"-\"])\n",
    "    \n",
    "    # SANS Alpha=0\n",
    "    if 'sans_alpha0_loss' in row:\n",
    "        display_row.append(f\"{row['sans_alpha0_loss']:.6f}\")\n",
    "        if 'sans_alpha0_diff%' in row:\n",
    "            display_row.append(f\"{row['sans_alpha0_diff%']:+.1f}%\")\n",
    "        else:\n",
    "            display_row.append(\"-\")\n",
    "    else:\n",
    "        display_row.extend([\"-\", \"-\"])\n",
    "    \n",
    "    display_data.append(display_row)\n",
    "\n",
    "headers = ['Step', 'Baseline', 'SANS(α=2.0)', 'Diff%', 'SANS(α=0)', 'Diff%']\n",
    "print(tabulate(display_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "# Parity test conclusion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARITY TEST ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'sans_alpha0_diff%' in comparison_df.columns:\n",
    "    alpha0_diffs = comparison_df['sans_alpha0_diff%'].dropna()\n",
    "    if len(alpha0_diffs) > 0:\n",
    "        max_diff = alpha0_diffs.abs().max()\n",
    "        avg_diff = alpha0_diffs.mean()\n",
    "        \n",
    "        print(f\"\\nSANS(α=0) vs Baseline:\")\n",
    "        print(f\"  Average difference: {avg_diff:+.2f}%\")\n",
    "        print(f\"  Maximum difference: {max_diff:.2f}%\")\n",
    "        \n",
    "        if max_diff <= 2.0:\n",
    "            print(\"\\n✅ PARITY TEST PASSED\")\n",
    "            print(\"  SANS with α=0 matches baseline within 2%\")\n",
    "            print(\"  → The SANS codepath is equivalent to baseline\")\n",
    "            print(\"  → Issues are likely in the weighting/temperature schedule\")\n",
    "        else:\n",
    "            print(\"\\n⚠ PARITY TEST FAILED\")\n",
    "            print(f\"  SANS with α=0 differs from baseline by {max_diff:.1f}%\")\n",
    "            print(\"  → The SANS codepath has structural differences\")\n",
    "            print(\"  → Check: negative term aggregation, loss mixing, or shape issues\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(Path(BASE_CONFIG['results_dir']) / 'parity_comparison.csv', index=False)\n",
    "print(f\"\\n✓ Parity comparison saved to {BASE_CONFIG['results_dir']}/parity_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('SANS Parity Test: Training Comparison', fontsize=16, y=1.02)\n",
    "\n",
    "# 1. Training Loss Curves\n",
    "ax = axes[0, 0]\n",
    "for config_name in ['baseline', 'sans_normal', 'sans_alpha0']:\n",
    "    if config_name in metrics_data:\n",
    "        df = metrics_data[config_name]\n",
    "        config = TEST_CONFIGS[config_name]\n",
    "        ax.plot(df['step'], df['loss'], \n",
    "                label=config['description'],\n",
    "                color=config['color'],\n",
    "                alpha=0.8, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Comparison')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add vertical lines at comparison steps\n",
    "for step in comparison_steps:\n",
    "    ax.axvline(x=step, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. Loss Difference from Baseline\n",
    "ax = axes[0, 1]\n",
    "if 'baseline' in metrics_data:\n",
    "    baseline_df = metrics_data['baseline']\n",
    "    \n",
    "    for config_name in ['sans_normal', 'sans_alpha0']:\n",
    "        if config_name in metrics_data:\n",
    "            df = metrics_data[config_name]\n",
    "            config = TEST_CONFIGS[config_name]\n",
    "            \n",
    "            # Interpolate to match baseline steps\n",
    "            interp_loss = np.interp(baseline_df['step'], df['step'], df['loss'])\n",
    "            diff_percent = (interp_loss - baseline_df['loss']) / baseline_df['loss'] * 100\n",
    "            \n",
    "            ax.plot(baseline_df['step'], diff_percent,\n",
    "                    label=config['description'],\n",
    "                    color=config['color'],\n",
    "                    alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax.axhline(y=2, color='red', linestyle='--', alpha=0.3, label='±2% threshold')\n",
    "    ax.axhline(y=-2, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Difference from Baseline (%)')\n",
    "    ax.set_title('Relative Performance vs Baseline')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss Components (if available)\n",
    "ax = axes[1, 0]\n",
    "for config_name in ['sans_normal', 'sans_alpha0']:\n",
    "    if config_name in metrics_data:\n",
    "        df = metrics_data[config_name]\n",
    "        if 'loss_energy' in df.columns:\n",
    "            config = TEST_CONFIGS[config_name]\n",
    "            ax.plot(df['step'], df['loss_energy'],\n",
    "                    label=f\"{config_name} energy\",\n",
    "                    color=config['color'],\n",
    "                    alpha=0.8, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Energy Loss')\n",
    "ax.set_title('Energy Loss Component')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Convergence Analysis\n",
    "ax = axes[1, 1]\n",
    "convergence_data = []\n",
    "\n",
    "for config_name in ['baseline', 'sans_normal', 'sans_alpha0']:\n",
    "    if config_name in metrics_data:\n",
    "        df = metrics_data[config_name]\n",
    "        config = TEST_CONFIGS[config_name]\n",
    "        \n",
    "        # Calculate smoothed derivative (rate of improvement)\n",
    "        window = 100\n",
    "        smoothed = df['loss'].rolling(window=window, min_periods=10).mean()\n",
    "        rate = -smoothed.diff() / window  # Negative for improvement\n",
    "        \n",
    "        ax.plot(df['step'][window:], rate[window:],\n",
    "                label=config['description'],\n",
    "                color=config['color'],\n",
    "                alpha=0.8, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Improvement Rate')\n",
    "ax.set_title('Training Convergence Rate')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(BASE_CONFIG['results_dir']) / 'parity_test_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Plots saved to {BASE_CONFIG['results_dir']}/parity_test_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARITY TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Dataset: {BASE_CONFIG['dataset']} (rank={BASE_CONFIG['rank']})\")\n",
    "print(f\"⚙️  Training Steps: {BASE_CONFIG['num_steps']}\")\n",
    "print(f\"🎯 Batch Size: {BASE_CONFIG['batch_size']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Analyze parity test results\n",
    "if 'sans_alpha0_diff%' in comparison_df.columns:\n",
    "    alpha0_diffs = comparison_df['sans_alpha0_diff%'].dropna()\n",
    "    if len(alpha0_diffs) > 0:\n",
    "        max_diff = alpha0_diffs.abs().max()\n",
    "        \n",
    "        if max_diff <= 2.0:\n",
    "            print(\"\\n✅ SANS Implementation is Structurally Correct\")\n",
    "            print(\"   • SANS(α=0) matches baseline within acceptable margin\")\n",
    "            print(\"   • The negative sampling and aggregation logic is correct\")\n",
    "            print(\"   • Performance issues are in the weighting mechanism or temperature schedule\")\n",
    "            \n",
    "            print(\"\\n📝 Recommended Actions:\")\n",
    "            print(\"   1. Review temperature schedule (currently reversed)\")\n",
    "            print(\"   2. Tune base temperature (α) value\")\n",
    "            print(\"   3. Verify gradient flow through weighted negatives\")\n",
    "            print(\"   4. Check if loss_opt should be included\")\n",
    "        else:\n",
    "            print(\"\\n⚠ SANS Implementation Has Structural Issues\")\n",
    "            print(f\"   • SANS(α=0) differs from baseline by {max_diff:.1f}%\")\n",
    "            print(\"   • The issue is NOT just in weighting\")\n",
    "            \n",
    "            print(\"\\n🔍 Areas to Investigate:\")\n",
    "            print(\"   1. Negative term aggregation (logsumexp vs mean)\")\n",
    "            print(\"   2. Loss component scaling (check loss_scale)\")\n",
    "            print(\"   3. Shape/dimension mismatches in energy computation\")\n",
    "            print(\"   4. Gradient accumulation differences\")\n",
    "            print(\"   5. Detach operations affecting gradient flow\")\n",
    "\n",
    "# Performance comparison\n",
    "if 'baseline' in metrics_data and 'sans_normal' in metrics_data:\n",
    "    baseline_final = metrics_data['baseline']['loss'].iloc[-1]\n",
    "    sans_final = metrics_data['sans_normal']['loss'].iloc[-1]\n",
    "    improvement = (baseline_final - sans_final) / baseline_final * 100\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"PERFORMANCE COMPARISON\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"\\nFinal Loss @ {BASE_CONFIG['num_steps']} steps:\")\n",
    "    print(f\"  Baseline:         {baseline_final:.6f}\")\n",
    "    print(f\"  SANS (α=2.0):     {sans_final:.6f}\")\n",
    "    if 'sans_alpha0' in metrics_data:\n",
    "        sans_alpha0_final = metrics_data['sans_alpha0']['loss'].iloc[-1]\n",
    "        print(f\"  SANS (α=0):       {sans_alpha0_final:.6f}\")\n",
    "    print(f\"\\n  SANS Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARITY TEST COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all output files\n",
    "print(\"\\n📁 Output Files:\")\n",
    "for file in Path(BASE_CONFIG['results_dir']).rglob('*'):\n",
    "    if file.is_file() and not file.name.startswith('.'):\n",
    "        print(f\"   • {file.relative_to(BASE_CONFIG['results_dir'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}