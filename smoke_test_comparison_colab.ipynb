{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iREM vs SANS-Modified Model Smoke Test (Google Colab)\n",
    "\n",
    "This notebook runs actual training comparing:\n",
    "1. **Baseline iREM** - The original iterative refinement energy model\n",
    "2. **SANS-Modified Model** - With Self-Adversarial Negative Sampling (RotatE-style)\n",
    "\n",
    "**Environment**: Google Colab with T4 GPU\n",
    "\n",
    "**Purpose**: Quick validation of training dynamics and performance differences using actual training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    if 'T4' in gpu_name:\n",
    "        print(\"‚úì T4 GPU detected\")\n",
    "        batch_size = 256\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"‚úì V100 GPU detected\")\n",
    "        batch_size = 512\n",
    "    else:\n",
    "        print(f\"‚úì {gpu_name} detected\")\n",
    "        batch_size = 128\n",
    "else:\n",
    "    print(\"‚ö† No GPU detected - running on CPU (will be slower)\")\n",
    "    batch_size = 32\n",
    "\n",
    "print(f\"\\nUsing batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "\n",
    "# If repo is private or doesn't exist on GitHub, upload the files directly\n",
    "# For now, we'll assume the files are uploaded to Colab\n",
    "import os\n",
    "if not os.path.exists('energy-based-model'):\n",
    "    print(\"Repository not found. Please either:\")\n",
    "    print(\"1. Update the git clone URL with your repository\")\n",
    "    print(\"2. Upload the energy-based-model folder to Colab\")\n",
    "    print(\"\\nCreating directory structure...\")\n",
    "    os.makedirs('energy-based-model', exist_ok=True)\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir('energy-based-model')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install -q accelerate ema-pytorch einops tabulate tqdm matplotlib seaborn pandas\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Training Modules and Configure Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Import training modules from the repository\n",
    "from diffusion_lib.denoising_diffusion_pytorch_1d import GaussianDiffusion1D, Trainer1D\n",
    "from models import EBM, DiffusionWrapper\n",
    "from dataset import Inverse, Addition, LowRankDataset\n",
    "\n",
    "# For baseline iREM\n",
    "from irem_lib.irem import Trainer1D as iREMTrainer1D\n",
    "\n",
    "print(\"‚úì Modules imported successfully\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for smoke test\n",
    "CONFIG = {\n",
    "    # Dataset parameters\n",
    "    'dataset': 'inverse',\n",
    "    'rank': 10,\n",
    "    'ood': False,\n",
    "    \n",
    "    # Model parameters\n",
    "    'model': 'mlp',\n",
    "    'diffusion_steps': 10,\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': batch_size,  # Set based on GPU\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_steps': 5000,  # Reduced for smoke test\n",
    "    'val_every': 250,\n",
    "    'save_every': 1000,\n",
    "    \n",
    "    # SANS parameters\n",
    "    'sans_enabled': True,\n",
    "    'sans_num_negs': 4,\n",
    "    'sans_temp': 1.0,\n",
    "    'sans_temp_schedule': True,\n",
    "    'sans_chunk': 0,\n",
    "    \n",
    "    # Other parameters\n",
    "    'supervise_energy_landscape': True,\n",
    "    'use_innerloop_opt': False,\n",
    "    'baseline': False,\n",
    "    'data_workers': 2,\n",
    "    'ema_decay': 0.995,\n",
    "    'gradient_accumulate_every': 1,\n",
    "    'amp': False,\n",
    "    \n",
    "    # Paths\n",
    "    'results_dir': 'smoke_test_results',\n",
    "    'baseline_dir': 'smoke_test_results/baseline_irem',\n",
    "    'sans_dir': 'smoke_test_results/sans_modified'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['results_dir'], CONFIG['baseline_dir'], CONFIG['sans_dir']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{CONFIG['results_dir']}/config.json\", 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.endswith('_dir'):\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Dataset and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "if CONFIG['dataset'] == 'inverse':\n",
    "    dataset = Inverse(\"train\", CONFIG['rank'], CONFIG['ood'])\n",
    "elif CONFIG['dataset'] == 'addition':\n",
    "    dataset = Addition(\"train\", CONFIG['rank'], CONFIG['ood'])\n",
    "elif CONFIG['dataset'] == 'lowrank':\n",
    "    dataset = LowRankDataset(\"train\", CONFIG['rank'], CONFIG['ood'])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {CONFIG['dataset']}\")\n",
    "\n",
    "validation_dataset = dataset\n",
    "metric = 'mse'\n",
    "\n",
    "print(f\"Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"  Input dimension: {dataset.inp_dim}\")\n",
    "print(f\"  Output dimension: {dataset.out_dim}\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Track Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# We need to extract metrics from the trainers after they run\n# The trainers log to console but don't save metrics.csv files\n# We'll need to modify the training approach or parse logs\n\nprint(\"‚úì Note: Trainers need to be modified to save metrics.csv or we need to parse their logs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Baseline iREM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"BASELINE iREM TRAINING\")\nprint(\"=\"*60)\nprint(\"\\n‚ö†Ô∏è NOTE: The iREM trainer does not save metrics.csv by default.\")\nprint(\"You need to modify irem_lib/irem.py to log metrics or capture console output.\")\nprint(\"For now, training will run but metrics.csv won't be created automatically.\\n\")\n\n# Create model for baseline\nbaseline_model = EBM(\n    inp_dim=dataset.inp_dim,\n    out_dim=dataset.out_dim,\n)\nbaseline_model = DiffusionWrapper(baseline_model)\n\n# Create baseline trainer\nbaseline_trainer = iREMTrainer1D(\n    baseline_model,\n    dataset,\n    train_batch_size=CONFIG['batch_size'],\n    validation_batch_size=min(256, CONFIG['batch_size']),\n    train_lr=CONFIG['learning_rate'],\n    train_num_steps=CONFIG['num_steps'],\n    gradient_accumulate_every=CONFIG['gradient_accumulate_every'],\n    ema_decay=CONFIG['ema_decay'],\n    data_workers=CONFIG['data_workers'],\n    amp=CONFIG['amp'],\n    metric=metric,\n    results_folder=CONFIG['baseline_dir'],\n    cond_mask=False,\n    validation_dataset=validation_dataset,\n    save_and_sample_every=CONFIG['save_every'],\n    evaluate_first=False\n)\n\nprint(f\"Training Baseline iREM for {CONFIG['num_steps']} steps...\")\nprint(f\"Results will be saved to: {CONFIG['baseline_dir']}\")\n\n# Run actual training\ntry:\n    baseline_trainer.train()\n    print(\"\\n‚úì Baseline iREM training complete\")\nexcept Exception as e:\n    print(f\"\\n‚ö† Training error: {str(e)}\")\n    print(\"Continuing with partial results...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run SANS-Modified Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"SANS-MODIFIED TRAINING\")\nprint(\"=\"*60)\nprint(\"\\n‚ö†Ô∏è NOTE: The SANS trainer does not save metrics.csv by default.\")\nprint(\"You need to modify diffusion_lib/denoising_diffusion_pytorch_1d.py to log metrics.\")\nprint(\"For now, training will run but metrics.csv won't be created automatically.\\n\")\n\n# Create model for SANS\nsans_model = EBM(\n    inp_dim=dataset.inp_dim,\n    out_dim=dataset.out_dim,\n)\nsans_model = DiffusionWrapper(sans_model)\n\n# Create diffusion with SANS\nsans_diffusion = GaussianDiffusion1D(\n    sans_model,\n    seq_length=32,\n    objective='pred_noise',\n    timesteps=CONFIG['diffusion_steps'],\n    sampling_timesteps=CONFIG['diffusion_steps'],\n    supervise_energy_landscape=CONFIG['supervise_energy_landscape'],\n    use_innerloop_opt=CONFIG['use_innerloop_opt'],\n    show_inference_tqdm=False,\n    # SANS parameters\n    sans_enabled=CONFIG['sans_enabled'],\n    sans_num_negs=CONFIG['sans_num_negs'],\n    sans_temp=CONFIG['sans_temp'],\n    sans_temp_schedule=CONFIG['sans_temp_schedule'],\n    sans_chunk=CONFIG['sans_chunk'],\n    continuous=True  # For inverse dataset\n)\n\n# Create SANS trainer\nsans_trainer = Trainer1D(\n    sans_diffusion,\n    dataset,\n    train_batch_size=CONFIG['batch_size'],\n    validation_batch_size=min(256, CONFIG['batch_size']),\n    train_lr=CONFIG['learning_rate'],\n    train_num_steps=CONFIG['num_steps'],\n    gradient_accumulate_every=CONFIG['gradient_accumulate_every'],\n    ema_decay=CONFIG['ema_decay'],\n    data_workers=CONFIG['data_workers'],\n    amp=CONFIG['amp'],\n    metric=metric,\n    results_folder=CONFIG['sans_dir'],\n    cond_mask=False,\n    validation_dataset=validation_dataset,\n    save_and_sample_every=CONFIG['save_every'],\n    evaluate_first=False\n)\n\nprint(f\"Training SANS-Modified for {CONFIG['num_steps']} steps...\")\nprint(f\"SANS Configuration:\")\nprint(f\"  - Number of negatives: {CONFIG['sans_num_negs']}\")\nprint(f\"  - Temperature: {CONFIG['sans_temp']}\")\nprint(f\"  - Temperature schedule: {CONFIG['sans_temp_schedule']}\")\nprint(f\"Results will be saved to: {CONFIG['sans_dir']}\")\n\n# Run actual training\ntry:\n    sans_trainer.train()\n    print(\"\\n‚úì SANS-Modified training complete\")\nexcept Exception as e:\n    print(f\"\\n‚ö† Training error: {str(e)}\")\n    print(\"Continuing with partial results...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if metrics files exist, if not provide instructions\nimport os\n\nbaseline_metrics_path = f\"{CONFIG['baseline_dir']}/metrics.csv\"\nsans_metrics_path = f\"{CONFIG['sans_dir']}/metrics.csv\"\n\nif not os.path.exists(baseline_metrics_path) or not os.path.exists(sans_metrics_path):\n    print(\"‚ö†Ô∏è ERROR: Metrics files not found!\")\n    print(\"\\nThe trainers do not automatically save metrics.csv files.\")\n    print(\"\\nTo fix this, you need to:\")\n    print(\"1. Modify irem_lib/irem.py to save training metrics to CSV\")\n    print(\"2. Modify diffusion_lib/denoising_diffusion_pytorch_1d.py to save training metrics to CSV\")\n    print(\"3. OR capture console output during training and parse it\")\n    print(\"\\nThe metrics.csv files should have columns: step, loss, val_loss, lr, time\")\n    \n    # Stop execution here\n    raise FileNotFoundError(\"Metrics files not found. See instructions above.\")\n\n# Load metrics from saved files\nbaseline_df = pd.read_csv(baseline_metrics_path)\nsans_df = pd.read_csv(sans_metrics_path)\n\n# Add model type column\nbaseline_df['model_type'] = 'Baseline iREM'\nsans_df['model_type'] = 'SANS-Modified'\n\n# Combine dataframes\ncombined_df = pd.concat([baseline_df, sans_df], ignore_index=True)\n\n# Calculate improvement metrics\nfinal_baseline_loss = baseline_df['loss'].iloc[-1]\nfinal_sans_loss = sans_df['loss'].iloc[-1]\nimprovement = (final_baseline_loss - final_sans_loss) / final_baseline_loss * 100\n\n# Calculate convergence speed\ndef get_convergence_step(df, threshold=0.1):\n    initial_loss = df['loss'].iloc[0]\n    final_loss = df['loss'].iloc[-1]\n    target_loss = initial_loss - 0.9 * (initial_loss - final_loss)\n    conv_idx = df[df['loss'] <= target_loss].index\n    if len(conv_idx) > 0:\n        return df.loc[conv_idx[0], 'step']\n    return df['step'].iloc[-1]\n\nbaseline_conv = get_convergence_step(baseline_df)\nsans_conv = get_convergence_step(sans_df)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nFinal Loss:\")\nprint(f\"  Baseline iREM: {final_baseline_loss:.6f}\")\nprint(f\"  SANS-Modified: {final_sans_loss:.6f}\")\nprint(f\"  Improvement: {improvement:.1f}%\")\n\nprint(f\"\\nConvergence Speed (90% reduction):\")\nprint(f\"  Baseline iREM: {baseline_conv} steps\")\nprint(f\"  SANS-Modified: {sans_conv} steps\")\nif sans_conv > 0:\n    print(f\"  Speedup: {baseline_conv/sans_conv:.2f}x\")\n\nprint(f\"\\nTraining Time:\")\nprint(f\"  Baseline iREM: {baseline_df['time'].iloc[-1]:.1f} seconds\")\nprint(f\"  SANS-Modified: {sans_df['time'].iloc[-1]:.1f} seconds\")\n\n# Save combined results\ncombined_df.to_csv(f\"{CONFIG['results_dir']}/combined_metrics.csv\", index=False)\nprint(f\"\\n‚úì Combined metrics saved to {CONFIG['results_dir']}/combined_metrics.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('iREM vs SANS-Modified Training Comparison', fontsize=16, y=1.02)\n",
    "\n",
    "# 1. Training Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(baseline_df['step'], baseline_df['loss'], label='Baseline iREM', \n",
    "        color='blue', alpha=0.8, linewidth=2)\n",
    "ax.plot(sans_df['step'], sans_df['loss'], label='SANS-Modified', \n",
    "        color='red', alpha=0.8, linewidth=2)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# 2. Validation Loss\n",
    "ax = axes[0, 1]\n",
    "baseline_val = baseline_df.dropna(subset=['val_loss'])\n",
    "sans_val = sans_df.dropna(subset=['val_loss'])\n",
    "if len(baseline_val) > 0:\n",
    "    ax.plot(baseline_val['step'], baseline_val['val_loss'], 'o-', \n",
    "            label='Baseline iREM', color='blue', alpha=0.8, markersize=6)\n",
    "if len(sans_val) > 0:\n",
    "    ax.plot(sans_val['step'], sans_val['val_loss'], 's-', \n",
    "            label='SANS-Modified', color='red', alpha=0.8, markersize=6)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Validation Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss Reduction Rate\n",
    "ax = axes[1, 0]\n",
    "window = min(50, len(baseline_df) // 10)\n",
    "baseline_smooth = baseline_df['loss'].rolling(window=window, min_periods=1).mean()\n",
    "sans_smooth = sans_df['loss'].rolling(window=window, min_periods=1).mean()\n",
    "ax.plot(baseline_df['step'], baseline_smooth, label='Baseline iREM', \n",
    "        color='blue', alpha=0.8, linewidth=2)\n",
    "ax.plot(sans_df['step'], sans_smooth, label='SANS-Modified', \n",
    "        color='red', alpha=0.8, linewidth=2)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Smoothed Loss')\n",
    "ax.set_title(f'Smoothed Loss (window={window})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Relative Improvement\n",
    "ax = axes[1, 1]\n",
    "if len(baseline_smooth) == len(sans_smooth):\n",
    "    improvement_curve = (baseline_smooth - sans_smooth) / baseline_smooth * 100\n",
    "    ax.plot(baseline_df['step'], improvement_curve, color='green', linewidth=2)\n",
    "    ax.fill_between(baseline_df['step'], 0, improvement_curve, \n",
    "                     where=(improvement_curve > 0), color='green', alpha=0.3, \n",
    "                     label='SANS Better')\n",
    "    ax.fill_between(baseline_df['step'], 0, improvement_curve, \n",
    "                     where=(improvement_curve <= 0), color='red', alpha=0.3, \n",
    "                     label='Baseline Better')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Improvement (%)')\n",
    "    ax.set_title('SANS Relative Improvement')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['results_dir']}/comparison_plots.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Plots saved to {CONFIG['results_dir']}/comparison_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Initial Loss',\n",
    "        'Final Loss',\n",
    "        'Loss Reduction (%)',\n",
    "        'Best Loss',\n",
    "        'Convergence Step (90%)',\n",
    "        'Training Time (s)',\n",
    "        'Steps per Second'\n",
    "    ],\n",
    "    'Baseline iREM': [],\n",
    "    'SANS-Modified': [],\n",
    "    'Improvement': []\n",
    "}\n",
    "\n",
    "# Calculate metrics for both models\n",
    "for df, col_name in [(baseline_df, 'Baseline iREM'), (sans_df, 'SANS-Modified')]:\n",
    "    initial_loss = df['loss'].iloc[0]\n",
    "    final_loss = df['loss'].iloc[-1]\n",
    "    best_loss = df['loss'].min()\n",
    "    loss_reduction = (initial_loss - final_loss) / initial_loss * 100\n",
    "    conv_step = get_convergence_step(df)\n",
    "    train_time = df['time'].iloc[-1]\n",
    "    steps_per_sec = len(df) / train_time if train_time > 0 else 0\n",
    "    \n",
    "    summary_data[col_name] = [\n",
    "        f\"{initial_loss:.6f}\",\n",
    "        f\"{final_loss:.6f}\",\n",
    "        f\"{loss_reduction:.1f}\",\n",
    "        f\"{best_loss:.6f}\",\n",
    "        f\"{conv_step}\",\n",
    "        f\"{train_time:.1f}\",\n",
    "        f\"{steps_per_sec:.1f}\"\n",
    "    ]\n",
    "\n",
    "# Calculate improvements\n",
    "for i, metric in enumerate(summary_data['Metric']):\n",
    "    try:\n",
    "        baseline_val = float(summary_data['Baseline iREM'][i].replace(',', ''))\n",
    "        sans_val = float(summary_data['SANS-Modified'][i].replace(',', ''))\n",
    "        \n",
    "        if metric in ['Final Loss', 'Best Loss', 'Convergence Step (90%)', 'Training Time (s)']:\n",
    "            # Lower is better\n",
    "            imp = (baseline_val - sans_val) / baseline_val * 100 if baseline_val != 0 else 0\n",
    "        else:\n",
    "            # Higher is better\n",
    "            imp = (sans_val - baseline_val) / baseline_val * 100 if baseline_val != 0 else 0\n",
    "        \n",
    "        summary_data['Improvement'].append(f\"{imp:+.1f}%\")\n",
    "    except:\n",
    "        summary_data['Improvement'].append(\"-\")\n",
    "\n",
    "# Display table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(f\"{CONFIG['results_dir']}/performance_summary.csv\", index=False)\n",
    "print(f\"\\n‚úì Performance summary saved to {CONFIG['results_dir']}/performance_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SMOKE TEST CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset: {CONFIG['dataset']} (rank={CONFIG['rank']})\")\n",
    "print(f\"üîß Model: {CONFIG['model']}\")\n",
    "print(f\"‚öôÔ∏è  Training Steps: {CONFIG['num_steps']}\")\n",
    "print(f\"üéØ Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"üîÑ Diffusion Steps: {CONFIG['diffusion_steps']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"\\n‚úÖ SANS Advantages:\")\n",
    "    print(f\"   ‚Ä¢ {improvement:.1f}% better final loss\")\n",
    "    if sans_conv < baseline_conv:\n",
    "        print(f\"   ‚Ä¢ {baseline_conv/sans_conv:.2f}x faster convergence\")\n",
    "    print(f\"   ‚Ä¢ Self-adversarial sampling improves gradient quality\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Mixed Results:\")\n",
    "    print(f\"   ‚Ä¢ Baseline performed better in this short test\")\n",
    "    print(f\"   ‚Ä¢ SANS may need more steps to show benefits\")\n",
    "\n",
    "print(f\"\\nüìù SANS Configuration Used:\")\n",
    "print(f\"   ‚Ä¢ Number of negatives (M): {CONFIG['sans_num_negs']}\")\n",
    "print(f\"   ‚Ä¢ Temperature (Œ±): {CONFIG['sans_temp']}\")\n",
    "print(f\"   ‚Ä¢ Temperature schedule: {'Enabled' if CONFIG['sans_temp_schedule'] else 'Disabled'}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"-\"*40)\n",
    "print(\"\\n1. For full evaluation:\")\n",
    "print(\"   ‚Ä¢ Run longer training (50k-100k steps)\")\n",
    "print(\"   ‚Ä¢ Test on harder datasets (sudoku, connectivity)\")\n",
    "print(\"   ‚Ä¢ Try different SANS hyperparameters\")\n",
    "\n",
    "print(\"\\n2. For optimization:\")\n",
    "print(\"   ‚Ä¢ Increase sans_num_negs for harder problems\")\n",
    "print(\"   ‚Ä¢ Adjust temperature based on dataset difficulty\")\n",
    "print(\"   ‚Ä¢ Enable AMP for faster training on compatible GPUs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SMOKE TEST COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all output files\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "for file in Path(CONFIG['results_dir']).rglob('*'):\n",
    "    if file.is_file():\n",
    "        print(f\"   ‚Ä¢ {file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}