{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzh7-Ram-j9r"
   },
   "source": [
    "# Multi-Curriculum Smoke Test: Comprehensive Comparison\n",
    "\n",
    "Enhanced smoke test comparing baseline training against multiple curriculum configurations:\n",
    "- **Baseline**: No curriculum learning\n",
    "- **Default Curriculum**: Balanced adversarial introduction\n",
    "- **Aggressive Curriculum**: Rapid adversarial escalation\n",
    "- **Conservative Curriculum**: Gradual adversarial introduction\n",
    "\n",
    "This notebook provides comprehensive analysis, visualization, and ranking of all curriculum approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttlLgjdR-j9t",
    "outputId": "d4d35e3f-d98e-46d6-e01a-2a91434ecad3"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "%cd energy-based-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUU1Z1rj-j9u"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0MwXmDg-j9v",
    "outputId": "0f2b9706-161e-4bbc-bb09-6106e9b278cc"
   },
   "outputs": [],
   "source": [
    "# Common training parameters\n",
    "COMMON_ARGS = {\n",
    "    'model': 'mlp',\n",
    "    'batch_size': 32,\n",
    "    'diffusion_steps': 10,\n",
    "    'supervise_energy_landscape': 'True',\n",
    "    'train_num_steps': 200,  # Increased for better curriculum comparison\n",
    "    'save_csv_logs': True,\n",
    "    'csv_log_interval': 100\n",
    "}\n",
    "\n",
    "# Tasks to test\n",
    "TASKS = ['inverse', 'addition', 'lowrank']\n",
    "\n",
    "# Curriculum configurations to test\n",
    "CURRICULUM_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'No curriculum learning',\n",
    "        'color': '#1f77b4',\n",
    "        'args': ['--disable-curriculum', 'True']\n",
    "    },\n",
    "    'default': {\n",
    "        'name': 'Default Curriculum',\n",
    "        'description': 'Balanced adversarial introduction',\n",
    "        'color': '#ff7f0e',\n",
    "        'args': ['--curriculum-config', 'default']\n",
    "    },\n",
    "    'aggressive': {\n",
    "        'name': 'Aggressive Curriculum',\n",
    "        'description': 'Rapid adversarial escalation',\n",
    "        'color': '#d62728',\n",
    "        'args': ['--curriculum-config', 'aggressive']\n",
    "    },\n",
    "    'conservative': {\n",
    "        'name': 'Conservative Curriculum',\n",
    "        'description': 'Gradual adversarial introduction',\n",
    "        'color': '#2ca02c',\n",
    "        'args': ['--curriculum-config', 'conservative']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(CURRICULUM_CONFIGS)} curriculum configurations on {len(TASKS)} tasks:\")\n",
    "print(f\"Tasks: {', '.join(TASKS)}\")\n",
    "for key, config in CURRICULUM_CONFIGS.items():\n",
    "    print(f\"  • {config['name']}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBg0XbEp-j9w"
   },
   "outputs": [],
   "source": [
    "def build_training_command(curriculum_key: str, task: str = 'inverse') -> str:\n",
    "    \"\"\"Build training command for a specific curriculum configuration and task.\"\"\"\n",
    "    config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "    base_cmd = f\"\"\"python train.py \\\n",
    "        --dataset {task} \\\n",
    "        --model {COMMON_ARGS['model']} \\\n",
    "        --batch_size {COMMON_ARGS['batch_size']} \\\n",
    "        --diffusion_steps {COMMON_ARGS['diffusion_steps']} \\\n",
    "        --supervise-energy-landscape {COMMON_ARGS['supervise_energy_landscape']} \\\n",
    "        --train-num-steps {COMMON_ARGS['train_num_steps']} \\\n",
    "        --save-csv-logs \\\n",
    "        --csv-log-interval {COMMON_ARGS['csv_log_interval']} \\\n",
    "        --csv-log-dir ./csv_logs_{task}_{curriculum_key}\"\"\"\n",
    "\n",
    "    # Add curriculum-specific arguments\n",
    "    if config['args']:\n",
    "        base_cmd += ' \\\n",
    "        ' + ' \\\n",
    "        '.join(config['args'])\n",
    "\n",
    "    return base_cmd\n",
    "\n",
    "def load_csv_data(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV data with error handling.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            return pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"Warning: {csv_path} not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_get_final_value(df: pd.DataFrame, column: str, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the final value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].iloc[-1])\n",
    "\n",
    "def safe_get_best_value(df: pd.DataFrame, column: str, minimize: bool = True, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the best (min/max) value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].min() if minimize else df[column].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHwwUrR-j9w"
   },
   "source": [
    "## Multi-Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all tasks with all curriculum configurations\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "# Train each task\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# TASK: {task.upper()}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    task_results = {}\n",
    "    \n",
    "    # For each curriculum configuration\n",
    "    for curriculum_key, config in CURRICULUM_CONFIGS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting {config['name']} training for {task} task...\")\n",
    "        print(f\"Description: {config['description']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Build training command\n",
    "        cmd = build_training_command(curriculum_key, task)\n",
    "        print(f\"\\nCommand: {cmd}\")\n",
    "        print(\"\\nTraining output:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Execute training with real-time output\n",
    "        try:\n",
    "            # Use subprocess to capture and display output in real-time\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "\n",
    "            # Display output line by line as it comes\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                if line:\n",
    "                    print(line.rstrip())\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Wait for process to complete\n",
    "            result = process.wait()\n",
    "            task_results[curriculum_key] = result\n",
    "\n",
    "            if result == 0:\n",
    "                print(f\"✓ {config['name']} training for {task} completed successfully\")\n",
    "            else:\n",
    "                print(f\"✗ {config['name']} training for {task} failed with exit code {result}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during {config['name']} training for {task}: {e}\")\n",
    "            task_results[curriculum_key] = -1\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    training_results[task] = task_results\n",
    "    \n",
    "    # Print task summary\n",
    "    successful = sum(1 for result in task_results.values() if result == 0)\n",
    "    print(f\"\\n{task.upper()} Task Summary: {successful}/{len(task_results)} configurations completed successfully\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All training completed!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print overall summary\n",
    "print(\"\\nOVERALL TRAINING SUMMARY:\")\n",
    "for task, task_results in training_results.items():\n",
    "    successful = sum(1 for result in task_results.values() if result == 0)\n",
    "    print(f\"\\n{task.upper()} Task: {successful}/{len(task_results)} configurations successful\")\n",
    "    for curriculum_key, result in task_results.items():\n",
    "        status = \"✓ Success\" if result == 0 else \"✗ Failed\"\n",
    "        print(f\"  {CURRICULUM_CONFIGS[curriculum_key]['name']}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNqMLoV_-j9x"
   },
   "outputs": [],
   "source": [
    "def load_all_curriculum_results() -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Load all curriculum training results from CSV files for all tasks.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for task in TASKS:\n",
    "        print(f\"\\nLoading results for {task} task...\")\n",
    "        task_results = {}\n",
    "        \n",
    "        for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "            print(f\"  Loading {curriculum_key} data...\")\n",
    "\n",
    "            csv_dir = Path(f\"./csv_logs_{task}_{curriculum_key}\")\n",
    "\n",
    "            # Load different types of training data - look for files with timestamps\n",
    "            import glob\n",
    "\n",
    "            curriculum_data = {}\n",
    "\n",
    "            # Find the most recent file for each metric type\n",
    "            patterns = {\n",
    "                'training': 'training_metrics_*.csv',\n",
    "                'validation': 'validation_metrics_*.csv',\n",
    "                'energy': 'energy_metrics_*.csv',\n",
    "                'curriculum': 'curriculum_metrics_*.csv',\n",
    "                'robustness': 'robustness_metrics_*.csv'\n",
    "            }\n",
    "\n",
    "            for key, pattern in patterns.items():\n",
    "                files = glob.glob(str(csv_dir / pattern))\n",
    "                if files:\n",
    "                    # Get most recent file\n",
    "                    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n",
    "                    curriculum_data[key] = load_csv_data(Path(latest_file))\n",
    "                else:\n",
    "                    curriculum_data[key] = None\n",
    "\n",
    "            # Count available data types\n",
    "            available = sum(1 for df in curriculum_data.values() if df is not None)\n",
    "            print(f\"    Found {available}/5 data files for {curriculum_key}\")\n",
    "\n",
    "            task_results[curriculum_key] = curriculum_data\n",
    "        \n",
    "        results[task] = task_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_curriculum_data(all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]]) -> pd.DataFrame:\n",
    "    \"\"\"Extract and standardize key metrics from all curriculum results across all tasks.\"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for task, task_results in all_results.items():\n",
    "        for curriculum_key, data in task_results.items():\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Extract key metrics\n",
    "            training_df = data.get('training')\n",
    "            validation_df = data.get('validation')\n",
    "            energy_df = data.get('energy')\n",
    "\n",
    "            # Extract task-specific metrics from validation data\n",
    "            val_accuracy = 0.0  # For inverse task\n",
    "            val_identity_error = float('inf')  # For inverse task\n",
    "            val_mse = float('inf')  # General metric\n",
    "            val_mae = float('inf')  # For addition/lowrank tasks\n",
    "\n",
    "            if validation_df is not None:\n",
    "                # Look for accuracy metric (for inverse task)\n",
    "                accuracy_rows = validation_df[validation_df['metric_name'] == 'accuracy']\n",
    "                if not accuracy_rows.empty:\n",
    "                    val_accuracy = accuracy_rows['metric_value'].iloc[-1]  # Get last value (fraction)\n",
    "\n",
    "                # Look for identity_error metric (for inverse task)\n",
    "                identity_rows = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "                if not identity_rows.empty:\n",
    "                    val_identity_error = identity_rows['metric_value'].iloc[-1]\n",
    "\n",
    "                # Look for MSE metric\n",
    "                mse_rows = validation_df[validation_df['metric_name'] == 'mse']\n",
    "                if not mse_rows.empty:\n",
    "                    val_mse = mse_rows['metric_value'].iloc[-1]\n",
    "                \n",
    "                # Look for MAE metric (for addition/lowrank)\n",
    "                mae_rows = validation_df[validation_df['metric_name'] == 'mae']\n",
    "                if not mae_rows.empty:\n",
    "                    val_mae = mae_rows['metric_value'].iloc[-1]\n",
    "\n",
    "            metrics = {\n",
    "                'task': task,\n",
    "                'curriculum': curriculum_key,\n",
    "                'name': config['name'],\n",
    "                'color': config['color'],\n",
    "\n",
    "                # Training metrics\n",
    "                'final_total_loss': safe_get_final_value(training_df, 'total_loss'),\n",
    "                'final_energy_loss': safe_get_final_value(training_df, 'loss_energy'),\n",
    "                'final_denoise_loss': safe_get_final_value(training_df, 'loss_denoise'),\n",
    "                'best_total_loss': safe_get_best_value(training_df, 'total_loss', minimize=True),\n",
    "                'avg_training_time': training_df['nn_time'].mean() if training_df is not None and 'nn_time' in training_df.columns else 0.0,\n",
    "\n",
    "                # Validation metrics\n",
    "                'final_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "                'best_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "                'final_identity_error': val_identity_error,\n",
    "                'final_val_mse': val_mse,\n",
    "                'final_val_mae': val_mae,\n",
    "\n",
    "                # Energy metrics\n",
    "                'final_energy_margin': safe_get_final_value(energy_df, 'energy_margin'),\n",
    "                'max_curriculum_weight': safe_get_best_value(energy_df, 'curriculum_weight', minimize=False) if energy_df is not None and 'curriculum_weight' in energy_df.columns else 0.0,\n",
    "            }\n",
    "\n",
    "            processed_data.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Load all results\n",
    "print(\"Loading all curriculum results for all tasks...\")\n",
    "all_results = load_all_curriculum_results()\n",
    "\n",
    "# Process data\n",
    "print(\"\\nProcessing curriculum data...\")\n",
    "summary_df = process_curriculum_data(all_results)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(summary_df)} configurations across {len(TASKS)} tasks\")\n",
    "\n",
    "# Display summary by task\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    print(f\"\\n{task.upper()} Task Summary:\")\n",
    "    display_cols = ['name', 'final_total_loss', 'final_val_mse']\n",
    "    if task == 'inverse':\n",
    "        display_cols.append('final_identity_error')\n",
    "    elif task in ['addition', 'lowrank']:\n",
    "        display_cols.append('final_val_mae')\n",
    "    \n",
    "    display(task_data[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRED Paper-Style Results Reporting\n",
    "\n",
    "This section generates results tables in the format used by the IRED paper, focusing on:\n",
    "- **Mean Squared Error (MSE)** as the primary metric for all matrix tasks\n",
    "- **Identity Error** for matrix inverse task (||Pred @ Input - I||²)\n",
    "- **Task-specific metrics** for comprehensive evaluation\n",
    "- **Consolidated performance tables** across all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IRED-Style Results Table Generation\n",
    "\n",
    "def generate_ired_style_table(summary_df: pd.DataFrame, task: str):\n",
    "    \"\"\"Generate IRED paper-style results table for a specific task.\"\"\"\n",
    "    import pandas as pd\n",
    "    from tabulate import tabulate\n",
    "    \n",
    "    task_data = summary_df[summary_df['task'] == task].copy()\n",
    "    \n",
    "    if len(task_data) == 0:\n",
    "        print(f\"No data available for {task} task\")\n",
    "        return\n",
    "    \n",
    "    # Sort by final MSE to rank methods\n",
    "    task_data = task_data.sort_values('final_val_mse')\n",
    "    \n",
    "    # Create IRED-style table\n",
    "    table_data = []\n",
    "    for _, row in task_data.iterrows():\n",
    "        method_name = row['name']\n",
    "        \n",
    "        # Main metric: MSE (as reported in IRED paper)\n",
    "        mse = row['final_val_mse'] if row['final_val_mse'] != float('inf') else 'N/A'\n",
    "        \n",
    "        # Task-specific additional metrics\n",
    "        if task == 'inverse':\n",
    "            identity_error = row['final_identity_error'] if row['final_identity_error'] != float('inf') else 'N/A'\n",
    "            if isinstance(mse, float) and isinstance(identity_error, float):\n",
    "                table_data.append([method_name, f\"{mse:.4f}\", f\"{identity_error:.4f}\"])\n",
    "            else:\n",
    "                table_data.append([method_name, str(mse), str(identity_error)])\n",
    "        else:\n",
    "            if isinstance(mse, float):\n",
    "                table_data.append([method_name, f\"{mse:.4f}\"])\n",
    "            else:\n",
    "                table_data.append([method_name, str(mse)])\n",
    "    \n",
    "    # Create table headers based on task\n",
    "    if task == 'inverse':\n",
    "        headers = ['Method', 'MSE', 'Identity Error']\n",
    "    else:\n",
    "        headers = ['Method', 'MSE']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"IRED-Style Results Table: {task.upper()} Task\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "    \n",
    "    # Add interpretation\n",
    "    if len(task_data) > 0:\n",
    "        best_method = task_data.iloc[0]\n",
    "        print(f\"\\n✓ Best performing method: {best_method['name']}\")\n",
    "        if isinstance(best_method['final_val_mse'], float):\n",
    "            print(f\"  MSE: {best_method['final_val_mse']:.4f}\")\n",
    "        if task == 'inverse' and isinstance(best_method['final_identity_error'], float):\n",
    "            print(f\"  Identity Error: {best_method['final_identity_error']:.4f}\")\n",
    "\n",
    "def generate_consolidated_ired_table(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate consolidated IRED-style table for all matrix tasks.\"\"\"\n",
    "    from tabulate import tabulate\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IRED-STYLE CONSOLIDATED RESULTS TABLE\")\n",
    "    print(\"Matrix Operations Performance (MSE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Prepare data for each task\n",
    "    methods = summary_df['name'].unique()\n",
    "    \n",
    "    # Create consolidated table\n",
    "    table_data = []\n",
    "    for method in methods:\n",
    "        method_data = summary_df[summary_df['name'] == method]\n",
    "        \n",
    "        row = [method]\n",
    "        \n",
    "        # Add MSE for each task\n",
    "        for task in ['addition', 'lowrank', 'inverse']:\n",
    "            task_row = method_data[method_data['task'] == task]\n",
    "            if len(task_row) > 0:\n",
    "                mse = task_row['final_val_mse'].iloc[0]\n",
    "                if mse != float('inf'):\n",
    "                    row.append(f\"{mse:.4f}\")\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "            else:\n",
    "                row.append(\"-\")\n",
    "        \n",
    "        # Add average MSE\n",
    "        mse_values = []\n",
    "        for task in ['addition', 'lowrank', 'inverse']:\n",
    "            task_row = method_data[method_data['task'] == task]\n",
    "            if len(task_row) > 0:\n",
    "                mse = task_row['final_val_mse'].iloc[0]\n",
    "                if mse != float('inf'):\n",
    "                    mse_values.append(mse)\n",
    "        \n",
    "        if mse_values:\n",
    "            avg_mse = np.mean(mse_values)\n",
    "            row.append(f\"{avg_mse:.4f}\")\n",
    "        else:\n",
    "            row.append(\"N/A\")\n",
    "        \n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Sort by average MSE\n",
    "    table_data.sort(key=lambda x: float(x[-1]) if x[-1] not in [\"N/A\", \"-\"] else float('inf'))\n",
    "    \n",
    "    headers = ['Method', 'Addition', 'Matrix Completion', 'Matrix Inverse', 'Average']\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "    \n",
    "    # Best method summary\n",
    "    if table_data:\n",
    "        print(f\"\\n✓ Best overall method: {table_data[0][0]}\")\n",
    "        print(f\"  Average MSE: {table_data[0][-1]}\")\n",
    "\n",
    "def generate_task_specific_metrics_table(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate detailed metrics table for each task.\"\"\"\n",
    "    from tabulate import tabulate\n",
    "    \n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task].copy()\n",
    "        \n",
    "        if len(task_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Detailed Metrics Table: {task.upper()} Task\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Sort by MSE\n",
    "        task_data = task_data.sort_values('final_val_mse')\n",
    "        \n",
    "        # Prepare table data\n",
    "        table_data = []\n",
    "        for _, row in task_data.iterrows():\n",
    "            row_data = [row['name']]\n",
    "            \n",
    "            # Add MSE\n",
    "            mse = row['final_val_mse']\n",
    "            row_data.append(f\"{mse:.4f}\" if mse != float('inf') else \"N/A\")\n",
    "            \n",
    "            # Add task-specific metrics\n",
    "            if task == 'inverse':\n",
    "                identity_error = row['final_identity_error']\n",
    "                accuracy = row['final_val_accuracy']\n",
    "                row_data.append(f\"{identity_error:.4f}\" if identity_error != float('inf') else \"N/A\")\n",
    "                row_data.append(f\"{accuracy:.1f}%\" if accuracy > 0 else \"N/A\")\n",
    "            \n",
    "            # Add training loss\n",
    "            row_data.append(f\"{row['final_total_loss']:.4f}\")\n",
    "            \n",
    "            # Add best loss achieved\n",
    "            row_data.append(f\"{row['best_total_loss']:.4f}\")\n",
    "            \n",
    "            table_data.append(row_data)\n",
    "        \n",
    "        # Define headers based on task\n",
    "        if task == 'inverse':\n",
    "            headers = ['Method', 'MSE', 'Identity Error', 'Accuracy', 'Final Loss', 'Best Loss']\n",
    "        else:\n",
    "            headers = ['Method', 'MSE', 'Final Loss', 'Best Loss']\n",
    "        \n",
    "        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "        \n",
    "        # Summary statistics\n",
    "        valid_mse = task_data[task_data['final_val_mse'] != float('inf')]['final_val_mse']\n",
    "        if len(valid_mse) > 0:\n",
    "            print(f\"\\nStatistics:\")\n",
    "            print(f\"  MSE Range: {valid_mse.min():.4f} - {valid_mse.max():.4f}\")\n",
    "            print(f\"  MSE Mean: {valid_mse.mean():.4f}\")\n",
    "            print(f\"  MSE Std: {valid_mse.std():.4f}\")\n",
    "\n",
    "# Generate all IRED-style tables\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IRED PAPER-STYLE RESULTS REPORTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Individual task tables\n",
    "for task in TASKS:\n",
    "    generate_ired_style_table(summary_df, task)\n",
    "\n",
    "# Consolidated table\n",
    "generate_consolidated_ired_table(summary_df)\n",
    "\n",
    "# Detailed metrics\n",
    "generate_task_specific_metrics_table(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_task_curriculum_comparison(all_results: Dict, summary_df: pd.DataFrame, task: str):\n",
    "    \"\"\"Create comprehensive side-by-side curriculum comparison visualizations for a specific task.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Multi-Curriculum Training Comparison - {task.upper()} Task', fontsize=16, fontweight='bold')\n",
    "\n",
    "    task_results = all_results.get(task, {})\n",
    "    task_summary = summary_df[summary_df['task'] == task]\n",
    "    \n",
    "    # 1. Total Loss Comparison\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in task_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['total_loss'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Total Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Energy Loss Comparison\n",
    "    ax = axes[0, 1]\n",
    "    for curriculum_key, data in task_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'loss_energy' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['loss_energy'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Energy Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Energy Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Prepare colors for bar charts\n",
    "    colors = task_summary['color'].tolist()\n",
    "    \n",
    "    # 3. Task-specific metric comparison\n",
    "    ax = axes[1, 0]\n",
    "    curricula = task_summary['name'].tolist()\n",
    "    \n",
    "    if task == 'inverse':\n",
    "        # Identity Error for inverse task\n",
    "        identity_errors = task_summary['final_identity_error'].tolist()\n",
    "        valid_errors = [(c, e, col) for c, e, col in zip(curricula, identity_errors, colors)\n",
    "                        if e != float('inf')]\n",
    "        \n",
    "        if valid_errors:\n",
    "            names, errors, cols = zip(*valid_errors)\n",
    "            bars = ax.bar(names, errors, color=cols, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title('Identity Error (Lower = Better)', fontweight='bold')\n",
    "            ax.set_ylabel('||Pred @ Input - I||²')\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, error in zip(bars, errors):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                        f'{error:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        # MAE for addition/lowrank tasks\n",
    "        mae_values = task_summary['final_val_mae'].tolist()\n",
    "        valid_mae = [(c, m, col) for c, m, col in zip(curricula, mae_values, colors)\n",
    "                     if m != float('inf')]\n",
    "        \n",
    "        if valid_mae:\n",
    "            names, maes, cols = zip(*valid_mae)\n",
    "            bars = ax.bar(names, maes, color=cols, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title('Mean Absolute Error (Lower = Better)', fontweight='bold')\n",
    "            ax.set_ylabel('MAE')\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, mae in zip(bars, maes):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                        f'{mae:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. MSE Comparison (common metric)\n",
    "    ax = axes[1, 1]\n",
    "    mse_values = task_summary['final_val_mse'].tolist()\n",
    "\n",
    "    # Filter out inf values\n",
    "    valid_mse = [(c, m, col) for c, m, col in zip(curricula, mse_values, colors)\n",
    "                 if m != float('inf')]\n",
    "\n",
    "    if valid_mse:\n",
    "        names, mses, cols = zip(*valid_mse)\n",
    "        bars = ax.bar(names, mses, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Mean Squared Error', fontweight='bold')\n",
    "        ax.set_ylabel('MSE')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, mse in zip(bars, mses):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{mse:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('MSE (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_cross_task_comparison(summary_df: pd.DataFrame):\n",
    "    \"\"\"Compare curriculum performance across different tasks.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Cross-Task Curriculum Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    tasks = summary_df['task'].unique()\n",
    "    \n",
    "    # Plot for each task\n",
    "    for idx, task in enumerate(tasks):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        task_data = summary_df[summary_df['task'] == task]\n",
    "        \n",
    "        # Create grouped bar chart for final losses\n",
    "        x = np.arange(len(task_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, task_data['final_total_loss'], width, \n",
    "                      label='Total Loss', alpha=0.7)\n",
    "        bars2 = ax.bar(x + width/2, task_data['final_val_mse'], width,\n",
    "                      label='MSE', alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'{task.upper()} Task Performance', fontweight='bold')\n",
    "        ax.set_ylabel('Loss/Error')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(task_data['name'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots if any\n",
    "    for idx in range(len(tasks), 6):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        fig.delaxes(axes[row, col])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_task_performance_heatmap(summary_df: pd.DataFrame):\n",
    "    \"\"\"Create a heatmap showing curriculum performance across tasks.\"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Pivot data for heatmap\n",
    "    pivot_loss = summary_df.pivot(index='name', columns='task', values='final_total_loss')\n",
    "    pivot_mse = summary_df.pivot(index='name', columns='task', values='final_val_mse')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Total Loss heatmap\n",
    "    sns.heatmap(pivot_loss, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax1, cbar_kws={'label': 'Total Loss'})\n",
    "    ax1.set_title('Total Loss by Curriculum and Task', fontweight='bold')\n",
    "    ax1.set_xlabel('Task')\n",
    "    ax1.set_ylabel('Curriculum')\n",
    "    \n",
    "    # MSE heatmap\n",
    "    # Replace inf values with NaN for better visualization\n",
    "    pivot_mse_clean = pivot_mse.replace([float('inf')], np.nan)\n",
    "    sns.heatmap(pivot_mse_clean, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax2, cbar_kws={'label': 'MSE'})\n",
    "    ax2.set_title('MSE by Curriculum and Task', fontweight='bold')\n",
    "    ax2.set_xlabel('Task')\n",
    "    ax2.set_ylabel('Curriculum')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations for each task\n",
    "print(\"Generating task-specific curriculum comparison visualizations...\")\n",
    "for task in TASKS:\n",
    "    print(f\"\\nVisualizing {task.upper()} task...\")\n",
    "    visualize_task_curriculum_comparison(all_results, summary_df, task)\n",
    "\n",
    "print(\"\\nGenerating cross-task comparison...\")\n",
    "visualize_cross_task_comparison(summary_df)\n",
    "\n",
    "print(\"\\nGenerating performance heatmap...\")\n",
    "create_task_performance_heatmap(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_ranking(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate comprehensive ranking for each task and overall.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-TASK CURRICULUM RANKING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Rank curricula for each task\n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task].copy()\n",
    "        \n",
    "        print(f\"\\n📊 {task.upper()} TASK RANKING\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create a scoring system\n",
    "        task_data['score'] = 0\n",
    "        \n",
    "        # Score based on final loss (lower is better)\n",
    "        loss_rank = task_data['final_total_loss'].rank()\n",
    "        task_data['score'] += (len(task_data) - loss_rank + 1) * 25\n",
    "        \n",
    "        # Score based on MSE (lower is better)\n",
    "        mse_valid = task_data['final_val_mse'] != float('inf')\n",
    "        if mse_valid.any():\n",
    "            mse_rank = task_data.loc[mse_valid, 'final_val_mse'].rank()\n",
    "            task_data.loc[mse_valid, 'score'] += (len(mse_rank) - mse_rank + 1) * 25\n",
    "        \n",
    "        # Task-specific scoring\n",
    "        if task == 'inverse':\n",
    "            # Score based on identity error\n",
    "            id_valid = task_data['final_identity_error'] != float('inf')\n",
    "            if id_valid.any():\n",
    "                id_rank = task_data.loc[id_valid, 'final_identity_error'].rank()\n",
    "                task_data.loc[id_valid, 'score'] += (len(id_rank) - id_rank + 1) * 25\n",
    "        elif task in ['addition', 'lowrank']:\n",
    "            # Score based on MAE\n",
    "            mae_valid = task_data['final_val_mae'] != float('inf')\n",
    "            if mae_valid.any():\n",
    "                mae_rank = task_data.loc[mae_valid, 'final_val_mae'].rank()\n",
    "                task_data.loc[mae_valid, 'score'] += (len(mae_rank) - mae_rank + 1) * 25\n",
    "        \n",
    "        # Score based on training efficiency\n",
    "        time_rank = task_data['avg_training_time'].rank()\n",
    "        task_data['score'] += (len(task_data) - time_rank + 1) * 25\n",
    "        \n",
    "        # Normalize scores to 0-100\n",
    "        max_score = task_data['score'].max()\n",
    "        if max_score > 0:\n",
    "            task_data['normalized_score'] = (task_data['score'] / max_score) * 100\n",
    "        else:\n",
    "            task_data['normalized_score'] = 0\n",
    "        \n",
    "        # Sort by score\n",
    "        task_data = task_data.sort_values('normalized_score', ascending=False)\n",
    "        \n",
    "        # Display ranking\n",
    "        rank_table = task_data[['name', 'normalized_score', 'final_total_loss', 'final_val_mse']].copy()\n",
    "        rank_table['rank'] = range(1, len(rank_table) + 1)\n",
    "        rank_table = rank_table[['rank', 'name', 'normalized_score', 'final_total_loss', 'final_val_mse']]\n",
    "        rank_table.columns = ['Rank', 'Curriculum', 'Score', 'Final Loss', 'MSE']\n",
    "        \n",
    "        print(rank_table.round(2).to_string(index=False))\n",
    "        \n",
    "        # Winner for this task\n",
    "        winner = task_data.iloc[0]\n",
    "        print(f\"\\n🏆 Winner for {task.upper()}: {winner['name']} (Score: {winner['normalized_score']:.1f})\")\n",
    "        \n",
    "        # Check if curriculum beats baseline\n",
    "        baseline_score = task_data[task_data['curriculum'] == 'baseline']['normalized_score'].values\n",
    "        if len(baseline_score) > 0 and winner['curriculum'] != 'baseline':\n",
    "            improvement = winner['normalized_score'] - baseline_score[0]\n",
    "            print(f\"   Improvement over baseline: +{improvement:.1f} points\")\n",
    "    \n",
    "    # Overall ranking across all tasks\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 OVERALL CROSS-TASK RANKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate average score across tasks for each curriculum\n",
    "    overall_scores = []\n",
    "    for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "        curriculum_data = summary_df[summary_df['curriculum'] == curriculum_key]\n",
    "        \n",
    "        if len(curriculum_data) > 0:\n",
    "            # Calculate average performance metrics\n",
    "            avg_loss = curriculum_data['final_total_loss'].mean()\n",
    "            avg_mse = curriculum_data[curriculum_data['final_val_mse'] != float('inf')]['final_val_mse'].mean()\n",
    "            avg_time = curriculum_data['avg_training_time'].mean()\n",
    "            \n",
    "            overall_scores.append({\n",
    "                'curriculum': curriculum_key,\n",
    "                'name': CURRICULUM_CONFIGS[curriculum_key]['name'],\n",
    "                'avg_loss': avg_loss,\n",
    "                'avg_mse': avg_mse if not np.isnan(avg_mse) else float('inf'),\n",
    "                'avg_time': avg_time,\n",
    "                'num_tasks': len(curriculum_data)\n",
    "            })\n",
    "    \n",
    "    overall_df = pd.DataFrame(overall_scores)\n",
    "    \n",
    "    # Rank based on average loss\n",
    "    overall_df['rank'] = overall_df['avg_loss'].rank().astype(int)\n",
    "    overall_df = overall_df.sort_values('rank')\n",
    "    \n",
    "    print(\"\\nOverall Performance Across All Tasks:\")\n",
    "    display_df = overall_df[['rank', 'name', 'avg_loss', 'avg_mse', 'avg_time']].copy()\n",
    "    display_df.columns = ['Rank', 'Curriculum', 'Avg Loss', 'Avg MSE', 'Avg Time']\n",
    "    print(display_df.round(4).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n🎯 KEY FINDINGS:\")\n",
    "    overall_winner = overall_df.iloc[0]\n",
    "    print(f\"   • Best Overall: {overall_winner['name']}\")\n",
    "    print(f\"   • Average Loss: {overall_winner['avg_loss']:.4f}\")\n",
    "    \n",
    "    # Check curriculum vs baseline\n",
    "    baseline_overall = overall_df[overall_df['curriculum'] == 'baseline']\n",
    "    if len(baseline_overall) > 0:\n",
    "        baseline_loss = baseline_overall['avg_loss'].iloc[0]\n",
    "        if overall_winner['curriculum'] != 'baseline':\n",
    "            improvement = (baseline_loss - overall_winner['avg_loss']) / baseline_loss * 100\n",
    "            print(f\"   • Improvement over baseline: {improvement:.1f}%\")\n",
    "        else:\n",
    "            print(\"   • Baseline performed best overall\")\n",
    "    \n",
    "    return overall_df\n",
    "\n",
    "# Generate comprehensive ranking\n",
    "print(\"Generating multi-task ranking analysis...\")\n",
    "overall_ranking = generate_task_ranking(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Summary Analysis and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "output_dir = Path('./multi_task_curriculum_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary data\n",
    "summary_df.to_csv(output_dir / 'multi_task_curriculum_summary.csv', index=False)\n",
    "overall_ranking.to_csv(output_dir / 'overall_curriculum_ranking.csv', index=False)\n",
    "\n",
    "# Create task-specific summaries\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    task_data.to_csv(output_dir / f'{task}_curriculum_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-TASK CURRICULUM TRAINING - FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📁 Results saved to: {output_dir}\")\n",
    "print(\"   • multi_task_curriculum_summary.csv - All results across tasks\")\n",
    "print(\"   • overall_curriculum_ranking.csv - Overall curriculum rankings\")\n",
    "for task in TASKS:\n",
    "    print(f\"   • {task}_curriculum_summary.csv - {task.capitalize()} task specific results\")\n",
    "\n",
    "# Training statistics\n",
    "total_configs = len(CURRICULUM_CONFIGS) * len(TASKS)\n",
    "successful_runs = sum(1 for _, task_results in training_results.items() \n",
    "                     for result in task_results.values() if result == 0)\n",
    "\n",
    "print(f\"\\n📊 TRAINING STATISTICS:\")\n",
    "print(f\"   • Total configurations tested: {total_configs}\")\n",
    "print(f\"   • Successful runs: {successful_runs}/{total_configs}\")\n",
    "print(f\"   • Tasks evaluated: {', '.join(TASKS)}\")\n",
    "print(f\"   • Curricula tested: {', '.join([cfg['name'] for cfg in CURRICULUM_CONFIGS.values()])}\")\n",
    "\n",
    "print(\"\\n🏆 EXECUTIVE SUMMARY:\")\n",
    "print(f\"   • Best Overall Curriculum: {overall_ranking.iloc[0]['name']}\")\n",
    "print(f\"   • Training steps per run: {COMMON_ARGS['train_num_steps']}\")\n",
    "print(f\"   • CSV logging interval: {COMMON_ARGS['csv_log_interval']} steps\")\n",
    "\n",
    "# Task-specific winners\n",
    "print(\"\\n🎯 TASK-SPECIFIC WINNERS:\")\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    if len(task_data) > 0:\n",
    "        # Find best by loss\n",
    "        best_idx = task_data['final_total_loss'].idxmin()\n",
    "        winner = task_data.loc[best_idx]\n",
    "        print(f\"   • {task.upper()}: {winner['name']} (Loss: {winner['final_total_loss']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 MULTI-TASK MULTI-CURRICULUM SMOKE TEST COMPLETED! 🚀\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nUse the generated CSV files and visualizations to:\")\n",
    "print(\"1. Select the best curriculum approach for each task\")\n",
    "print(\"2. Compare baseline vs curriculum learning effectiveness\")\n",
    "print(\"3. Understand trade-offs between different curriculum strategies\")\n",
    "print(\"4. Make informed decisions for production training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Export and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kkTaPFho-j9x",
    "outputId": "581b4dfa-db3a-49dd-9a32-1c3777b540e1"
   },
   "outputs": [],
   "source": [
    "def visualize_curriculum_comparison(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive side-by-side curriculum comparison visualizations.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Curriculum Training Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Total Loss Comparison\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['total_loss'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Total Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Energy Loss Comparison\n",
    "    ax = axes[0, 1]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'loss_energy' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['loss_energy'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Energy Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Energy Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Prepare colors for bar charts\n",
    "    colors = summary_df['color'].tolist()\n",
    "\n",
    "    # 3. Identity Error Comparison\n",
    "    ax = axes[1, 0]\n",
    "    identity_errors = summary_df['final_identity_error'].tolist()\n",
    "    curricula = summary_df['name'].tolist()\n",
    "\n",
    "    # Filter out inf values for plotting\n",
    "    valid_errors = [(c, e, col) for c, e, col in zip(curricula, identity_errors, colors)\n",
    "                    if e != float('inf')]\n",
    "\n",
    "    if valid_errors:\n",
    "        names, errors, cols = zip(*valid_errors)\n",
    "        bars = ax.bar(names, errors, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Identity Error (Lower = Better)', fontweight='bold')\n",
    "        ax.set_ylabel('||Pred @ Input - I||²')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, error in zip(bars, errors):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{error:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error (N/A)', fontweight='bold')\n",
    "\n",
    "    # 4. MSE Comparison\n",
    "    ax = axes[1, 1]\n",
    "    mse_values = summary_df['final_val_mse'].tolist()\n",
    "\n",
    "    # Filter out inf values\n",
    "    valid_mse = [(c, m, col) for c, m, col in zip(curricula, mse_values, colors)\n",
    "                 if m != float('inf')]\n",
    "\n",
    "    if valid_mse:\n",
    "        names, mses, cols = zip(*valid_mse)\n",
    "        bars = ax.bar(names, mses, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Mean Squared Error', fontweight='bold')\n",
    "        ax.set_ylabel('MSE')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, mse in zip(bars, mses):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{mse:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('MSE (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_inverse_task_metrics(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Visualize task-specific validation metrics over time (accuracy removed).\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Validation Metrics (Without Accuracy)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Identity Error over time\n",
    "    ax = axes[0, 0]\n",
    "    data_found = False\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        validation_df = data.get('validation')\n",
    "        if validation_df is not None:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Filter for identity_error metric\n",
    "            identity_df = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "            if not identity_df.empty:\n",
    "                ax.plot(identity_df['step'], identity_df['metric_value'],\n",
    "                        color=config['color'], label=config['name'],\n",
    "                        linewidth=2, alpha=0.8, marker='o')\n",
    "                data_found = True\n",
    "\n",
    "    if not data_found:\n",
    "        ax.text(0.5, 0.5, 'No identity error data available\\n(Validation runs every 50 steps)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "    ax.set_title('Identity Error Evolution (Lower = Better)', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('||Pred @ Input - I||²')\n",
    "    if data_found:\n",
    "        ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. MSE over time\n",
    "    ax = axes[0, 1]\n",
    "    data_found = False\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        validation_df = data.get('validation')\n",
    "        if validation_df is not None:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Filter for mse metric\n",
    "            mse_df = validation_df[validation_df['metric_name'] == 'mse']\n",
    "            if not mse_df.empty:\n",
    "                ax.plot(mse_df['step'], mse_df['metric_value'],\n",
    "                        color=config['color'], label=config['name'],\n",
    "                        linewidth=2, alpha=0.8, marker='o')\n",
    "                data_found = True\n",
    "\n",
    "    if not data_found:\n",
    "        ax.text(0.5, 0.5, 'No MSE data available\\n(Validation runs every 50 steps)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "    ax.set_title('Mean Squared Error Evolution', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('MSE')\n",
    "    if data_found:\n",
    "        ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Summary comparison (accuracy removed)\n",
    "    ax = axes[1, 0]\n",
    "\n",
    "    # Create grouped bar chart for final metrics (without accuracy)\n",
    "    metrics_to_plot = ['final_identity_error', 'final_val_mse']\n",
    "    metric_labels = ['Identity Error', 'MSE']\n",
    "\n",
    "    x = np.arange(len(summary_df))\n",
    "    width = 0.35\n",
    "\n",
    "    has_valid_data = False\n",
    "    for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "        values = summary_df[metric].values\n",
    "\n",
    "        # Check if we have any valid (non-inf) values\n",
    "        valid_vals = values[values != float('inf')]\n",
    "        if len(valid_vals) > 0:\n",
    "            has_valid_data = True\n",
    "            # For errors, invert so higher is better\n",
    "            max_val = valid_vals.max()\n",
    "            norm_values = 1 - (values / (max_val + 1e-8))\n",
    "            norm_values[values == float('inf')] = 0\n",
    "\n",
    "            bars = ax.bar(x + i*width, norm_values, width, label=label, alpha=0.7)\n",
    "\n",
    "    if has_valid_data:\n",
    "        ax.set_title('Normalized Performance Comparison (Higher = Better)', fontweight='bold')\n",
    "        ax.set_xlabel('Curriculum')\n",
    "        ax.set_ylabel('Normalized Score (0-1)')\n",
    "        ax.set_xticks(x + width / 2)\n",
    "        ax.set_xticklabels(summary_df['name'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Performance Comparison (N/A)', fontweight='bold')\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Remove unused fourth subplot\n",
    "    fig.delaxes(axes[1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_curriculum_convergence(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Analyze and visualize convergence patterns across curricula.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('Curriculum Convergence Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Loss convergence rate\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate rolling average for smoother curve\n",
    "            window = min(10, len(training_df) // 5)\n",
    "            if window > 1:\n",
    "                smoothed = training_df['total_loss'].rolling(window=window, min_periods=1).mean()\n",
    "                ax.plot(training_df['step'], smoothed,\n",
    "                        color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Smoothed Loss Convergence', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss (Moving Average)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Learning efficiency (loss reduction per step)\n",
    "    ax = axes[0, 1]\n",
    "    efficiency_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 1:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate loss reduction rate\n",
    "            initial_loss = training_df['total_loss'].iloc[:5].mean()\n",
    "            final_loss = training_df['total_loss'].iloc[-5:].mean()\n",
    "            steps = training_df['step'].iloc[-1] - training_df['step'].iloc[0]\n",
    "\n",
    "            if steps > 0 and initial_loss > 0:\n",
    "                efficiency = (initial_loss - final_loss) / steps\n",
    "                efficiency_data.append({\n",
    "                    'name': config['name'],\n",
    "                    'efficiency': efficiency,\n",
    "                    'color': config['color']\n",
    "                })\n",
    "\n",
    "    if efficiency_data:\n",
    "        eff_df = pd.DataFrame(efficiency_data)\n",
    "        bars = ax.bar(eff_df['name'], eff_df['efficiency'], color=eff_df['color'], alpha=0.7)\n",
    "        ax.set_title('Learning Efficiency (Loss Reduction per Step)', fontweight='bold')\n",
    "        ax.set_ylabel('Loss Reduction Rate')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for efficiency calculation',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Learning Efficiency (N/A)', fontweight='bold')\n",
    "\n",
    "    # 3. Stability analysis (loss variance)\n",
    "    ax = axes[1, 0]\n",
    "    stability_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 10:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate variance in second half of training\n",
    "            half_point = len(training_df) // 2\n",
    "            second_half_variance = training_df['total_loss'].iloc[half_point:].var()\n",
    "\n",
    "            stability_data.append({\n",
    "                'name': config['name'],\n",
    "                'variance': second_half_variance,\n",
    "                'color': config['color']\n",
    "            })\n",
    "\n",
    "    if stability_data:\n",
    "        stab_df = pd.DataFrame(stability_data)\n",
    "        bars = ax.bar(stab_df['name'], stab_df['variance'], color=stab_df['color'], alpha=0.7)\n",
    "        ax.set_title('Training Stability (Lower Variance = More Stable)', fontweight='bold')\n",
    "        ax.set_ylabel('Loss Variance (Second Half)')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for stability analysis',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Training Stability (N/A)', fontweight='bold')\n",
    "\n",
    "    # 4. Convergence speed comparison\n",
    "    ax = axes[1, 1]\n",
    "    convergence_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 5:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Find step where loss reaches 90% of final reduction\n",
    "            initial_loss = training_df['total_loss'].iloc[:5].mean()\n",
    "            final_loss = training_df['total_loss'].iloc[-5:].mean()\n",
    "            target_loss = initial_loss - 0.9 * (initial_loss - final_loss)\n",
    "\n",
    "            # Find first step where loss goes below target\n",
    "            below_target = training_df[training_df['total_loss'] <= target_loss]\n",
    "            if not below_target.empty:\n",
    "                convergence_step = below_target['step'].iloc[0]\n",
    "            else:\n",
    "                convergence_step = training_df['step'].iloc[-1]\n",
    "\n",
    "            convergence_data.append({\n",
    "                'name': config['name'],\n",
    "                'steps_to_converge': convergence_step,\n",
    "                'color': config['color']\n",
    "            })\n",
    "\n",
    "    if convergence_data:\n",
    "        conv_df = pd.DataFrame(convergence_data)\n",
    "        bars = ax.bar(conv_df['name'], conv_df['steps_to_converge'], color=conv_df['color'], alpha=0.7)\n",
    "        ax.set_title('Steps to 90% Convergence (Lower = Faster)', fontweight='bold')\n",
    "        ax.set_ylabel('Training Steps')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for convergence analysis',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Convergence Speed (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"Generating curriculum comparison visualizations...\")\n",
    "visualize_curriculum_comparison(all_results, summary_df)\n",
    "\n",
    "print(\"\\nGenerating inverse task specific visualizations...\")\n",
    "visualize_inverse_task_metrics(all_results, summary_df)\n",
    "\n",
    "print(\"\\nGenerating convergence analysis...\")\n",
    "visualize_curriculum_convergence(all_results, summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Consolidated Summary\n",
    "\n",
    "def generate_final_summary_report(summary_df: pd.DataFrame, all_results: Dict):\n",
    "    \"\"\"Generate a comprehensive final summary report matching IRED paper format.\"\"\"\n",
    "    from tabulate import tabulate\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY: IRED-STYLE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Overall Best Methods Table\n",
    "    print(\"\\n📊 BEST PERFORMING METHODS BY TASK\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    best_methods = []\n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task]\n",
    "        if len(task_data) > 0:\n",
    "            # Find best by MSE\n",
    "            valid_data = task_data[task_data['final_val_mse'] != float('inf')]\n",
    "            if len(valid_data) > 0:\n",
    "                best = valid_data.loc[valid_data['final_val_mse'].idxmin()]\n",
    "                \n",
    "                row = [\n",
    "                    task.upper(),\n",
    "                    best['name'],\n",
    "                    f\"{best['final_val_mse']:.4f}\" if isinstance(best['final_val_mse'], float) else \"N/A\"\n",
    "                ]\n",
    "                \n",
    "                # Add task-specific metrics\n",
    "                if task == 'inverse':\n",
    "                    identity = best['final_identity_error']\n",
    "                    row.append(f\"{identity:.4f}\" if identity != float('inf') else \"N/A\")\n",
    "                else:\n",
    "                    row.append(\"-\")\n",
    "                \n",
    "                best_methods.append(row)\n",
    "    \n",
    "    headers = ['Task', 'Best Method', 'MSE', 'Identity Error']\n",
    "    print(tabulate(best_methods, headers=headers, tablefmt='grid'))\n",
    "    \n",
    "    # 2. Training Efficiency Summary\n",
    "    print(\"\\n⚡ TRAINING EFFICIENCY METRICS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    efficiency_data = []\n",
    "    for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "        curriculum_data = summary_df[summary_df['curriculum'] == curriculum_key]\n",
    "        if len(curriculum_data) > 0:\n",
    "            avg_time = curriculum_data['avg_training_time'].mean()\n",
    "            avg_loss = curriculum_data['final_total_loss'].mean()\n",
    "            \n",
    "            # Calculate convergence rate if training data available\n",
    "            convergence_rate = \"N/A\"\n",
    "            if curriculum_key in all_results:\n",
    "                training_df = all_results[curriculum_key].get('training')\n",
    "                if training_df is not None and len(training_df) > 10:\n",
    "                    initial_loss = training_df['total_loss'].iloc[:5].mean()\n",
    "                    final_loss = training_df['total_loss'].iloc[-5:].mean()\n",
    "                    if initial_loss > 0:\n",
    "                        improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "                        convergence_rate = f\"{improvement:.1f}%\"\n",
    "            \n",
    "            efficiency_data.append([\n",
    "                CURRICULUM_CONFIGS[curriculum_key]['name'],\n",
    "                f\"{avg_time:.2f}\",\n",
    "                f\"{avg_loss:.4f}\",\n",
    "                convergence_rate\n",
    "            ])\n",
    "    \n",
    "    headers = ['Method', 'Avg Time (s)', 'Avg Loss', 'Loss Reduction']\n",
    "    print(tabulate(efficiency_data, headers=headers, tablefmt='grid'))\n",
    "    \n",
    "    # 3. Key Findings Summary\n",
    "    print(\"\\n🎯 KEY FINDINGS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find overall best method\n",
    "    valid_mse = summary_df[summary_df['final_val_mse'] != float('inf')]\n",
    "    if len(valid_mse) > 0:\n",
    "        overall_best = valid_mse.groupby('name')['final_val_mse'].mean().idxmin()\n",
    "        overall_best_mse = valid_mse.groupby('name')['final_val_mse'].mean().min()\n",
    "        \n",
    "        print(f\"• Best Overall Method: {overall_best}\")\n",
    "        print(f\"  Average MSE across tasks: {overall_best_mse:.4f}\")\n",
    "    \n",
    "    # Check baseline comparison\n",
    "    baseline_data = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "    non_baseline = summary_df[summary_df['curriculum'] != 'baseline']\n",
    "    \n",
    "    if len(baseline_data) > 0 and len(non_baseline) > 0:\n",
    "        baseline_avg = baseline_data[baseline_data['final_val_mse'] != float('inf')]['final_val_mse'].mean()\n",
    "        best_non_baseline = non_baseline[non_baseline['final_val_mse'] != float('inf')].groupby('name')['final_val_mse'].mean().min()\n",
    "        \n",
    "        if baseline_avg > 0 and not np.isnan(baseline_avg) and not np.isnan(best_non_baseline):\n",
    "            improvement = (baseline_avg - best_non_baseline) / baseline_avg * 100\n",
    "            print(f\"\\n• Curriculum Learning Impact:\")\n",
    "            print(f\"  Best curriculum improves over baseline by {improvement:.1f}%\")\n",
    "    \n",
    "    # Task-specific insights\n",
    "    print(\"\\n• Task-Specific Insights:\")\n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task]\n",
    "        valid_task_data = task_data[task_data['final_val_mse'] != float('inf')]\n",
    "        \n",
    "        if len(valid_task_data) > 1:\n",
    "            mse_range = valid_task_data['final_val_mse'].max() - valid_task_data['final_val_mse'].min()\n",
    "            print(f\"  {task.upper()}: MSE varies by {mse_range:.4f} across methods\")\n",
    "            \n",
    "            if task == 'inverse':\n",
    "                id_errors = task_data[task_data['final_identity_error'] != float('inf')]['final_identity_error']\n",
    "                if len(id_errors) > 0:\n",
    "                    print(f\"    Identity error range: {id_errors.min():.4f} - {id_errors.max():.4f}\")\n",
    "    \n",
    "    # 4. Recommendation\n",
    "    print(\"\\n💡 RECOMMENDATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find most consistent performer\n",
    "    method_scores = {}\n",
    "    for method in summary_df['name'].unique():\n",
    "        method_data = summary_df[summary_df['name'] == method]\n",
    "        valid_mse = method_data[method_data['final_val_mse'] != float('inf')]['final_val_mse']\n",
    "        \n",
    "        if len(valid_mse) == len(TASKS):  # Has results for all tasks\n",
    "            avg_mse = valid_mse.mean()\n",
    "            std_mse = valid_mse.std()\n",
    "            # Score based on low average and low variance (consistency)\n",
    "            score = avg_mse + 0.5 * std_mse  \n",
    "            method_scores[method] = (avg_mse, std_mse, score)\n",
    "    \n",
    "    if method_scores:\n",
    "        best_consistent = min(method_scores.items(), key=lambda x: x[1][2])\n",
    "        print(f\"• Most consistent method: {best_consistent[0]}\")\n",
    "        print(f\"  Avg MSE: {best_consistent[1][0]:.4f}, Std: {best_consistent[1][1]:.4f}\")\n",
    "    \n",
    "    print(\"\\n• For production use, consider:\")\n",
    "    print(\"  - Use curriculum learning for improved convergence\")\n",
    "    print(\"  - Monitor identity error for matrix inverse tasks\")\n",
    "    print(\"  - Track MSE as primary metric for all matrix operations\")\n",
    "\n",
    "# Generate the final summary report\n",
    "generate_final_summary_report(summary_df, all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N97tTHix-j9y",
    "outputId": "eb1787d1-fa57-4637-aabf-37893a596711"
   },
   "outputs": [],
   "source": [
    "def create_performance_radar_chart(summary_df: pd.DataFrame):\n",
    "    \"\"\"Create radar chart comparing multiple performance metrics.\"\"\"\n",
    "    from math import pi\n",
    "\n",
    "    # Select metrics for radar chart (normalized to 0-1 scale, higher = better)\n",
    "    metrics = {\n",
    "        'Loss Reduction': 1 / (summary_df['final_total_loss'] + 1e-6),  # Lower loss = better\n",
    "        'Training Speed': 1 / (summary_df['avg_training_time'] + 1e-6),   # Faster = better\n",
    "        'Best Loss': 1 / (summary_df['best_total_loss'] + 1e-6),         # Lower loss = better\n",
    "        'Val Accuracy': summary_df['best_val_accuracy'] / 100.0,          # Higher accuracy = better\n",
    "        'Energy Margin': summary_df['final_energy_margin'] / (summary_df['final_energy_margin'].max() + 1e-6)  # Higher margin = better\n",
    "    }\n",
    "\n",
    "    # Normalize all metrics to 0-1 scale\n",
    "    for metric_name, values in metrics.items():\n",
    "        if values.max() > 0:\n",
    "            metrics[metric_name] = values / values.max()\n",
    "\n",
    "    # Create radar chart\n",
    "    categories = list(metrics.keys())\n",
    "    N = len(categories)\n",
    "\n",
    "    # Compute angles\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    ax.set_title('Multi-Curriculum Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    # Plot each curriculum\n",
    "    for idx, row in summary_df.iterrows():\n",
    "        values = [metrics[cat].iloc[idx] for cat in categories]\n",
    "        values += values[:1]  # Complete the circle\n",
    "\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['name'],\n",
    "                color=row['color'], alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.15, color=row['color'])\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], alpha=0.7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def summary_analysis(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate comprehensive summary analysis with recommendations.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE CURRICULUM ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Performance metrics comparison\n",
    "    print(\"\\n📊 PERFORMANCE METRICS COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    comparison_table = summary_df[['name', 'final_total_loss', 'best_total_loss',\n",
    "                                  'final_val_accuracy', 'best_val_accuracy', 'avg_training_time']].copy()\n",
    "    comparison_table.columns = ['Curriculum', 'Final Loss', 'Best Loss', 'Final Acc (%)', 'Best Acc (%)', 'Avg Time (s)']\n",
    "\n",
    "    print(comparison_table.round(4).to_string(index=False))\n",
    "\n",
    "    # Statistical analysis\n",
    "    print(\"\\n📈 STATISTICAL INSIGHTS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if len(summary_df) > 1:\n",
    "        # Best performers\n",
    "        best_final_loss = summary_df.loc[summary_df['final_total_loss'].idxmin(), 'name']\n",
    "        best_convergence = summary_df.loc[summary_df['best_total_loss'].idxmin(), 'name']\n",
    "        fastest_training = summary_df.loc[summary_df['avg_training_time'].idxmin(), 'name']\n",
    "\n",
    "        print(f\"🎯 Best Final Loss: {best_final_loss}\")\n",
    "        print(f\"🎯 Best Convergence: {best_convergence}\")\n",
    "        print(f\"⚡ Fastest Training: {fastest_training}\")\n",
    "\n",
    "        if summary_df['best_val_accuracy'].max() > 0:\n",
    "            best_accuracy = summary_df.loc[summary_df['best_val_accuracy'].idxmax(), 'name']\n",
    "            print(f\"🎯 Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "        # Improvement analysis vs baseline\n",
    "        baseline_metrics = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        if len(baseline_metrics) > 0:\n",
    "            baseline_loss = baseline_metrics['final_total_loss'].iloc[0]\n",
    "\n",
    "            print(f\"\\n📈 IMPROVEMENT OVER BASELINE\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            for _, row in summary_df.iterrows():\n",
    "                if row['curriculum'] != 'baseline':\n",
    "                    improvement = (baseline_loss - row['final_total_loss']) / baseline_loss * 100\n",
    "                    print(f\"{row['name']}: {improvement:+.1f}% loss improvement\")\n",
    "\n",
    "    # Generate radar chart\n",
    "    print(\"\\n🎯 MULTI-DIMENSIONAL PERFORMANCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    create_performance_radar_chart(summary_df)\n",
    "\n",
    "    return comparison_table\n",
    "\n",
    "# Generate summary analysis\n",
    "comparison_table = summary_analysis(summary_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
