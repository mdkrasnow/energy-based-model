{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzh7-Ram-j9r"
   },
   "source": [
    "# Multi-Curriculum Smoke Test: Comprehensive Comparison\n",
    "\n",
    "Enhanced smoke test comparing baseline training against multiple curriculum configurations:\n",
    "- **Baseline**: No curriculum learning\n",
    "- **Default Curriculum**: Balanced adversarial introduction\n",
    "- **Aggressive Curriculum**: Rapid adversarial escalation\n",
    "- **Conservative Curriculum**: Gradual adversarial introduction\n",
    "\n",
    "This notebook provides comprehensive analysis, visualization, and ranking of all curriculum approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttlLgjdR-j9t",
    "outputId": "d4d35e3f-d98e-46d6-e01a-2a91434ecad3"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "%cd energy-based-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUU1Z1rj-j9u"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAmsGBDQ-j9v"
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZLi5DGT-j9v"
   },
   "source": [
    "## Configuration and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0MwXmDg-j9v",
    "outputId": "0f2b9706-161e-4bbc-bb09-6106e9b278cc"
   },
   "outputs": [],
   "source": [
    "# Common training parameters\n",
    "COMMON_ARGS = {\n",
    "    'dataset': 'inverse',\n",
    "    'model': 'mlp',\n",
    "    'batch_size': 32,\n",
    "    'diffusion_steps': 10,\n",
    "    'supervise_energy_landscape': 'True',\n",
    "    'train_num_steps': 10000,  # Increased for better curriculum comparison\n",
    "    'save_csv_logs': True,\n",
    "    'csv_log_interval': 50\n",
    "}\n",
    "\n",
    "# Curriculum configurations to test\n",
    "CURRICULUM_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'No curriculum learning',\n",
    "        'color': '#1f77b4',\n",
    "        'args': ['--disable-curriculum', 'True']\n",
    "    },\n",
    "    'default': {\n",
    "        'name': 'Default Curriculum',\n",
    "        'description': 'Balanced adversarial introduction',\n",
    "        'color': '#ff7f0e',\n",
    "        'args': ['--curriculum-config', 'default']\n",
    "    },\n",
    "    'aggressive': {\n",
    "        'name': 'Aggressive Curriculum',\n",
    "        'description': 'Rapid adversarial escalation',\n",
    "        'color': '#d62728',\n",
    "        'args': ['--curriculum-config', 'aggressive']\n",
    "    },\n",
    "    'conservative': {\n",
    "        'name': 'Conservative Curriculum',\n",
    "        'description': 'Gradual adversarial introduction',\n",
    "        'color': '#2ca02c',\n",
    "        'args': ['--curriculum-config', 'conservative']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(CURRICULUM_CONFIGS)} curriculum configurations:\")\n",
    "for key, config in CURRICULUM_CONFIGS.items():\n",
    "    print(f\"  • {config['name']}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBg0XbEp-j9w"
   },
   "outputs": [],
   "source": [
    "def build_training_command(curriculum_key: str) -> str:\n",
    "    \"\"\"Build training command for a specific curriculum configuration.\"\"\"\n",
    "    config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "    base_cmd = f\"\"\"python train.py \\\n",
    "        --dataset {COMMON_ARGS['dataset']} \\\n",
    "        --model {COMMON_ARGS['model']} \\\n",
    "        --batch_size {COMMON_ARGS['batch_size']} \\\n",
    "        --diffusion_steps {COMMON_ARGS['diffusion_steps']} \\\n",
    "        --supervise-energy-landscape {COMMON_ARGS['supervise_energy_landscape']} \\\n",
    "        --train-num-steps {COMMON_ARGS['train_num_steps']} \\\n",
    "        --save-csv-logs \\\n",
    "        --csv-log-interval {COMMON_ARGS['csv_log_interval']} \\\n",
    "        --csv-log-dir ./csv_logs_{curriculum_key}\"\"\"\n",
    "\n",
    "    # Add curriculum-specific arguments\n",
    "    if config['args']:\n",
    "        base_cmd += ' \\\n",
    "        ' + ' \\\n",
    "        '.join(config['args'])\n",
    "\n",
    "    return base_cmd\n",
    "\n",
    "def load_csv_data(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV data with error handling.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            return pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"Warning: {csv_path} not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_get_final_value(df: pd.DataFrame, column: str, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the final value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].iloc[-1])\n",
    "\n",
    "def safe_get_best_value(df: pd.DataFrame, column: str, minimize: bool = True, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the best (min/max) value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].min() if minimize else df[column].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHwwUrR-j9w"
   },
   "source": [
    "## Multi-Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i95hNsVz-j9w",
    "outputId": "2366f057-5bd2-43a1-9267-ef9745a36b8d"
   },
   "outputs": [],
   "source": [
    "# Train all curriculum configurations\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "for curriculum_key, config in CURRICULUM_CONFIGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting {config['name']} training...\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Build training command\n",
    "    cmd = build_training_command(curriculum_key)\n",
    "    print(f\"\\nCommand: {cmd}\")\n",
    "    print(\"\\nTraining output:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Execute training with real-time output\n",
    "    try:\n",
    "        # Use subprocess to capture and display output in real-time\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "\n",
    "        # Display output line by line as it comes\n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            if line:\n",
    "                print(line.rstrip())\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # Wait for process to complete\n",
    "        result = process.wait()\n",
    "        training_results[curriculum_key] = result\n",
    "\n",
    "        if result == 0:\n",
    "            print(f\"✓ {config['name']} training completed successfully\")\n",
    "        else:\n",
    "            print(f\"✗ {config['name']} training failed with exit code {result}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during {config['name']} training: {e}\")\n",
    "        training_results[curriculum_key] = -1\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All training completed!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print summary\n",
    "successful = sum(1 for result in training_results.values() if result == 0)\n",
    "print(f\"\\nTraining Summary: {successful}/{len(training_results)} configurations completed successfully\")\n",
    "for curriculum_key, result in training_results.items():\n",
    "    status = \"✓ Success\" if result == 0 else \"✗ Failed\"\n",
    "    print(f\"  {CURRICULUM_CONFIGS[curriculum_key]['name']}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNqMLoV_-j9x"
   },
   "source": [
    "## Comprehensive Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "YNPgrc2A-j9x",
    "outputId": "31e5a93b-e184-4817-f100-a0d58cd84c50"
   },
   "outputs": [],
   "source": [
    "def load_all_curriculum_results() -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Load all curriculum training results from CSV files.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "        print(f\"Loading {curriculum_key} data...\")\n",
    "\n",
    "        csv_dir = Path(f\"./csv_logs_{curriculum_key}\")\n",
    "\n",
    "        # Load different types of training data - look for files with timestamps\n",
    "        import glob\n",
    "\n",
    "        curriculum_data = {}\n",
    "\n",
    "        # Find the most recent file for each metric type\n",
    "        patterns = {\n",
    "            'training': 'training_metrics_*.csv',\n",
    "            'validation': 'validation_metrics_*.csv',\n",
    "            'energy': 'energy_metrics_*.csv',\n",
    "            'curriculum': 'curriculum_metrics_*.csv',\n",
    "            'robustness': 'robustness_metrics_*.csv'\n",
    "        }\n",
    "\n",
    "        for key, pattern in patterns.items():\n",
    "            files = glob.glob(str(csv_dir / pattern))\n",
    "            if files:\n",
    "                # Get most recent file\n",
    "                latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n",
    "                curriculum_data[key] = load_csv_data(Path(latest_file))\n",
    "            else:\n",
    "                curriculum_data[key] = None\n",
    "\n",
    "        # Count available data types\n",
    "        available = sum(1 for df in curriculum_data.values() if df is not None)\n",
    "        print(f\"  Found {available}/5 data files for {curriculum_key}\")\n",
    "\n",
    "        results[curriculum_key] = curriculum_data\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_curriculum_data(all_results: Dict[str, Dict[str, pd.DataFrame]]) -> pd.DataFrame:\n",
    "    \"\"\"Extract and standardize key metrics from all curriculum results.\"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "        # Extract key metrics\n",
    "        training_df = data.get('training')\n",
    "        validation_df = data.get('validation')\n",
    "        energy_df = data.get('energy')\n",
    "\n",
    "        # Extract accuracy from validation data for inverse task\n",
    "        val_accuracy = 0.0  # Changed from val_accuracy_pct, now using fraction\n",
    "        val_identity_error = float('inf')\n",
    "        val_mse = float('inf')\n",
    "\n",
    "        if validation_df is not None:\n",
    "            # Look for accuracy metric (changed from accuracy_pct)\n",
    "            accuracy_rows = validation_df[validation_df['metric_name'] == 'accuracy']\n",
    "            if not accuracy_rows.empty:\n",
    "                val_accuracy = accuracy_rows['metric_value'].iloc[-1]  # Get last value (now a fraction)\n",
    "\n",
    "            # Look for identity_error metric\n",
    "            identity_rows = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "            if not identity_rows.empty:\n",
    "                val_identity_error = identity_rows['metric_value'].iloc[-1]\n",
    "\n",
    "            # Look for MSE metric\n",
    "            mse_rows = validation_df[validation_df['metric_name'] == 'mse']\n",
    "            if not mse_rows.empty:\n",
    "                val_mse = mse_rows['metric_value'].iloc[-1]\n",
    "\n",
    "        metrics = {\n",
    "            'curriculum': curriculum_key,\n",
    "            'name': config['name'],\n",
    "            'color': config['color'],\n",
    "\n",
    "            # Training metrics\n",
    "            'final_total_loss': safe_get_final_value(training_df, 'total_loss'),\n",
    "            'final_energy_loss': safe_get_final_value(training_df, 'loss_energy'),\n",
    "            'final_denoise_loss': safe_get_final_value(training_df, 'loss_denoise'),\n",
    "            'best_total_loss': safe_get_best_value(training_df, 'total_loss', minimize=True),\n",
    "            'avg_training_time': training_df['nn_time'].mean() if training_df is not None and 'nn_time' in training_df.columns else 0.0,\n",
    "\n",
    "            # Validation metrics - now from actual data (accuracy as percentage for display)\n",
    "            'final_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "            'best_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "            'final_identity_error': val_identity_error,\n",
    "            'final_val_mse': val_mse,\n",
    "\n",
    "            # Energy metrics\n",
    "            'final_energy_margin': safe_get_final_value(energy_df, 'energy_margin'),\n",
    "            'max_curriculum_weight': safe_get_best_value(energy_df, 'curriculum_weight', minimize=False) if energy_df is not None and 'curriculum_weight' in energy_df.columns else 0.0,\n",
    "        }\n",
    "\n",
    "        processed_data.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Load all results\n",
    "print(\"Loading all curriculum results...\")\n",
    "all_results = load_all_curriculum_results()\n",
    "\n",
    "# Process data\n",
    "print(\"\\nProcessing curriculum data...\")\n",
    "summary_df = process_curriculum_data(all_results)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(summary_df)} curricula\")\n",
    "print(\"\\nSummary DataFrame:\")\n",
    "display(summary_df[['name', 'final_total_loss', 'final_val_accuracy', 'final_identity_error', 'final_val_mse']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1wtufOg-j9x"
   },
   "source": [
    "## Advanced Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kkTaPFho-j9x",
    "outputId": "581b4dfa-db3a-49dd-9a32-1c3777b540e1"
   },
   "outputs": [],
   "source": [
    "def visualize_curriculum_comparison(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Create comprehensive side-by-side curriculum comparison visualizations.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Curriculum Training Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Total Loss Comparison\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['total_loss'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Total Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Energy Loss Comparison\n",
    "    ax = axes[0, 1]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'loss_energy' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['loss_energy'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Energy Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Energy Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Prepare colors for bar charts\n",
    "    colors = summary_df['color'].tolist()\n",
    "\n",
    "    # 3. Identity Error Comparison\n",
    "    ax = axes[1, 0]\n",
    "    identity_errors = summary_df['final_identity_error'].tolist()\n",
    "    curricula = summary_df['name'].tolist()\n",
    "\n",
    "    # Filter out inf values for plotting\n",
    "    valid_errors = [(c, e, col) for c, e, col in zip(curricula, identity_errors, colors)\n",
    "                    if e != float('inf')]\n",
    "\n",
    "    if valid_errors:\n",
    "        names, errors, cols = zip(*valid_errors)\n",
    "        bars = ax.bar(names, errors, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Identity Error (Lower = Better)', fontweight='bold')\n",
    "        ax.set_ylabel('||Pred @ Input - I||²')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, error in zip(bars, errors):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{error:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Identity Error (N/A)', fontweight='bold')\n",
    "\n",
    "    # 4. MSE Comparison\n",
    "    ax = axes[1, 1]\n",
    "    mse_values = summary_df['final_val_mse'].tolist()\n",
    "\n",
    "    # Filter out inf values\n",
    "    valid_mse = [(c, m, col) for c, m, col in zip(curricula, mse_values, colors)\n",
    "                 if m != float('inf')]\n",
    "\n",
    "    if valid_mse:\n",
    "        names, mses, cols = zip(*valid_mse)\n",
    "        bars = ax.bar(names, mses, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Mean Squared Error', fontweight='bold')\n",
    "        ax.set_ylabel('MSE')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, mse in zip(bars, mses):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{mse:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('MSE (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_inverse_task_metrics(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Visualize task-specific validation metrics over time (accuracy removed).\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Validation Metrics (Without Accuracy)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Identity Error over time\n",
    "    ax = axes[0, 0]\n",
    "    data_found = False\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        validation_df = data.get('validation')\n",
    "        if validation_df is not None:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Filter for identity_error metric\n",
    "            identity_df = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "            if not identity_df.empty:\n",
    "                ax.plot(identity_df['step'], identity_df['metric_value'],\n",
    "                        color=config['color'], label=config['name'],\n",
    "                        linewidth=2, alpha=0.8, marker='o')\n",
    "                data_found = True\n",
    "\n",
    "    if not data_found:\n",
    "        ax.text(0.5, 0.5, 'No identity error data available\\n(Validation runs every 50 steps)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "    ax.set_title('Identity Error Evolution (Lower = Better)', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('||Pred @ Input - I||²')\n",
    "    if data_found:\n",
    "        ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. MSE over time\n",
    "    ax = axes[0, 1]\n",
    "    data_found = False\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        validation_df = data.get('validation')\n",
    "        if validation_df is not None:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Filter for mse metric\n",
    "            mse_df = validation_df[validation_df['metric_name'] == 'mse']\n",
    "            if not mse_df.empty:\n",
    "                ax.plot(mse_df['step'], mse_df['metric_value'],\n",
    "                        color=config['color'], label=config['name'],\n",
    "                        linewidth=2, alpha=0.8, marker='o')\n",
    "                data_found = True\n",
    "\n",
    "    if not data_found:\n",
    "        ax.text(0.5, 0.5, 'No MSE data available\\n(Validation runs every 50 steps)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "\n",
    "    ax.set_title('Mean Squared Error Evolution', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('MSE')\n",
    "    if data_found:\n",
    "        ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Summary comparison (accuracy removed)\n",
    "    ax = axes[1, 0]\n",
    "\n",
    "    # Create grouped bar chart for final metrics (without accuracy)\n",
    "    metrics_to_plot = ['final_identity_error', 'final_val_mse']\n",
    "    metric_labels = ['Identity Error', 'MSE']\n",
    "\n",
    "    x = np.arange(len(summary_df))\n",
    "    width = 0.35\n",
    "\n",
    "    has_valid_data = False\n",
    "    for i, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
    "        values = summary_df[metric].values\n",
    "\n",
    "        # Check if we have any valid (non-inf) values\n",
    "        valid_vals = values[values != float('inf')]\n",
    "        if len(valid_vals) > 0:\n",
    "            has_valid_data = True\n",
    "            # For errors, invert so higher is better\n",
    "            max_val = valid_vals.max()\n",
    "            norm_values = 1 - (values / (max_val + 1e-8))\n",
    "            norm_values[values == float('inf')] = 0\n",
    "\n",
    "            bars = ax.bar(x + i*width, norm_values, width, label=label, alpha=0.7)\n",
    "\n",
    "    if has_valid_data:\n",
    "        ax.set_title('Normalized Performance Comparison (Higher = Better)', fontweight='bold')\n",
    "        ax.set_xlabel('Curriculum')\n",
    "        ax.set_ylabel('Normalized Score (0-1)')\n",
    "        ax.set_xticks(x + width / 2)\n",
    "        ax.set_xticklabels(summary_df['name'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('Performance Comparison (N/A)', fontweight='bold')\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Remove unused fourth subplot\n",
    "    fig.delaxes(axes[1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_curriculum_convergence(all_results: Dict, summary_df: pd.DataFrame):\n",
    "    \"\"\"Analyze and visualize convergence patterns across curricula.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('Curriculum Convergence Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Loss convergence rate\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate rolling average for smoother curve\n",
    "            window = min(10, len(training_df) // 5)\n",
    "            if window > 1:\n",
    "                smoothed = training_df['total_loss'].rolling(window=window, min_periods=1).mean()\n",
    "                ax.plot(training_df['step'], smoothed,\n",
    "                        color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Smoothed Loss Convergence', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss (Moving Average)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Learning efficiency (loss reduction per step)\n",
    "    ax = axes[0, 1]\n",
    "    efficiency_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 1:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate loss reduction rate\n",
    "            initial_loss = training_df['total_loss'].iloc[:5].mean()\n",
    "            final_loss = training_df['total_loss'].iloc[-5:].mean()\n",
    "            steps = training_df['step'].iloc[-1] - training_df['step'].iloc[0]\n",
    "\n",
    "            if steps > 0 and initial_loss > 0:\n",
    "                efficiency = (initial_loss - final_loss) / steps\n",
    "                efficiency_data.append({\n",
    "                    'name': config['name'],\n",
    "                    'efficiency': efficiency,\n",
    "                    'color': config['color']\n",
    "                })\n",
    "\n",
    "    if efficiency_data:\n",
    "        eff_df = pd.DataFrame(efficiency_data)\n",
    "        bars = ax.bar(eff_df['name'], eff_df['efficiency'], color=eff_df['color'], alpha=0.7)\n",
    "        ax.set_title('Learning Efficiency (Loss Reduction per Step)', fontweight='bold')\n",
    "        ax.set_ylabel('Loss Reduction Rate')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for efficiency calculation',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Learning Efficiency (N/A)', fontweight='bold')\n",
    "\n",
    "    # 3. Stability analysis (loss variance)\n",
    "    ax = axes[1, 0]\n",
    "    stability_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 10:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Calculate variance in second half of training\n",
    "            half_point = len(training_df) // 2\n",
    "            second_half_variance = training_df['total_loss'].iloc[half_point:].var()\n",
    "\n",
    "            stability_data.append({\n",
    "                'name': config['name'],\n",
    "                'variance': second_half_variance,\n",
    "                'color': config['color']\n",
    "            })\n",
    "\n",
    "    if stability_data:\n",
    "        stab_df = pd.DataFrame(stability_data)\n",
    "        bars = ax.bar(stab_df['name'], stab_df['variance'], color=stab_df['color'], alpha=0.7)\n",
    "        ax.set_title('Training Stability (Lower Variance = More Stable)', fontweight='bold')\n",
    "        ax.set_ylabel('Loss Variance (Second Half)')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for stability analysis',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Training Stability (N/A)', fontweight='bold')\n",
    "\n",
    "    # 4. Convergence speed comparison\n",
    "    ax = axes[1, 1]\n",
    "    convergence_data = []\n",
    "    for curriculum_key, data in all_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns and len(training_df) > 5:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Find step where loss reaches 90% of final reduction\n",
    "            initial_loss = training_df['total_loss'].iloc[:5].mean()\n",
    "            final_loss = training_df['total_loss'].iloc[-5:].mean()\n",
    "            target_loss = initial_loss - 0.9 * (initial_loss - final_loss)\n",
    "\n",
    "            # Find first step where loss goes below target\n",
    "            below_target = training_df[training_df['total_loss'] <= target_loss]\n",
    "            if not below_target.empty:\n",
    "                convergence_step = below_target['step'].iloc[0]\n",
    "            else:\n",
    "                convergence_step = training_df['step'].iloc[-1]\n",
    "\n",
    "            convergence_data.append({\n",
    "                'name': config['name'],\n",
    "                'steps_to_converge': convergence_step,\n",
    "                'color': config['color']\n",
    "            })\n",
    "\n",
    "    if convergence_data:\n",
    "        conv_df = pd.DataFrame(convergence_data)\n",
    "        bars = ax.bar(conv_df['name'], conv_df['steps_to_converge'], color=conv_df['color'], alpha=0.7)\n",
    "        ax.set_title('Steps to 90% Convergence (Lower = Faster)', fontweight='bold')\n",
    "        ax.set_ylabel('Training Steps')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient data for convergence analysis',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Convergence Speed (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"Generating curriculum comparison visualizations...\")\n",
    "visualize_curriculum_comparison(all_results, summary_df)\n",
    "\n",
    "print(\"\\nGenerating inverse task specific visualizations...\")\n",
    "visualize_inverse_task_metrics(all_results, summary_df)\n",
    "\n",
    "print(\"\\nGenerating convergence analysis...\")\n",
    "visualize_curriculum_convergence(all_results, summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwCctp-A-j9y"
   },
   "source": [
    "## Enhanced Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N97tTHix-j9y",
    "outputId": "eb1787d1-fa57-4637-aabf-37893a596711"
   },
   "outputs": [],
   "source": [
    "def create_performance_radar_chart(summary_df: pd.DataFrame):\n",
    "    \"\"\"Create radar chart comparing multiple performance metrics.\"\"\"\n",
    "    from math import pi\n",
    "\n",
    "    # Select metrics for radar chart (normalized to 0-1 scale, higher = better)\n",
    "    metrics = {\n",
    "        'Loss Reduction': 1 / (summary_df['final_total_loss'] + 1e-6),  # Lower loss = better\n",
    "        'Training Speed': 1 / (summary_df['avg_training_time'] + 1e-6),   # Faster = better\n",
    "        'Best Loss': 1 / (summary_df['best_total_loss'] + 1e-6),         # Lower loss = better\n",
    "        'Val Accuracy': summary_df['best_val_accuracy'] / 100.0,          # Higher accuracy = better\n",
    "        'Energy Margin': summary_df['final_energy_margin'] / (summary_df['final_energy_margin'].max() + 1e-6)  # Higher margin = better\n",
    "    }\n",
    "\n",
    "    # Normalize all metrics to 0-1 scale\n",
    "    for metric_name, values in metrics.items():\n",
    "        if values.max() > 0:\n",
    "            metrics[metric_name] = values / values.max()\n",
    "\n",
    "    # Create radar chart\n",
    "    categories = list(metrics.keys())\n",
    "    N = len(categories)\n",
    "\n",
    "    # Compute angles\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    ax.set_title('Multi-Curriculum Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    # Plot each curriculum\n",
    "    for idx, row in summary_df.iterrows():\n",
    "        values = [metrics[cat].iloc[idx] for cat in categories]\n",
    "        values += values[:1]  # Complete the circle\n",
    "\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['name'],\n",
    "                color=row['color'], alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.15, color=row['color'])\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], alpha=0.7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def summary_analysis(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate comprehensive summary analysis with recommendations.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE CURRICULUM ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Performance metrics comparison\n",
    "    print(\"\\n📊 PERFORMANCE METRICS COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    comparison_table = summary_df[['name', 'final_total_loss', 'best_total_loss',\n",
    "                                  'final_val_accuracy', 'best_val_accuracy', 'avg_training_time']].copy()\n",
    "    comparison_table.columns = ['Curriculum', 'Final Loss', 'Best Loss', 'Final Acc (%)', 'Best Acc (%)', 'Avg Time (s)']\n",
    "\n",
    "    print(comparison_table.round(4).to_string(index=False))\n",
    "\n",
    "    # Statistical analysis\n",
    "    print(\"\\n📈 STATISTICAL INSIGHTS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if len(summary_df) > 1:\n",
    "        # Best performers\n",
    "        best_final_loss = summary_df.loc[summary_df['final_total_loss'].idxmin(), 'name']\n",
    "        best_convergence = summary_df.loc[summary_df['best_total_loss'].idxmin(), 'name']\n",
    "        fastest_training = summary_df.loc[summary_df['avg_training_time'].idxmin(), 'name']\n",
    "\n",
    "        print(f\"🎯 Best Final Loss: {best_final_loss}\")\n",
    "        print(f\"🎯 Best Convergence: {best_convergence}\")\n",
    "        print(f\"⚡ Fastest Training: {fastest_training}\")\n",
    "\n",
    "        if summary_df['best_val_accuracy'].max() > 0:\n",
    "            best_accuracy = summary_df.loc[summary_df['best_val_accuracy'].idxmax(), 'name']\n",
    "            print(f\"🎯 Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "        # Improvement analysis vs baseline\n",
    "        baseline_metrics = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "        if len(baseline_metrics) > 0:\n",
    "            baseline_loss = baseline_metrics['final_total_loss'].iloc[0]\n",
    "\n",
    "            print(f\"\\n📈 IMPROVEMENT OVER BASELINE\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            for _, row in summary_df.iterrows():\n",
    "                if row['curriculum'] != 'baseline':\n",
    "                    improvement = (baseline_loss - row['final_total_loss']) / baseline_loss * 100\n",
    "                    print(f\"{row['name']}: {improvement:+.1f}% loss improvement\")\n",
    "\n",
    "    # Generate radar chart\n",
    "    print(\"\\n🎯 MULTI-DIMENSIONAL PERFORMANCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    create_performance_radar_chart(summary_df)\n",
    "\n",
    "    return comparison_table\n",
    "\n",
    "# Generate summary analysis\n",
    "comparison_table = summary_analysis(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa1Jektc-j9y"
   },
   "source": [
    "## Final Ranking System and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTaJY-ps-j9y",
    "outputId": "970d4e12-37df-4142-abe0-791a526be5b8"
   },
   "outputs": [],
   "source": [
    "def rank_curricula(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Comprehensive curriculum ranking with scoring breakdown.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL CURRICULUM RANKING AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Define scoring weights for different aspects\n",
    "    scoring_weights = {\n",
    "        'final_loss': 0.3,      # 30% weight on final performance\n",
    "        'convergence': 0.25,     # 25% weight on best achieved loss\n",
    "        'accuracy': 0.25,        # 25% weight on validation accuracy\n",
    "        'efficiency': 0.2        # 20% weight on training efficiency\n",
    "    }\n",
    "\n",
    "    # Calculate individual scores (0-100 scale, higher = better)\n",
    "    ranking_df = summary_df.copy()\n",
    "\n",
    "    # Final loss score (lower is better -> invert)\n",
    "    if ranking_df['final_total_loss'].max() > ranking_df['final_total_loss'].min():\n",
    "        ranking_df['final_loss_score'] = 100 * (1 - (ranking_df['final_total_loss'] - ranking_df['final_total_loss'].min()) /\n",
    "                                                (ranking_df['final_total_loss'].max() - ranking_df['final_total_loss'].min()))\n",
    "    else:\n",
    "        ranking_df['final_loss_score'] = 100.0\n",
    "\n",
    "    # Convergence score (best loss achieved)\n",
    "    if ranking_df['best_total_loss'].max() > ranking_df['best_total_loss'].min():\n",
    "        ranking_df['convergence_score'] = 100 * (1 - (ranking_df['best_total_loss'] - ranking_df['best_total_loss'].min()) /\n",
    "                                                 (ranking_df['best_total_loss'].max() - ranking_df['best_total_loss'].min()))\n",
    "    else:\n",
    "        ranking_df['convergence_score'] = 100.0\n",
    "\n",
    "    # Accuracy score\n",
    "    if ranking_df['best_val_accuracy'].max() > 0:\n",
    "        ranking_df['accuracy_score'] = 100 * ranking_df['best_val_accuracy'] / ranking_df['best_val_accuracy'].max()\n",
    "    else:\n",
    "        ranking_df['accuracy_score'] = 50.0  # Neutral score when no accuracy data\n",
    "\n",
    "    # Efficiency score (faster is better -> invert time)\n",
    "    if ranking_df['avg_training_time'].max() > ranking_df['avg_training_time'].min() and ranking_df['avg_training_time'].min() > 0:\n",
    "        ranking_df['efficiency_score'] = 100 * (1 - (ranking_df['avg_training_time'] - ranking_df['avg_training_time'].min()) /\n",
    "                                               (ranking_df['avg_training_time'].max() - ranking_df['avg_training_time'].min()))\n",
    "    else:\n",
    "        ranking_df['efficiency_score'] = 100.0\n",
    "\n",
    "    # Calculate overall score\n",
    "    ranking_df['overall_score'] = (ranking_df['final_loss_score'] * scoring_weights['final_loss'] +\n",
    "                                  ranking_df['convergence_score'] * scoring_weights['convergence'] +\n",
    "                                  ranking_df['accuracy_score'] * scoring_weights['accuracy'] +\n",
    "                                  ranking_df['efficiency_score'] * scoring_weights['efficiency'])\n",
    "\n",
    "    # Sort by overall score\n",
    "    ranking_df = ranking_df.sort_values('overall_score', ascending=False)\n",
    "\n",
    "    # Display ranking table\n",
    "    print(\"\\n🏆 CURRICULUM RANKING (Overall Score)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    ranking_table = ranking_df[['name', 'overall_score', 'final_loss_score',\n",
    "                               'convergence_score', 'accuracy_score', 'efficiency_score']].copy()\n",
    "    ranking_table.columns = ['Curriculum', 'Overall', 'Final Loss', 'Convergence', 'Accuracy', 'Efficiency']\n",
    "    ranking_table = ranking_table.round(1)\n",
    "\n",
    "    for i, (idx, row) in enumerate(ranking_table.iterrows()):\n",
    "        rank_emoji = ['🥇', '🥈', '🥉', '🏅'][min(i, 3)]\n",
    "        print(f\"{rank_emoji} {i+1}. {row['Curriculum']}: {row['Overall']:.1f} points\")\n",
    "        print(f\"    Final Loss: {row['Final Loss']:.1f} | Convergence: {row['Convergence']:.1f} | \"\n",
    "              f\"Accuracy: {row['Accuracy']:.1f} | Efficiency: {row['Efficiency']:.1f}\")\n",
    "\n",
    "    # Use-case specific recommendations\n",
    "    print(\"\\n🎯 USE-CASE SPECIFIC RECOMMENDATIONS\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Production deployment (balanced performance)\n",
    "    production_idx = ranking_df['overall_score'].idxmax()\n",
    "    production_winner = ranking_df.loc[production_idx, 'name']\n",
    "\n",
    "    # Research/accuracy focus\n",
    "    if ranking_df['best_val_accuracy'].max() > 0:\n",
    "        accuracy_idx = ranking_df['best_val_accuracy'].idxmax()\n",
    "        accuracy_winner = ranking_df.loc[accuracy_idx, 'name']\n",
    "    else:\n",
    "        accuracy_winner = ranking_df.loc[ranking_df['convergence_score'].idxmax(), 'name']\n",
    "\n",
    "    # Speed/efficiency focus\n",
    "    speed_idx = ranking_df['efficiency_score'].idxmax()\n",
    "    speed_winner = ranking_df.loc[speed_idx, 'name']\n",
    "\n",
    "    # Robustness focus (best final performance)\n",
    "    robust_idx = ranking_df['final_loss_score'].idxmax()\n",
    "    robust_winner = ranking_df.loc[robust_idx, 'name']\n",
    "\n",
    "    print(f\"🏭 Production Deployment: {production_winner}\")\n",
    "    print(f\"   (Best overall balance of performance, efficiency, and reliability)\")\n",
    "\n",
    "    print(f\"\\n🔬 Research/Accuracy Focus: {accuracy_winner}\")\n",
    "    print(f\"   (Highest validation accuracy or best convergence)\")\n",
    "\n",
    "    print(f\"\\n⚡ Speed/Efficiency Focus: {speed_winner}\")\n",
    "    print(f\"   (Fastest training with acceptable performance)\")\n",
    "\n",
    "    print(f\"\\n🛡️ Robustness Focus: {robust_winner}\")\n",
    "    print(f\"   (Most stable final performance)\")\n",
    "\n",
    "    # Confidence assessment\n",
    "    print(\"\\n📊 CONFIDENCE ASSESSMENT\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    score_spread = ranking_df['overall_score'].max() - ranking_df['overall_score'].min()\n",
    "    top_2_diff = ranking_df['overall_score'].iloc[0] - ranking_df['overall_score'].iloc[1] if len(ranking_df) > 1 else 100\n",
    "\n",
    "    if top_2_diff > 10:\n",
    "        confidence = \"HIGH\"\n",
    "        confidence_desc = \"Clear winner with significant performance advantage\"\n",
    "    elif top_2_diff > 5:\n",
    "        confidence = \"MEDIUM\"\n",
    "        confidence_desc = \"Moderate performance difference, winner is reasonably clear\"\n",
    "    else:\n",
    "        confidence = \"LOW\"\n",
    "        confidence_desc = \"Very close performance, longer training may be needed for definitive ranking\"\n",
    "\n",
    "    print(f\"Confidence Level: {confidence}\")\n",
    "    print(f\"Assessment: {confidence_desc}\")\n",
    "    print(f\"Score Spread: {score_spread:.1f} points\")\n",
    "    print(f\"Top 2 Difference: {top_2_diff:.1f} points\")\n",
    "\n",
    "    return ranking_df\n",
    "\n",
    "# Generate final ranking\n",
    "final_ranking = rank_curricula(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-ruQSaM-j9y"
   },
   "source": [
    "## Results Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yait6iFw-j9y",
    "outputId": "66eaa819-6062-407a-c056-bc8b9a1adcd5"
   },
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "output_dir = Path('./multi_curriculum_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary data\n",
    "summary_df.to_csv(output_dir / 'curriculum_summary.csv', index=False)\n",
    "final_ranking.to_csv(output_dir / 'curriculum_ranking.csv', index=False)\n",
    "comparison_table.to_csv(output_dir / 'performance_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS EXPORT AND FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📁 Results saved to: {output_dir}\")\n",
    "print(\"   • curriculum_summary.csv - Raw metrics for all curricula\")\n",
    "print(\"   • curriculum_ranking.csv - Final ranking with scores\")\n",
    "print(\"   • performance_comparison.csv - Performance comparison table\")\n",
    "\n",
    "# Generate final executive summary\n",
    "winner = final_ranking.iloc[0]\n",
    "baseline_perf = summary_df[summary_df['curriculum'] == 'baseline']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🎯 WINNER: {winner['name']}\")\n",
    "print(f\"   Overall Score: {winner['overall_score']:.1f}/100\")\n",
    "print(f\"   Final Loss: {winner['final_total_loss']:.4f}\")\n",
    "print(f\"   Best Loss: {winner['best_total_loss']:.4f}\")\n",
    "print(f\"   Training Time: {winner['avg_training_time']:.3f}s per step\")\n",
    "\n",
    "if len(baseline_perf) > 0:\n",
    "    baseline_loss = baseline_perf['final_total_loss'].iloc[0]\n",
    "    winner_improvement = (baseline_loss - winner['final_total_loss']) / baseline_loss * 100\n",
    "    print(f\"   Improvement over Baseline: {winner_improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\n📈 KEY FINDINGS:\")\n",
    "successful_configs = len([r for r in training_results.values() if r == 0])\n",
    "print(f\"   • Successfully tested {successful_configs}/{len(CURRICULUM_CONFIGS)} curriculum configurations\")\n",
    "\n",
    "if len(final_ranking) > 1:\n",
    "    top_2_diff = final_ranking['overall_score'].iloc[0] - final_ranking['overall_score'].iloc[1]\n",
    "    print(f\"   • Performance gap between top 2: {top_2_diff:.1f} points\")\n",
    "\n",
    "    if winner['curriculum'] != 'baseline':\n",
    "        print(f\"   • Curriculum learning outperformed baseline training\")\n",
    "    else:\n",
    "        print(f\"   • Baseline training performed competitively with curriculum approaches\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDATIONS:\")\n",
    "print(f\"   • For production use: Deploy {winner['name']}\")\n",
    "print(f\"   • Training steps: {COMMON_ARGS['train_num_steps']} steps provided good differentiation\")\n",
    "print(f\"   • Consider longer training for final model selection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 MULTI-CURRICULUM SMOKE TEST COMPLETED SUCCESSFULLY! 🚀\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
