{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzh7-Ram-j9r"
   },
   "source": [
    "# Multi-Curriculum Smoke Test: Comprehensive Comparison\n",
    "\n",
    "Enhanced smoke test comparing baseline training against multiple curriculum configurations:\n",
    "- **Baseline**: No curriculum learning\n",
    "- **Default Curriculum**: Balanced adversarial introduction\n",
    "- **Aggressive Curriculum**: Rapid adversarial escalation\n",
    "- **Conservative Curriculum**: Gradual adversarial introduction\n",
    "\n",
    "This notebook provides comprehensive analysis, visualization, and ranking of all curriculum approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttlLgjdR-j9t",
    "outputId": "83d61168-db61-4121-d3ff-9a9c36e2a36d"
   },
   "outputs": [],
   "source": [
    "# Clone repository and setup\n",
    "!rm -rf energy-based-model\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model.git\n",
    "%cd energy-based-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUU1Z1rj-j9u"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8HcG3_HdafT"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import subprocess\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0MwXmDg-j9v",
    "outputId": "796dd879-bf6c-4cda-b4d0-cc51aa6bae57"
   },
   "outputs": [],
   "source": [
    "# Common training parameters\n",
    "COMMON_ARGS = {\n",
    "    'model': 'mlp',\n",
    "    'batch_size': 2048,\n",
    "    'diffusion_steps': 10,\n",
    "    'supervise_energy_landscape': 'True',\n",
    "    'train_num_steps': 50000,  \n",
    "    'save_csv_logs': True,\n",
    "    'csv_log_interval': 1000\n",
    "}\n",
    "\n",
    "# Tasks to test\n",
    "TASKS = ['inverse', 'addition', 'lowrank']\n",
    "\n",
    "# Curriculum configurations to test\n",
    "CURRICULUM_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'No curriculum learning',\n",
    "        'color': '#1f77b4',\n",
    "        'args': ['--disable-curriculum', 'True']\n",
    "    },\n",
    "    'aggressive': {\n",
    "        'name': 'Aggressive Curriculum',\n",
    "        'description': 'Rapid adversarial escalation',\n",
    "        'color': '#d62728',\n",
    "        'args': ['--curriculum-config', 'aggressive']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(CURRICULUM_CONFIGS)} curriculum configurations on {len(TASKS)} tasks:\")\n",
    "print(f\"Tasks: {', '.join(TASKS)}\")\n",
    "for key, config in CURRICULUM_CONFIGS.items():\n",
    "    print(f\"  • {config['name']}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBg0XbEp-j9w"
   },
   "outputs": [],
   "source": [
    "def build_training_command(curriculum_key: str, task: str = 'inverse') -> str:\n",
    "    \"\"\"Build training command for a specific curriculum configuration and task.\"\"\"\n",
    "    config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "    base_cmd = f\"\"\"python train.py \\\n",
    "        --dataset {task} \\\n",
    "        --model {COMMON_ARGS['model']} \\\n",
    "        --batch_size {COMMON_ARGS['batch_size']} \\\n",
    "        --diffusion_steps {COMMON_ARGS['diffusion_steps']} \\\n",
    "        --supervise-energy-landscape {COMMON_ARGS['supervise_energy_landscape']} \\\n",
    "        --train-num-steps {COMMON_ARGS['train_num_steps']} \\\n",
    "        --save-csv-logs \\\n",
    "        --csv-log-interval {COMMON_ARGS['csv_log_interval']} \\\n",
    "        --csv-log-dir ./csv_logs_{task}_{curriculum_key}\"\"\"\n",
    "\n",
    "    # Add curriculum-specific arguments\n",
    "    if config['args']:\n",
    "        base_cmd += ' \\\n",
    "        ' + ' \\\n",
    "        '.join(config['args'])\n",
    "\n",
    "    return base_cmd\n",
    "\n",
    "def load_csv_data(csv_path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV data with error handling.\"\"\"\n",
    "    try:\n",
    "        if csv_path.exists():\n",
    "            return pd.read_csv(csv_path)\n",
    "        else:\n",
    "            print(f\"Warning: {csv_path} not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def safe_get_final_value(df: pd.DataFrame, column: str, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the final value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].iloc[-1])\n",
    "\n",
    "def safe_get_best_value(df: pd.DataFrame, column: str, minimize: bool = True, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely get the best (min/max) value from a dataframe column.\"\"\"\n",
    "    if df is None or column not in df.columns or len(df) == 0:\n",
    "        return default\n",
    "    return float(df[column].min() if minimize else df[column].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHwwUrR-j9w"
   },
   "source": [
    "## Multi-Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqTqOuXadD4d",
    "outputId": "13538288-55c4-45fc-eeda-8c929a1e2328"
   },
   "outputs": [],
   "source": [
    "# Train all tasks with all curriculum configurations\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "# Train each task\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# TASK: {task.upper()}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "\n",
    "    task_results = {}\n",
    "\n",
    "    # For each curriculum configuration\n",
    "    for curriculum_key, config in CURRICULUM_CONFIGS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting {config['name']} training for {task} task...\")\n",
    "        print(f\"Description: {config['description']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Build training command\n",
    "        cmd = build_training_command(curriculum_key, task)\n",
    "        print(f\"\\nCommand: {cmd}\")\n",
    "        print(\"\\nTraining output:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Execute training with real-time output\n",
    "        try:\n",
    "            # Use subprocess to capture and display output in real-time\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "\n",
    "            # Display output line by line as it comes\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                if line:\n",
    "                    print(line.rstrip())\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Wait for process to complete\n",
    "            result = process.wait()\n",
    "            task_results[curriculum_key] = result\n",
    "\n",
    "            if result == 0:\n",
    "                print(f\"✓ {config['name']} training for {task} completed successfully\")\n",
    "            else:\n",
    "                print(f\"✗ {config['name']} training for {task} failed with exit code {result}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during {config['name']} training for {task}: {e}\")\n",
    "            task_results[curriculum_key] = -1\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    training_results[task] = task_results\n",
    "\n",
    "    # Print task summary\n",
    "    successful = sum(1 for result in task_results.values() if result == 0)\n",
    "    print(f\"\\n{task.upper()} Task Summary: {successful}/{len(task_results)} configurations completed successfully\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All training completed!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Print overall summary\n",
    "print(\"\\nOVERALL TRAINING SUMMARY:\")\n",
    "for task, task_results in training_results.items():\n",
    "    successful = sum(1 for result in task_results.values() if result == 0)\n",
    "    print(f\"\\n{task.upper()} Task: {successful}/{len(task_results)} configurations successful\")\n",
    "    for curriculum_key, result in task_results.items():\n",
    "        status = \"✓ Success\" if result == 0 else \"✗ Failed\"\n",
    "        print(f\"  {CURRICULUM_CONFIGS[curriculum_key]['name']}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "KNqMLoV_-j9x",
    "outputId": "e99d2124-461c-4dc6-d94e-540b2444823d"
   },
   "outputs": [],
   "source": [
    "def load_all_curriculum_results() -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Load all curriculum training results from CSV files for all tasks.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for task in TASKS:\n",
    "        print(f\"\\nLoading results for {task} task...\")\n",
    "        task_results = {}\n",
    "\n",
    "        for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "            print(f\"  Loading {curriculum_key} data...\")\n",
    "\n",
    "            csv_dir = Path(f\"./csv_logs_{task}_{curriculum_key}\")\n",
    "\n",
    "            # Load different types of training data - look for files with timestamps\n",
    "            import glob\n",
    "\n",
    "            curriculum_data = {}\n",
    "\n",
    "            # Find the most recent file for each metric type\n",
    "            patterns = {\n",
    "                'training': 'training_metrics_*.csv',\n",
    "                'validation': 'validation_metrics_*.csv',\n",
    "                'energy': 'energy_metrics_*.csv',\n",
    "                'curriculum': 'curriculum_metrics_*.csv',\n",
    "                'robustness': 'robustness_metrics_*.csv'\n",
    "            }\n",
    "\n",
    "            for key, pattern in patterns.items():\n",
    "                files = glob.glob(str(csv_dir / pattern))\n",
    "                if files:\n",
    "                    # Get most recent file\n",
    "                    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime if Path(x).exists() else 0)\n",
    "                    curriculum_data[key] = load_csv_data(Path(latest_file))\n",
    "                else:\n",
    "                    curriculum_data[key] = None\n",
    "\n",
    "            # Count available data types\n",
    "            available = sum(1 for df in curriculum_data.values() if df is not None)\n",
    "            print(f\"    Found {available}/5 data files for {curriculum_key}\")\n",
    "\n",
    "            task_results[curriculum_key] = curriculum_data\n",
    "\n",
    "        results[task] = task_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_curriculum_data(all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]]) -> pd.DataFrame:\n",
    "    \"\"\"Extract and standardize key metrics from all curriculum results across all tasks.\"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for task, task_results in all_results.items():\n",
    "        for curriculum_key, data in task_results.items():\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "\n",
    "            # Extract key metrics\n",
    "            training_df = data.get('training')\n",
    "            validation_df = data.get('validation')\n",
    "            energy_df = data.get('energy')\n",
    "\n",
    "            # Extract task-specific metrics from validation data\n",
    "            val_accuracy = 0.0  # For inverse task\n",
    "            val_identity_error = float('inf')  # For inverse task\n",
    "            val_mse = float('inf')  # General metric\n",
    "            val_mae = float('inf')  # For addition/lowrank tasks\n",
    "\n",
    "            if validation_df is not None:\n",
    "                # Look for accuracy metric (for inverse task)\n",
    "                accuracy_rows = validation_df[validation_df['metric_name'] == 'accuracy']\n",
    "                if not accuracy_rows.empty:\n",
    "                    val_accuracy = accuracy_rows['metric_value'].iloc[-1]  # Get last value (fraction)\n",
    "\n",
    "                # Look for identity_error metric (for inverse task)\n",
    "                identity_rows = validation_df[validation_df['metric_name'] == 'identity_error']\n",
    "                if not identity_rows.empty:\n",
    "                    val_identity_error = identity_rows['metric_value'].iloc[-1]\n",
    "\n",
    "                # Look for MSE metric\n",
    "                mse_rows = validation_df[validation_df['metric_name'] == 'mse']\n",
    "                if not mse_rows.empty:\n",
    "                    val_mse = mse_rows['metric_value'].iloc[-1]\n",
    "\n",
    "                # Look for MAE metric (for addition/lowrank)\n",
    "                mae_rows = validation_df[validation_df['metric_name'] == 'mae']\n",
    "                if not mae_rows.empty:\n",
    "                    val_mae = mae_rows['metric_value'].iloc[-1]\n",
    "\n",
    "            metrics = {\n",
    "                'task': task,\n",
    "                'curriculum': curriculum_key,\n",
    "                'name': config['name'],\n",
    "                'color': config['color'],\n",
    "\n",
    "                # Training metrics\n",
    "                'final_total_loss': safe_get_final_value(training_df, 'total_loss'),\n",
    "                'final_energy_loss': safe_get_final_value(training_df, 'loss_energy'),\n",
    "                'final_denoise_loss': safe_get_final_value(training_df, 'loss_denoise'),\n",
    "                'best_total_loss': safe_get_best_value(training_df, 'total_loss', minimize=True),\n",
    "                'avg_training_time': training_df['nn_time'].mean() if training_df is not None and 'nn_time' in training_df.columns else 0.0,\n",
    "\n",
    "                # Validation metrics\n",
    "                'final_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "                'best_val_accuracy': val_accuracy * 100,  # Convert fraction to percentage for display\n",
    "                'final_identity_error': val_identity_error,\n",
    "                'final_val_mse': val_mse,\n",
    "                'final_val_mae': val_mae,\n",
    "\n",
    "                # Energy metrics\n",
    "                'final_energy_margin': safe_get_final_value(energy_df, 'energy_margin'),\n",
    "                'max_curriculum_weight': safe_get_best_value(energy_df, 'curriculum_weight', minimize=False) if energy_df is not None and 'curriculum_weight' in energy_df.columns else 0.0,\n",
    "            }\n",
    "\n",
    "            processed_data.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Load all results\n",
    "print(\"Loading all curriculum results for all tasks...\")\n",
    "all_results = load_all_curriculum_results()\n",
    "\n",
    "# Process data\n",
    "print(\"\\nProcessing curriculum data...\")\n",
    "summary_df = process_curriculum_data(all_results)\n",
    "\n",
    "print(f\"\\nLoaded data for {len(summary_df)} configurations across {len(TASKS)} tasks\")\n",
    "\n",
    "# Display summary by task\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    print(f\"\\n{task.upper()} Task Summary:\")\n",
    "    display_cols = ['name', 'final_total_loss', 'final_val_mse']\n",
    "    if task == 'inverse':\n",
    "        display_cols.append('final_identity_error')\n",
    "    elif task in ['addition', 'lowrank']:\n",
    "        display_cols.append('final_val_mae')\n",
    "\n",
    "    display(task_data[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5mjst8GdD4e"
   },
   "source": [
    "## IRED Paper-Style Results Reporting\n",
    "\n",
    "This section generates results tables in the format used by the IRED paper, focusing on:\n",
    "- **Mean Squared Error (MSE)** as the primary metric for all matrix tasks\n",
    "- **Identity Error** for matrix inverse task (||Pred @ Input - I||²)\n",
    "- **Task-specific metrics** for comprehensive evaluation\n",
    "- **Consolidated performance tables** across all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaSJfbqpdD4f",
    "outputId": "929b458a-d5ad-4b78-87ee-66a4eb9afc0e"
   },
   "outputs": [],
   "source": [
    "## IRED-Style Results Table Generation\n",
    "\n",
    "def generate_ired_style_table(summary_df: pd.DataFrame, task: str):\n",
    "    \"\"\"Generate IRED paper-style results table for a specific task.\"\"\"\n",
    "    import pandas as pd\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    task_data = summary_df[summary_df['task'] == task].copy()\n",
    "\n",
    "    if len(task_data) == 0:\n",
    "        print(f\"No data available for {task} task\")\n",
    "        return\n",
    "\n",
    "    # Sort by final MSE to rank methods\n",
    "    task_data = task_data.sort_values('final_val_mse')\n",
    "\n",
    "    # Create IRED-style table\n",
    "    table_data = []\n",
    "    for _, row in task_data.iterrows():\n",
    "        method_name = row['name']\n",
    "\n",
    "        # Main metric: MSE (as reported in IRED paper)\n",
    "        mse = row['final_val_mse'] if row['final_val_mse'] != float('inf') else 'N/A'\n",
    "\n",
    "        # Task-specific additional metrics\n",
    "        if task == 'inverse':\n",
    "            identity_error = row['final_identity_error'] if row['final_identity_error'] != float('inf') else 'N/A'\n",
    "            if isinstance(mse, float) and isinstance(identity_error, float):\n",
    "                table_data.append([method_name, f\"{mse:.4f}\", f\"{identity_error:.4f}\"])\n",
    "            else:\n",
    "                table_data.append([method_name, str(mse), str(identity_error)])\n",
    "        else:\n",
    "            if isinstance(mse, float):\n",
    "                table_data.append([method_name, f\"{mse:.4f}\"])\n",
    "            else:\n",
    "                table_data.append([method_name, str(mse)])\n",
    "\n",
    "    # Create table headers based on task\n",
    "    if task == 'inverse':\n",
    "        headers = ['Method', 'MSE', 'Identity Error']\n",
    "    else:\n",
    "        headers = ['Method', 'MSE']\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"IRED-Style Results Table: {task.upper()} Task\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "    # Add interpretation\n",
    "    if len(task_data) > 0:\n",
    "        best_method = task_data.iloc[0]\n",
    "        print(f\"\\n✓ Best performing method: {best_method['name']}\")\n",
    "        if isinstance(best_method['final_val_mse'], float):\n",
    "            print(f\"  MSE: {best_method['final_val_mse']:.4f}\")\n",
    "        if task == 'inverse' and isinstance(best_method['final_identity_error'], float):\n",
    "            print(f\"  Identity Error: {best_method['final_identity_error']:.4f}\")\n",
    "\n",
    "def generate_consolidated_ired_table(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate consolidated IRED-style table for all matrix tasks.\"\"\"\n",
    "    from tabulate import tabulate\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IRED-STYLE CONSOLIDATED RESULTS TABLE\")\n",
    "    print(\"Matrix Operations Performance (MSE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Prepare data for each task\n",
    "    methods = summary_df['name'].unique()\n",
    "\n",
    "    # Create consolidated table\n",
    "    table_data = []\n",
    "    for method in methods:\n",
    "        method_data = summary_df[summary_df['name'] == method]\n",
    "\n",
    "        row = [method]\n",
    "\n",
    "        # Add MSE for each task\n",
    "        for task in ['addition', 'lowrank', 'inverse']:\n",
    "            task_row = method_data[method_data['task'] == task]\n",
    "            if len(task_row) > 0:\n",
    "                mse = task_row['final_val_mse'].iloc[0]\n",
    "                if mse != float('inf'):\n",
    "                    row.append(f\"{mse:.4f}\")\n",
    "                else:\n",
    "                    row.append(\"N/A\")\n",
    "            else:\n",
    "                row.append(\"-\")\n",
    "\n",
    "        # Add average MSE\n",
    "        mse_values = []\n",
    "        for task in ['addition', 'lowrank', 'inverse']:\n",
    "            task_row = method_data[method_data['task'] == task]\n",
    "            if len(task_row) > 0:\n",
    "                mse = task_row['final_val_mse'].iloc[0]\n",
    "                if mse != float('inf'):\n",
    "                    mse_values.append(mse)\n",
    "\n",
    "        if mse_values:\n",
    "            avg_mse = np.mean(mse_values)\n",
    "            row.append(f\"{avg_mse:.4f}\")\n",
    "        else:\n",
    "            row.append(\"N/A\")\n",
    "\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Sort by average MSE\n",
    "    table_data.sort(key=lambda x: float(x[-1]) if x[-1] not in [\"N/A\", \"-\"] else float('inf'))\n",
    "\n",
    "    headers = ['Method', 'Addition', 'Matrix Completion', 'Matrix Inverse', 'Average']\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "    # Best method summary\n",
    "    if table_data:\n",
    "        print(f\"\\n✓ Best overall method: {table_data[0][0]}\")\n",
    "        print(f\"  Average MSE: {table_data[0][-1]}\")\n",
    "\n",
    "def generate_task_specific_metrics_table(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate detailed metrics table for each task.\"\"\"\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task].copy()\n",
    "\n",
    "        if len(task_data) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Detailed Metrics Table: {task.upper()} Task\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Sort by MSE\n",
    "        task_data = task_data.sort_values('final_val_mse')\n",
    "\n",
    "        # Prepare table data\n",
    "        table_data = []\n",
    "        for _, row in task_data.iterrows():\n",
    "            row_data = [row['name']]\n",
    "\n",
    "            # Add MSE\n",
    "            mse = row['final_val_mse']\n",
    "            row_data.append(f\"{mse:.4f}\" if mse != float('inf') else \"N/A\")\n",
    "\n",
    "            # Add task-specific metrics\n",
    "            if task == 'inverse':\n",
    "                identity_error = row['final_identity_error']\n",
    "                accuracy = row['final_val_accuracy']\n",
    "                row_data.append(f\"{identity_error:.4f}\" if identity_error != float('inf') else \"N/A\")\n",
    "                row_data.append(f\"{accuracy:.1f}%\" if accuracy > 0 else \"N/A\")\n",
    "\n",
    "            # Add training loss\n",
    "            row_data.append(f\"{row['final_total_loss']:.4f}\")\n",
    "\n",
    "            # Add best loss achieved\n",
    "            row_data.append(f\"{row['best_total_loss']:.4f}\")\n",
    "\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        # Define headers based on task\n",
    "        if task == 'inverse':\n",
    "            headers = ['Method', 'MSE', 'Identity Error', 'Accuracy', 'Final Loss', 'Best Loss']\n",
    "        else:\n",
    "            headers = ['Method', 'MSE', 'Final Loss', 'Best Loss']\n",
    "\n",
    "        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "        # Summary statistics\n",
    "        valid_mse = task_data[task_data['final_val_mse'] != float('inf')]['final_val_mse']\n",
    "        if len(valid_mse) > 0:\n",
    "            print(f\"\\nStatistics:\")\n",
    "            print(f\"  MSE Range: {valid_mse.min():.4f} - {valid_mse.max():.4f}\")\n",
    "            print(f\"  MSE Mean: {valid_mse.mean():.4f}\")\n",
    "            print(f\"  MSE Std: {valid_mse.std():.4f}\")\n",
    "\n",
    "# Generate all IRED-style tables\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IRED PAPER-STYLE RESULTS REPORTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Individual task tables\n",
    "for task in TASKS:\n",
    "    generate_ired_style_table(summary_df, task)\n",
    "\n",
    "# Consolidated table\n",
    "generate_consolidated_ired_table(summary_df)\n",
    "\n",
    "# Detailed metrics\n",
    "generate_task_specific_metrics_table(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_constraint_aware_performance(summary_df: pd.DataFrame):\n",
    "    \"\"\"Analyze the specific impact of constraint-aware corruption on low-rank task.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONSTRAINT-AWARE CORRUPTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Focus on lowrank task\n",
    "    lowrank_data = summary_df[summary_df['task'] == 'lowrank'].copy()\n",
    "    lowrank_data = lowrank_data.sort_values('final_val_mse')\n",
    "    \n",
    "    print(\"\\nLow-Rank Matrix Completion Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for _, row in lowrank_data.iterrows():\n",
    "        comparison_data.append({\n",
    "            'Method': row['name'],\n",
    "            'MSE': row['final_val_mse'],\n",
    "            'Final Loss': row['final_total_loss'],\n",
    "            'Improvement': '',\n",
    "        })\n",
    "    \n",
    "    # Calculate improvements\n",
    "    baseline_mse = lowrank_data[lowrank_data['name'] == 'Baseline']['final_val_mse'].values\n",
    "    if len(baseline_mse) > 0:\n",
    "        baseline_mse = baseline_mse[0]\n",
    "        for item in comparison_data:\n",
    "            if item['Method'] != 'Baseline':\n",
    "                improvement = ((baseline_mse - item['MSE']) / baseline_mse) * 100\n",
    "                if improvement > 0:\n",
    "                    item['Improvement'] = f\"↑ {improvement:.1f}%\"\n",
    "                else:\n",
    "                    item['Improvement'] = f\"↓ {abs(improvement):.1f}%\"\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(tabulate(comparison_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Check if constraint-aware configuration exists\n",
    "    constraint_aware_results = lowrank_data[lowrank_data['name'].str.contains('Constraint-Aware')]\n",
    "    aggressive_results = lowrank_data[lowrank_data['name'] == 'Aggressive Curriculum']\n",
    "    \n",
    "    if not constraint_aware_results.empty and not aggressive_results.empty:\n",
    "        print(\"\\n🔍 KEY FINDINGS:\")\n",
    "        \n",
    "        constraint_mse = constraint_aware_results['final_val_mse'].iloc[0]\n",
    "        aggressive_mse = aggressive_results['final_val_mse'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n1. Standard Aggressive Curriculum:\")\n",
    "        print(f\"   MSE: {aggressive_mse:.4f}\")\n",
    "        if aggressive_mse > 0.8:\n",
    "            print(f\"   ⚠️ CATASTROPHIC FAILURE - Performance degraded significantly!\")\n",
    "        \n",
    "        print(f\"\\n2. Constraint-Aware Aggressive Curriculum:\")\n",
    "        print(f\"   MSE: {constraint_mse:.4f}\")\n",
    "        \n",
    "        improvement = ((aggressive_mse - constraint_mse) / aggressive_mse) * 100\n",
    "        print(f\"\\n3. Impact of Constraint Preservation:\")\n",
    "        print(f\"   Improvement over standard aggressive: {improvement:.1f}%\")\n",
    "        \n",
    "        if constraint_mse < 0.5 and aggressive_mse > 0.8:\n",
    "            print(f\"   ✅ SUCCESS: Constraint-aware corruption fixed the catastrophic failure!\")\n",
    "            print(f\"   The factorized parameterization preserved rank constraints during adversarial training.\")\n",
    "    \n",
    "    # Theoretical analysis\n",
    "    print(\"\\n📊 THEORETICAL EXPLANATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Standard adversarial corruption in unconstrained space:\")\n",
    "    print(\"  • Generates full-rank matrices (rank ≈ 20) from rank-10 targets\")\n",
    "    print(\"  • Model learns to detect off-manifold samples instead of task quality\")\n",
    "    print(\"  • Result: MSE increases from ~0.4 to ~0.85+\")\n",
    "    print(\"\\nConstraint-aware corruption with factorized parameterization:\")\n",
    "    print(\"  • X = U·V^T where U, V ∈ ℝ^(20×10) → guarantees rank ≤ 10\")\n",
    "    print(\"  • Adversarial optimization in factor space preserves constraint\")\n",
    "    print(\"  • Model learns actual task difficulty, not constraint violations\")\n",
    "    print(\"  • Result: MSE should remain ~0.4 or improve\")\n",
    "    \n",
    "    return lowrank_data\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Analyzing constraint-aware corruption impact...\")\n",
    "lowrank_analysis = analyze_constraint_aware_performance(summary_df)\n",
    "\n",
    "# Create a specific visualization for constraint-aware vs standard\n",
    "def plot_constraint_comparison(lowrank_data):\n",
    "    \"\"\"Create a focused comparison plot.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # MSE Comparison\n",
    "    methods = lowrank_data['name'].tolist()\n",
    "    mse_values = lowrank_data['final_val_mse'].tolist()\n",
    "    colors = lowrank_data['color'].tolist()\n",
    "    \n",
    "    bars1 = ax1.bar(methods, mse_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('Low-Rank Task: MSE Comparison', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Mean Squared Error (Lower is Better)')\n",
    "    ax1.set_ylim(0, max(mse_values) * 1.1)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, mse_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add threshold line for \"good\" performance\n",
    "    ax1.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Acceptable MSE')\n",
    "    ax1.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Failure Threshold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Loss Comparison\n",
    "    loss_values = lowrank_data['final_total_loss'].tolist()\n",
    "    bars2 = ax2.bar(methods, loss_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Low-Rank Task: Training Loss', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('Final Total Loss')\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars2, loss_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Constraint-Aware Corruption Impact on Low-Rank Matrix Completion', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the comparison plot\n",
    "plot_constraint_comparison(lowrank_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint-Aware Corruption Analysis\n",
    "\n",
    "This section specifically analyzes the impact of constraint-aware adversarial corruption on the low-rank matrix completion task, which was previously failing with standard adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6NnERK8SdD4f",
    "outputId": "3952d8b0-b0be-422b-9b8b-453ac4058c88"
   },
   "outputs": [],
   "source": [
    "def visualize_task_curriculum_comparison(all_results: Dict, summary_df: pd.DataFrame, task: str):\n",
    "    \"\"\"Create comprehensive side-by-side curriculum comparison visualizations for a specific task.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Multi-Curriculum Training Comparison - {task.upper()} Task', fontsize=16, fontweight='bold')\n",
    "\n",
    "    task_results = all_results.get(task, {})\n",
    "    task_summary = summary_df[summary_df['task'] == task]\n",
    "\n",
    "    # 1. Total Loss Comparison\n",
    "    ax = axes[0, 0]\n",
    "    for curriculum_key, data in task_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'total_loss' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['total_loss'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Total Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Energy Loss Comparison\n",
    "    ax = axes[0, 1]\n",
    "    for curriculum_key, data in task_results.items():\n",
    "        training_df = data.get('training')\n",
    "        if training_df is not None and 'loss_energy' in training_df.columns:\n",
    "            config = CURRICULUM_CONFIGS[curriculum_key]\n",
    "            ax.plot(training_df['step'], training_df['loss_energy'],\n",
    "                    color=config['color'], label=config['name'], linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title('Energy Loss Curves', fontweight='bold')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Energy Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Prepare colors for bar charts\n",
    "    colors = task_summary['color'].tolist()\n",
    "\n",
    "    # 3. Task-specific metric comparison\n",
    "    ax = axes[1, 0]\n",
    "    curricula = task_summary['name'].tolist()\n",
    "\n",
    "    if task == 'inverse':\n",
    "        # Identity Error for inverse task\n",
    "        identity_errors = task_summary['final_identity_error'].tolist()\n",
    "        valid_errors = [(c, e, col) for c, e, col in zip(curricula, identity_errors, colors)\n",
    "                        if e != float('inf')]\n",
    "\n",
    "        if valid_errors:\n",
    "            names, errors, cols = zip(*valid_errors)\n",
    "            bars = ax.bar(names, errors, color=cols, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title('Identity Error (Lower = Better)', fontweight='bold')\n",
    "            ax.set_ylabel('||Pred @ Input - I||²')\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "            # Add value labels\n",
    "            for bar, error in zip(bars, errors):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                        f'{error:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        # MAE for addition/lowrank tasks\n",
    "        mae_values = task_summary['final_val_mae'].tolist()\n",
    "        valid_mae = [(c, m, col) for c, m, col in zip(curricula, mae_values, colors)\n",
    "                     if m != float('inf')]\n",
    "\n",
    "        if valid_mae:\n",
    "            names, maes, cols = zip(*valid_mae)\n",
    "            bars = ax.bar(names, maes, color=cols, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title('Mean Absolute Error (Lower = Better)', fontweight='bold')\n",
    "            ax.set_ylabel('MAE')\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "            # Add value labels\n",
    "            for bar, mae in zip(bars, maes):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                        f'{mae:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 4. MSE Comparison (common metric)\n",
    "    ax = axes[1, 1]\n",
    "    mse_values = task_summary['final_val_mse'].tolist()\n",
    "\n",
    "    # Filter out inf values\n",
    "    valid_mse = [(c, m, col) for c, m, col in zip(curricula, mse_values, colors)\n",
    "                 if m != float('inf')]\n",
    "\n",
    "    if valid_mse:\n",
    "        names, mses, cols = zip(*valid_mse)\n",
    "        bars = ax.bar(names, mses, color=cols, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Mean Squared Error', fontweight='bold')\n",
    "        ax.set_ylabel('MSE')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, mse in zip(bars, mses):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                    f'{mse:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No validation data available\\n(Run longer training for validation metrics)',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('MSE (N/A)', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_cross_task_comparison(summary_df: pd.DataFrame):\n",
    "    \"\"\"Compare curriculum performance across different tasks.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Cross-Task Curriculum Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    tasks = summary_df['task'].unique()\n",
    "\n",
    "    # Plot for each task\n",
    "    for idx, task in enumerate(tasks):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        task_data = summary_df[summary_df['task'] == task]\n",
    "\n",
    "        # Create grouped bar chart for final losses\n",
    "        x = np.arange(len(task_data))\n",
    "        width = 0.35\n",
    "\n",
    "        bars1 = ax.bar(x - width/2, task_data['final_total_loss'], width,\n",
    "                      label='Total Loss', alpha=0.7)\n",
    "        bars2 = ax.bar(x + width/2, task_data['final_val_mse'], width,\n",
    "                      label='MSE', alpha=0.7)\n",
    "\n",
    "        ax.set_title(f'{task.upper()} Task Performance', fontweight='bold')\n",
    "        ax.set_ylabel('Loss/Error')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(task_data['name'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for idx in range(len(tasks), 6):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        fig.delaxes(axes[row, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_task_performance_heatmap(summary_df: pd.DataFrame):\n",
    "    \"\"\"Create a heatmap showing curriculum performance across tasks.\"\"\"\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Pivot data for heatmap\n",
    "    pivot_loss = summary_df.pivot(index='name', columns='task', values='final_total_loss')\n",
    "    pivot_mse = summary_df.pivot(index='name', columns='task', values='final_val_mse')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Total Loss heatmap\n",
    "    sns.heatmap(pivot_loss, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax1, cbar_kws={'label': 'Total Loss'})\n",
    "    ax1.set_title('Total Loss by Curriculum and Task', fontweight='bold')\n",
    "    ax1.set_xlabel('Task')\n",
    "    ax1.set_ylabel('Curriculum')\n",
    "\n",
    "    # MSE heatmap\n",
    "    # Replace inf values with NaN for better visualization\n",
    "    pivot_mse_clean = pivot_mse.replace([float('inf')], np.nan)\n",
    "    sns.heatmap(pivot_mse_clean, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax2, cbar_kws={'label': 'MSE'})\n",
    "    ax2.set_title('MSE by Curriculum and Task', fontweight='bold')\n",
    "    ax2.set_xlabel('Task')\n",
    "    ax2.set_ylabel('Curriculum')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations for each task\n",
    "print(\"Generating task-specific curriculum comparison visualizations...\")\n",
    "for task in TASKS:\n",
    "    print(f\"\\nVisualizing {task.upper()} task...\")\n",
    "    visualize_task_curriculum_comparison(all_results, summary_df, task)\n",
    "\n",
    "print(\"\\nGenerating cross-task comparison...\")\n",
    "visualize_cross_task_comparison(summary_df)\n",
    "\n",
    "print(\"\\nGenerating performance heatmap...\")\n",
    "create_task_performance_heatmap(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TybLskZrdD4g",
    "outputId": "bb4a6ba1-97a0-4014-e44c-263228ea2701"
   },
   "outputs": [],
   "source": [
    "def generate_task_ranking(summary_df: pd.DataFrame):\n",
    "    \"\"\"Generate comprehensive ranking for each task and overall.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-TASK CURRICULUM RANKING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Rank curricula for each task\n",
    "    for task in TASKS:\n",
    "        task_data = summary_df[summary_df['task'] == task].copy()\n",
    "\n",
    "        print(f\"\\n📊 {task.upper()} TASK RANKING\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Create a scoring system\n",
    "        task_data['score'] = 0\n",
    "\n",
    "        # Score based on final loss (lower is better)\n",
    "        loss_rank = task_data['final_total_loss'].rank()\n",
    "        task_data['score'] += (len(task_data) - loss_rank + 1) * 25\n",
    "\n",
    "        # Score based on MSE (lower is better)\n",
    "        mse_valid = task_data['final_val_mse'] != float('inf')\n",
    "        if mse_valid.any():\n",
    "            mse_rank = task_data.loc[mse_valid, 'final_val_mse'].rank()\n",
    "            task_data.loc[mse_valid, 'score'] += (len(mse_rank) - mse_rank + 1) * 25\n",
    "\n",
    "        # Task-specific scoring\n",
    "        if task == 'inverse':\n",
    "            # Score based on identity error\n",
    "            id_valid = task_data['final_identity_error'] != float('inf')\n",
    "            if id_valid.any():\n",
    "                id_rank = task_data.loc[id_valid, 'final_identity_error'].rank()\n",
    "                task_data.loc[id_valid, 'score'] += (len(id_rank) - id_rank + 1) * 25\n",
    "        elif task in ['addition', 'lowrank']:\n",
    "            # Score based on MAE\n",
    "            mae_valid = task_data['final_val_mae'] != float('inf')\n",
    "            if mae_valid.any():\n",
    "                mae_rank = task_data.loc[mae_valid, 'final_val_mae'].rank()\n",
    "                task_data.loc[mae_valid, 'score'] += (len(mae_rank) - mae_rank + 1) * 25\n",
    "\n",
    "        # Score based on training efficiency\n",
    "        time_rank = task_data['avg_training_time'].rank()\n",
    "        task_data['score'] += (len(task_data) - time_rank + 1) * 25\n",
    "\n",
    "        # Normalize scores to 0-100\n",
    "        max_score = task_data['score'].max()\n",
    "        if max_score > 0:\n",
    "            task_data['normalized_score'] = (task_data['score'] / max_score) * 100\n",
    "        else:\n",
    "            task_data['normalized_score'] = 0\n",
    "\n",
    "        # Sort by score\n",
    "        task_data = task_data.sort_values('normalized_score', ascending=False)\n",
    "\n",
    "        # Display ranking\n",
    "        rank_table = task_data[['name', 'normalized_score', 'final_total_loss', 'final_val_mse']].copy()\n",
    "        rank_table['rank'] = range(1, len(rank_table) + 1)\n",
    "        rank_table = rank_table[['rank', 'name', 'normalized_score', 'final_total_loss', 'final_val_mse']]\n",
    "        rank_table.columns = ['Rank', 'Curriculum', 'Score', 'Final Loss', 'MSE']\n",
    "\n",
    "        print(rank_table.round(2).to_string(index=False))\n",
    "\n",
    "        # Winner for this task\n",
    "        winner = task_data.iloc[0]\n",
    "        print(f\"\\n🏆 Winner for {task.upper()}: {winner['name']} (Score: {winner['normalized_score']:.1f})\")\n",
    "\n",
    "        # Check if curriculum beats baseline\n",
    "        baseline_score = task_data[task_data['curriculum'] == 'baseline']['normalized_score'].values\n",
    "        if len(baseline_score) > 0 and winner['curriculum'] != 'baseline':\n",
    "            improvement = winner['normalized_score'] - baseline_score[0]\n",
    "            print(f\"   Improvement over baseline: +{improvement:.1f} points\")\n",
    "\n",
    "    # Overall ranking across all tasks\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 OVERALL CROSS-TASK RANKING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Calculate average score across tasks for each curriculum\n",
    "    overall_scores = []\n",
    "    for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "        curriculum_data = summary_df[summary_df['curriculum'] == curriculum_key]\n",
    "\n",
    "        if len(curriculum_data) > 0:\n",
    "            # Calculate average performance metrics\n",
    "            avg_loss = curriculum_data['final_total_loss'].mean()\n",
    "            avg_mse = curriculum_data[curriculum_data['final_val_mse'] != float('inf')]['final_val_mse'].mean()\n",
    "            avg_time = curriculum_data['avg_training_time'].mean()\n",
    "\n",
    "            overall_scores.append({\n",
    "                'curriculum': curriculum_key,\n",
    "                'name': CURRICULUM_CONFIGS[curriculum_key]['name'],\n",
    "                'avg_loss': avg_loss,\n",
    "                'avg_mse': avg_mse if not np.isnan(avg_mse) else float('inf'),\n",
    "                'avg_time': avg_time,\n",
    "                'num_tasks': len(curriculum_data)\n",
    "            })\n",
    "\n",
    "    overall_df = pd.DataFrame(overall_scores)\n",
    "\n",
    "    # Rank based on average loss\n",
    "    overall_df['rank'] = overall_df['avg_loss'].rank().astype(int)\n",
    "    overall_df = overall_df.sort_values('rank')\n",
    "\n",
    "    print(\"\\nOverall Performance Across All Tasks:\")\n",
    "    display_df = overall_df[['rank', 'name', 'avg_loss', 'avg_mse', 'avg_time']].copy()\n",
    "    display_df.columns = ['Rank', 'Curriculum', 'Avg Loss', 'Avg MSE', 'Avg Time']\n",
    "    print(display_df.round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\n🎯 KEY FINDINGS:\")\n",
    "    overall_winner = overall_df.iloc[0]\n",
    "    print(f\"   • Best Overall: {overall_winner['name']}\")\n",
    "    print(f\"   • Average Loss: {overall_winner['avg_loss']:.4f}\")\n",
    "\n",
    "    # Check curriculum vs baseline\n",
    "    baseline_overall = overall_df[overall_df['curriculum'] == 'baseline']\n",
    "    if len(baseline_overall) > 0:\n",
    "        baseline_loss = baseline_overall['avg_loss'].iloc[0]\n",
    "        if overall_winner['curriculum'] != 'baseline':\n",
    "            improvement = (baseline_loss - overall_winner['avg_loss']) / baseline_loss * 100\n",
    "            print(f\"   • Improvement over baseline: {improvement:.1f}%\")\n",
    "        else:\n",
    "            print(\"   • Baseline performed best overall\")\n",
    "\n",
    "    return overall_df\n",
    "\n",
    "# Generate comprehensive ranking\n",
    "print(\"Generating multi-task ranking analysis...\")\n",
    "overall_ranking = generate_task_ranking(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqZlmO-fdD4g"
   },
   "source": [
    "## Multi-Task Summary Analysis and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAwUsIyTdD4g",
    "outputId": "a21914b2-54ac-487a-efb3-905b9218aeae"
   },
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "output_dir = Path('./multi_task_curriculum_results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary data\n",
    "summary_df.to_csv(output_dir / 'multi_task_curriculum_summary.csv', index=False)\n",
    "overall_ranking.to_csv(output_dir / 'overall_curriculum_ranking.csv', index=False)\n",
    "\n",
    "# Create task-specific summaries\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    task_data.to_csv(output_dir / f'{task}_curriculum_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-TASK CURRICULUM TRAINING - FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📁 Results saved to: {output_dir}\")\n",
    "print(\"   • multi_task_curriculum_summary.csv - All results across tasks\")\n",
    "print(\"   • overall_curriculum_ranking.csv - Overall curriculum rankings\")\n",
    "for task in TASKS:\n",
    "    print(f\"   • {task}_curriculum_summary.csv - {task.capitalize()} task specific results\")\n",
    "\n",
    "# Training statistics\n",
    "total_configs = len(CURRICULUM_CONFIGS) * len(TASKS)\n",
    "successful_runs = sum(1 for _, task_results in training_results.items()\n",
    "                     for result in task_results.values() if result == 0)\n",
    "\n",
    "print(f\"\\n📊 TRAINING STATISTICS:\")\n",
    "print(f\"   • Total configurations tested: {total_configs}\")\n",
    "print(f\"   • Successful runs: {successful_runs}/{total_configs}\")\n",
    "print(f\"   • Tasks evaluated: {', '.join(TASKS)}\")\n",
    "print(f\"   • Curricula tested: {', '.join([cfg['name'] for cfg in CURRICULUM_CONFIGS.values()])}\")\n",
    "\n",
    "print(\"\\n🏆 EXECUTIVE SUMMARY:\")\n",
    "print(f\"   • Best Overall Curriculum: {overall_ranking.iloc[0]['name']}\")\n",
    "print(f\"   • Training steps per run: {COMMON_ARGS['train_num_steps']}\")\n",
    "print(f\"   • CSV logging interval: {COMMON_ARGS['csv_log_interval']} steps\")\n",
    "\n",
    "# Task-specific winners\n",
    "print(\"\\n🎯 TASK-SPECIFIC WINNERS:\")\n",
    "for task in TASKS:\n",
    "    task_data = summary_df[summary_df['task'] == task]\n",
    "    if len(task_data) > 0:\n",
    "        # Find best by loss\n",
    "        best_idx = task_data['final_total_loss'].idxmin()\n",
    "        winner = task_data.loc[best_idx]\n",
    "        print(f\"   • {task.upper()}: {winner['name']} (Loss: {winner['final_total_loss']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 MULTI-TASK MULTI-CURRICULUM SMOKE TEST COMPLETED! 🚀\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nUse the generated CSV files and visualizations to:\")\n",
    "print(\"1. Select the best curriculum approach for each task\")\n",
    "print(\"2. Compare baseline vs curriculum learning effectiveness\")\n",
    "print(\"3. Understand trade-offs between different curriculum strategies\")\n",
    "print(\"4. Make informed decisions for production training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation (20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION AT STEP 50K SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# IRED Paper Table 1 Reference Values\n",
    "IRED_PAPER_RESULTS = {\n",
    "    'addition': {\n",
    "        'same_difficulty': 0.0002,\n",
    "        'harder_difficulty': 0.0020\n",
    "    },\n",
    "    'lowrank': {  # Matrix Completion\n",
    "        'same_difficulty': 0.0174,\n",
    "        'harder_difficulty': 0.2054\n",
    "    },\n",
    "    'inverse': {\n",
    "        'same_difficulty': 0.0095,\n",
    "        'harder_difficulty': 0.2063\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluation configuration\n",
    "EVAL_CONFIG = {\n",
    "    'milestone': '50',  # Step 50k checkpoint\n",
    "    'num_eval_samples': 2000,  # Match paper's 20k test samples\n",
    "    'batch_size': 2048,\n",
    "    'rank': 20,\n",
    "    'ood': False,  # Set to True for \"harder difficulty\" evaluation\n",
    "    'num_opt_steps': 40,  # ADD THIS - match paper's reported results\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Checkpoint milestone: {EVAL_CONFIG['milestone']} (step ~50,000)\")\n",
    "print(f\"Number of evaluation samples: {EVAL_CONFIG['num_eval_samples']}\")\n",
    "print(f\"Evaluation batch size: {EVAL_CONFIG['batch_size']}\")\n",
    "print(f\"Matrix rank: {EVAL_CONFIG['rank']}\")\n",
    "print(f\"OOD (Harder difficulty): {EVAL_CONFIG['ood']}\")\n",
    "\n",
    "num_samples_evaluated = 0  # Initialize at start\n",
    "NUM_LANDSCAPES = 10  # From IRED paper (K=10)\n",
    "OPT_STEPS_PER_LANDSCAPE = 40  # From IRED paper Table 2t_grad = model(inp, lab\n",
    "STEP_SIZES = [0.1] * NUM_LANDSCAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "\n",
    "\n",
    "def evaluate_at_50k(task: str, curriculum_key: str, ood: bool = False, num_samples: int = 20000) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained IRED model at 50k checkpoint using train.py --evaluate.\n",
    "    \n",
    "    This is a lightweight wrapper around the built-in evaluation functionality,\n",
    "    making it easy to batch evaluate and collect results programmatically.\n",
    "    \n",
    "    Args:\n",
    "        task: str - One of ['inverse', 'addition', 'lowrank']\n",
    "        curriculum_key: str - Curriculum configuration key (e.g., 'baseline', 'aggressive')\n",
    "        ood: bool - If True, evaluate on harder difficulty (OOD)\n",
    "        num_samples: int - Number of samples (note: train.py uses full validation set)\n",
    "    \n",
    "    Returns:\n",
    "        dict - Evaluation results with metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 EVALUATING: {task.upper()} | {CURRICULUM_CONFIGS[curriculum_key]['name']} | \"\n",
    "          f\"{'HARDER (OOD)' if ood else 'SAME'} DIFFICULTY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Determine checkpoint path\n",
    "    checkpoint_path = Path(f'results/ds_{task}/model_mlp_diffsteps_10/model-50.pt')\n",
    "    # checkpoint_path = Path(f'results/ds_{task}/model_mlp_diffsteps_10/model-1.pt')\n",
    "\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"❌ ERROR: Checkpoint not found at {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📂 Checkpoint: {checkpoint_path}\")\n",
    "    print(f\"🎯 OOD (harder difficulty): {ood}\")\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        'python', 'train.py',\n",
    "        '--dataset', task,\n",
    "        '--model', 'mlp',\n",
    "        '--rank', '20',\n",
    "        '--batch_size', '2048',\n",
    "        '--diffusion_steps', '10',\n",
    "        '--load-milestone', str(checkpoint_path),\n",
    "        '--evaluate',\n",
    "        '--supervise-energy-landscape', 'True'\n",
    "    ]\n",
    "    \n",
    "    # Add OOD flag if needed\n",
    "    if ood:\n",
    "        cmd.extend(['--ood'])\n",
    "    \n",
    "    print(f\"\\n🚀 Running evaluation...\")\n",
    "    print(f\"   Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd='.',  # Run in current directory (energy-based-model)\n",
    "            timeout=600  # 10 minute timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"❌ ERROR: Evaluation failed with return code {result.returncode}\")\n",
    "            print(f\"stderr: {result.stderr}\")\n",
    "            return None\n",
    "        \n",
    "        # Parse output\n",
    "        output = result.stdout\n",
    "        print(f\"\\n📊 Parsing results...\")\n",
    "        \n",
    "        # Extract metrics from output\n",
    "        metrics = parse_evaluation_output(output, task)\n",
    "        \n",
    "        if not metrics:\n",
    "            print(f\"⚠️  WARNING: Could not parse metrics from output\")\n",
    "            print(f\"Output:\\n{output}\")\n",
    "            return None\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'task': task,\n",
    "            'curriculum': curriculum_key,\n",
    "            'curriculum_name': CURRICULUM_CONFIGS[curriculum_key]['name'],\n",
    "            'difficulty': 'harder' if ood else 'same',\n",
    "            'ood': ood,\n",
    "            'num_samples': num_samples,\n",
    "            'milestone': 50,\n",
    "            'step': 50000,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n✅ Evaluation Results:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {key}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"   {key}: {value}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"❌ ERROR: Evaluation timed out after 10 minutes\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_evaluation_output(output: str, task: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse metrics from train.py --evaluate output.\n",
    "    \n",
    "    FIXED: Now handles plain tabulate format without box-drawing characters.\n",
    "    \n",
    "    The output contains tables in plain format like:\n",
    "    --------------  ----------\n",
    "    mse                2.99348\n",
    "    identity_error  1094.47\n",
    "    --------------  ----------\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Strategy 1: Parse plain tabulate format (KEY-VALUE pairs with whitespace)\n",
    "    # Split output into lines and look for metric lines\n",
    "    lines = output.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines, separator lines, and header lines\n",
    "        if not line or line.startswith('-') or line.startswith('='):\n",
    "            continue\n",
    "        \n",
    "        # Skip lines that look like headers or section titles\n",
    "        if 'Validation Result' in line or 'training complete' in line:\n",
    "            continue\n",
    "            \n",
    "        # Try to parse as \"metric_name  value\" format\n",
    "        # Split on whitespace and check if we have exactly 2 parts\n",
    "        parts = line.split()\n",
    "        \n",
    "        if len(parts) == 2:\n",
    "            metric_name, value_str = parts\n",
    "            \n",
    "            # Validate that metric_name looks like a metric (lowercase with underscores)\n",
    "            # and value_str looks like a number\n",
    "            if metric_name.replace('_', '').isalpha() and metric_name.islower():\n",
    "                try:\n",
    "                    value = float(value_str)\n",
    "                    metrics[metric_name] = value\n",
    "                except ValueError:\n",
    "                    # Not a valid number, skip\n",
    "                    continue\n",
    "    \n",
    "    # Strategy 2: Also try the old box-drawing format as fallback\n",
    "    # Look for patterns like: │ metric_name │ value │\n",
    "    pattern = r'│\\s*([a-z_]+)\\s*│\\s*([0-9.]+)\\s*│'\n",
    "    matches = re.findall(pattern, output)\n",
    "    \n",
    "    for metric_name, value in matches:\n",
    "        try:\n",
    "            metrics[metric_name] = float(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Ensure we have at least MSE\n",
    "    if 'mse' not in metrics and 'mse_error' in metrics:\n",
    "        metrics['mse'] = metrics['mse_error']\n",
    "    \n",
    "    # For inverse task, ensure MAE\n",
    "    if task == 'inverse':\n",
    "        if 'mae' not in metrics and 'mean_abs_error' in metrics:\n",
    "            metrics['mae'] = metrics['mean_abs_error']\n",
    "    \n",
    "    # For other tasks, compute MAE if we only have MSE\n",
    "    if 'mae' not in metrics and 'mse' in metrics:\n",
    "        # Approximate MAE as sqrt(MSE) - rough estimate\n",
    "        metrics['mae'] = metrics['mse'] ** 0.5\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def parse_csv_validation_results(task: str, curriculum_key: str, ood: bool) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Alternative: Parse results from CSV logs if available.\n",
    "    This is even simpler if you have CSV logs enabled during training.\n",
    "    \"\"\"\n",
    "    csv_dir = Path(f'./csv_logs_{task}_{curriculum_key}')\n",
    "    \n",
    "    if not csv_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Find validation CSV file\n",
    "    csv_files = list(csv_dir.glob('validation_metrics_*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Read the most recent CSV\n",
    "    csv_path = sorted(csv_files)[-1]\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter for step 50000 and correct difficulty\n",
    "    difficulty_name = 'harder' if ood else 'same'\n",
    "    \n",
    "    # Get metrics at step 50k\n",
    "    results_at_50k = df[\n",
    "        (df['step'] == 50000) & \n",
    "        (df['dataset_name'].str.contains(difficulty_name, case=False, na=False))\n",
    "    ]\n",
    "    \n",
    "    if len(results_at_50k) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    metrics = {}\n",
    "    for _, row in results_at_50k.iterrows():\n",
    "        metric_name = row['metric_name']\n",
    "        metric_value = row['metric_value']\n",
    "        metrics[metric_name] = metric_value\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "def print_evaluation_summary(all_results):\n",
    "    \"\"\"Print a formatted summary of all evaluation results.\"\"\"\n",
    "    if not all_results:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📋 EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in all_results:\n",
    "        print(f\"\\n{result['task'].upper()} | {result['curriculum_name']} | \"\n",
    "              f\"{'HARDER' if result['ood'] else 'SAME'}\")\n",
    "        print(f\"   MSE: {result.get('mse', 0.0):.6f}\")\n",
    "        print(f\"   MAE: {result.get('mae', 0.0):.6f}\")\n",
    "        if 'accuracy' in result:\n",
    "            print(f\"   Accuracy: {result['accuracy']:.4f}\")\n",
    "        if 'identity_error' in result:\n",
    "            print(f\"   Identity Error: {result['identity_error']:.6f}\")\n",
    "\n",
    "\n",
    "def compare_with_paper(results, paper_results):\n",
    "    \"\"\"Compare results with IRED paper Table 1.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to compare\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 COMPARISON WITH IRED PAPER TABLE 1\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for task in ['addition', 'lowrank', 'inverse']:\n",
    "        print(f\"\\n{task.upper()}:\")\n",
    "        task_results = [r for r in results if r['task'] == task]\n",
    "        \n",
    "        if not task_results:\n",
    "            print(f\"   No results found for {task}\")\n",
    "            continue\n",
    "        \n",
    "        for r in task_results:\n",
    "            difficulty = 'same_difficulty' if not r['ood'] else 'harder_difficulty'\n",
    "            paper_mse = paper_results.get(task, {}).get(difficulty, 0.0)\n",
    "            our_mse = r.get('mse', 0.0)\n",
    "            \n",
    "            if paper_mse > 0:\n",
    "                diff_pct = ((our_mse - paper_mse) / paper_mse * 100)\n",
    "            else:\n",
    "                diff_pct = 0.0\n",
    "            \n",
    "            print(f\"   {r['curriculum_name']:20s} | {'Same' if not r['ood'] else 'Harder':6s} | \"\n",
    "                  f\"Ours: {our_mse:.4f} | Paper: {paper_mse:.4f} | Diff: {diff_pct:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE EVALUATIONS FOR ALL MODELS AT 50K STEP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTING EVALUATIONS AT 50K CHECKPOINT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_evaluation_results = []\n",
    "\n",
    "# Evaluate all combinations: 3 tasks × 2 curricula × 2 difficulties = 12 evaluations\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# EVALUATING TASK: {task.upper()}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    for curriculum_key in CURRICULUM_CONFIGS.keys():\n",
    "        curriculum_name = CURRICULUM_CONFIGS[curriculum_key]['name']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Curriculum: {curriculum_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Evaluate on SAME difficulty\n",
    "        print(f\"\\n▶ Evaluating on SAME difficulty...\")\n",
    "        result_same = evaluate_at_50k(\n",
    "            task=task,\n",
    "            curriculum_key=curriculum_key,\n",
    "            ood=False,\n",
    "            num_samples=EVAL_CONFIG['num_eval_samples']\n",
    "        )\n",
    "        if result_same:\n",
    "            all_evaluation_results.append(result_same)\n",
    "        \n",
    "        # Evaluate on HARDER difficulty\n",
    "        print(f\"\\n▶ Evaluating on HARDER difficulty...\")\n",
    "        result_harder = evaluate_at_50k(\n",
    "            task=task,\n",
    "            curriculum_key=curriculum_key,\n",
    "            ood=True,\n",
    "            num_samples=EVAL_CONFIG['num_eval_samples']\n",
    "        )\n",
    "        if result_harder:\n",
    "            all_evaluation_results.append(result_harder)\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESS AND DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE - PROCESSING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "eval_df = pd.DataFrame(all_evaluation_results)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print_evaluation_summary(all_evaluation_results)\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON WITH IRED PAPER TABLE 1\n",
    "# ============================================================================\n",
    "\n",
    "compare_with_paper(all_evaluation_results, IRED_PAPER_RESULTS)\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE DETAILED COMPARISON TABLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED EVALUATION RESULTS BY TASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for task in TASKS:\n",
    "    task_results = eval_df[eval_df['task'] == task]\n",
    "    \n",
    "    if len(task_results) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{task.upper()} TASK EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    table_data = []\n",
    "    for _, row in task_results.iterrows():\n",
    "        difficulty = 'SAME' if not row['ood'] else 'HARDER'\n",
    "        paper_key = 'same_difficulty' if not row['ood'] else 'harder_difficulty'\n",
    "        paper_mse = IRED_PAPER_RESULTS.get(task, {}).get(paper_key, 0.0)\n",
    "        \n",
    "        our_mse = row.get('mse', 0.0)\n",
    "        diff = our_mse - paper_mse\n",
    "        diff_pct = (diff / paper_mse * 100) if paper_mse > 0 else 0.0\n",
    "        \n",
    "        table_data.append({\n",
    "            'Curriculum': row['curriculum_name'],\n",
    "            'Difficulty': difficulty,\n",
    "            'Our MSE': f\"{our_mse:.6f}\",\n",
    "            'Paper MSE': f\"{paper_mse:.6f}\",\n",
    "            'Difference': f\"{diff:+.6f}\",\n",
    "            'Diff %': f\"{diff_pct:+.1f}%\"\n",
    "        })\n",
    "        \n",
    "        # Add task-specific metrics\n",
    "        if task == 'inverse' and 'identity_error' in row:\n",
    "            table_data[-1]['Identity Error'] = f\"{row['identity_error']:.6f}\"\n",
    "        if 'mae' in row:\n",
    "            table_data[-1]['MAE'] = f\"{row['mae']:.6f}\"\n",
    "    \n",
    "    # Display table\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    print(tabulate(table_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "# ============================================================================\n",
    "# AGGREGATE METRICS AND RANKINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CURRICULUM PERFORMANCE RANKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate average performance across all tasks and difficulties\n",
    "curriculum_summary = []\n",
    "for curriculum_key, curriculum_config in CURRICULUM_CONFIGS.items():\n",
    "    curriculum_results = eval_df[eval_df['curriculum'] == curriculum_key]\n",
    "    \n",
    "    if len(curriculum_results) > 0:\n",
    "        avg_mse = curriculum_results['mse'].mean()\n",
    "        avg_mae = curriculum_results['mae'].mean() if 'mae' in curriculum_results.columns else 0.0\n",
    "        \n",
    "        # Calculate success rate (how many evaluations ran successfully)\n",
    "        success_rate = len(curriculum_results) / (len(TASKS) * 2) * 100  # 2 = same + harder\n",
    "        \n",
    "        curriculum_summary.append({\n",
    "            'Curriculum': curriculum_config['name'],\n",
    "            'Avg MSE': avg_mse,\n",
    "            'Avg MAE': avg_mae,\n",
    "            'Success Rate': f\"{success_rate:.0f}%\",\n",
    "            'Num Evals': len(curriculum_results)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(curriculum_summary).sort_values('Avg MSE')\n",
    "print(\"\\nOverall Performance Summary:\")\n",
    "print(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False, floatfmt='.6f'))\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = Path('./evaluation_results_50k')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save detailed results\n",
    "eval_df.to_csv(output_dir / 'detailed_evaluation_results.csv', index=False)\n",
    "pd.DataFrame(curriculum_summary).to_csv(output_dir / 'curriculum_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\n📁 Results saved to: {output_dir}\")\n",
    "print(\"   • detailed_evaluation_results.csv\")\n",
    "print(\"   • curriculum_summary.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 EVALUATION AT 50K CHECKPOINT - FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✅ Completed {len(all_evaluation_results)} evaluations:\")\n",
    "print(f\"   • {len(TASKS)} tasks: {', '.join(TASKS)}\")\n",
    "print(f\"   • {len(CURRICULUM_CONFIGS)} curricula: {', '.join([c['name'] for c in CURRICULUM_CONFIGS.values()])}\")\n",
    "print(f\"   • 2 difficulty levels: SAME and HARDER\")\n",
    "\n",
    "if len(summary_df) > 0:\n",
    "    best_curriculum = summary_df.iloc[0]\n",
    "    print(f\"\\n🏆 BEST PERFORMING CURRICULUM: {best_curriculum['Curriculum']}\")\n",
    "    print(f\"   Average MSE: {best_curriculum['Avg MSE']:.6f}\")\n",
    "    print(f\"   Completed: {best_curriculum['Num Evals']}/6 task evaluations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 Evaluation complete! Review the results above and saved CSV files.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
